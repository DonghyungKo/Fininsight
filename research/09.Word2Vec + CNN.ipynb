{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unset PYTHONPATH first\n",
    "from ko_text import *\n",
    "from ko_crawler import *\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = NLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = pd.read_csv('Data/meta_morphs_final.csv', encoding = 'utf-8')\n",
    "\n",
    "# 용량을 줄이기 위해 '단어 단어' 꼴로 묶어둔 token을 ['단어', '단어'] 꼴로 풀기\n",
    "token_df['Token'] = [token.split() for token in token_df['Token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Text</th>\n",
       "      <th>Token</th>\n",
       "      <th>Num of Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>financial</td>\n",
       "      <td>\\n\\n\\n텀블벅에서 크라우드 펀딩이 이뤄지고 있는 `아침달 시집`.\\n\\n    ...</td>\n",
       "      <td>[텀블벅, 크라, 우드, 펀딩, 이뤄지고, 아침, 시집, 많지, 않은, 금액, 으로...</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>economy</td>\n",
       "      <td>\\n\\n\\n[사진 제공: 연합뉴스]\\n\\n                     유류...</td>\n",
       "      <td>[유류, 인하, 국제, 유가, 급락, 입어, 국내, 휘발유, 경유, 하락, 특히, ...</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>financial</td>\n",
       "      <td>부득이한 사정으로 매월 내는 보험료가 부담이 될 때 계약은 그대로 유지하면서 보험...</td>\n",
       "      <td>[부득이, 사정, 매월, 내는, 보험료, 부담, 계약, 그대로, 유지, 보험료, 부...</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>estate</td>\n",
       "      <td>한때 `미분양의 늪`으로 통하던 경기도 파주시 부동산 시장이 달라지고 있다. 지난해...</td>\n",
       "      <td>[한때, 미분, 하던, 경기도, 파주시, 부동산, 시장, 달라지고, 분양, 파주, ...</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>economy</td>\n",
       "      <td>\\n\\n\\n인디고뱅크의 `미키인서울` 컬래버 맨투맨  &lt;사진제공=월트디즈니코리아&gt;\\...</td>\n",
       "      <td>[인디고, 뱅크, 미키, 서울, 컬래버, 투맨, 월트디즈니, 사의, 마스코트, 미키...</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Section                                               Text  \\\n",
       "0  financial  \\n\\n\\n텀블벅에서 크라우드 펀딩이 이뤄지고 있는 `아침달 시집`.\\n\\n    ...   \n",
       "1    economy  \\n\\n\\n[사진 제공: 연합뉴스]\\n\\n                     유류...   \n",
       "2  financial   부득이한 사정으로 매월 내는 보험료가 부담이 될 때 계약은 그대로 유지하면서 보험...   \n",
       "3     estate  한때 `미분양의 늪`으로 통하던 경기도 파주시 부동산 시장이 달라지고 있다. 지난해...   \n",
       "4    economy  \\n\\n\\n인디고뱅크의 `미키인서울` 컬래버 맨투맨  <사진제공=월트디즈니코리아>\\...   \n",
       "\n",
       "                                               Token  Num of Tokens  \n",
       "0  [텀블벅, 크라, 우드, 펀딩, 이뤄지고, 아침, 시집, 많지, 않은, 금액, 으로...            263  \n",
       "1  [유류, 인하, 국제, 유가, 급락, 입어, 국내, 휘발유, 경유, 하락, 특히, ...            166  \n",
       "2  [부득이, 사정, 매월, 내는, 보험료, 부담, 계약, 그대로, 유지, 보험료, 부...            314  \n",
       "3  [한때, 미분, 하던, 경기도, 파주시, 부동산, 시장, 달라지고, 분양, 파주, ...            165  \n",
       "4  [인디고, 뱅크, 미키, 서울, 컬래버, 투맨, 월트디즈니, 사의, 마스코트, 미키...            196  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41418, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Word2Vec + CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Word2VecCNN():\n",
    "    \n",
    "    def __init__(self, path_to_word2vec_model = ''):\n",
    "        print('초기 세팅을 시작합니다.')\n",
    "        print('사전에 학습된 Word2Vec 모델을 불러옵니다.')\n",
    "        try: \n",
    "            self.w2v_model = Word2Vec.load(path_to_word2vec_model)\n",
    "            print('학습된 Word2Vec 모델을 성공적으로 불러왔습니다.')\n",
    "            \n",
    "        except : \n",
    "            self.w2v_model = Word2Vec(min_count = 1)\n",
    "            print('Word2Vec 모델을 불러오는데 실패하였습니다.')\n",
    "            print('=================================================================================')\n",
    "            print('Default 세팅의 Word2Vec 모델을 새롭게 생성합니다.')\n",
    "            print('Process 진행에 앞서, Word2Vec 모델의 학습이 필요합니다.')\n",
    "            print('bulid_and_train_w2v_model 함수를 사용하여, word2vec 모델을 학습하시기 바랍니다')\n",
    "            print('=================================================================================')\n",
    "            print('Word2Vec의 Hyper-parameter 튜닝을 원하신다면 self.w2v_model을 새롭게 생성한 모델로 덮어 쓰시면 됩니다. ')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def build_and_train_w2v_model(self, token_ls):\n",
    "        '''\n",
    "        Word2Vec 모델을 학습하는 함수입니다.\n",
    "        \n",
    "        #inputs   \n",
    "        token_ls : iterable, 문서가 토큰으로 구부된 형태로 저장된 리스트\n",
    "        \n",
    "        #return\n",
    "        모델 학습\n",
    "        '''\n",
    "        import logging\n",
    "        #logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "        self.w2v_model.build_vocab(token_ls)\n",
    "\n",
    "        self.w2v_model.train(token_ls,\n",
    "                            total_examples = self.w2v_model.corpus_count,\n",
    "                            epochs = 10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_w2v_model(self, path_to_model):\n",
    "        '''\n",
    "        사전에 학습한 Word2Vec 모델을 불러오는 함수입니다.\n",
    "        '''\n",
    "    \n",
    "        self.w2v_model = Word2Vec.load(path_to_model)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_equal_sequence_doc_vector_for_cnn(self, doc, sequence_length = 10):\n",
    "        '''\n",
    "        CNN 학습을 위해, 모든 문서의 token 길이를 동일한 값(sequence_length)로 맞춰주는 함수입니다,\n",
    "        한 개의 문서(token list)를 Word2Vec으로 벡터화하여, CNN학습에 적합한 4D로 변환합니다.\n",
    "        문서의 토큰 수가 sequence_length보다 적은 경우, 부족한 만큼 zero padding을 추가합니다.\n",
    "\n",
    "        input\n",
    "        doc : iterable, 토큰으로 구분된 array 형태의 문서\n",
    "        w2v_model : word2vec_model, 개별 토큰을 벡터화하기 위한 word2vec 모델\n",
    "        sequence_length : int, 한 문서 당, 최대 토큰의 수\n",
    "        '''\n",
    "        n_dim = self.w2v_model.vector_size\n",
    "        \n",
    "        if len(doc) < 1:\n",
    "            return np.zeros((sequence_length,self.w2v_model.vector_size)).reshape(sequence_length, n_dim, -1)\n",
    "\n",
    "        elif len(doc) < sequence_length:\n",
    "            # 해당 단어가 w2v 모델에 있으면, 해당 벡터 값으로, 없으면 0벡터로 변환\n",
    "            return_array = np.array([self.w2v_model.wv.__getitem__(token) if self.w2v_model.wv.__contains__(token) else [0] * n_dim for token in doc])\n",
    "\n",
    "            # 길이가 짧은 문서는 0백터로 max_len의 크기에 맞도록 패딩을 해준다.\n",
    "            n_padding = sequence_length - len(doc)\n",
    "            return_array = np.concatenate((return_array, np.zeros((n_padding, n_dim))))\n",
    "\n",
    "\n",
    "        # 문서의 길이가 max_length보다 길면 앞에서 max_length의 토큰까지 짜른다.\n",
    "        elif len(doc) >= sequence_length:\n",
    "            # 해당 단어가 w2v 모델에 있으면, 해당 벡터 값으로, 없으면 0벡터로 변환\n",
    "            return_array = np.array([self.w2v_model.wv.__getitem__(token) if self.w2v_model.wv.__contains__(token) else [0] * n_dim for token in doc[:sequence_length]])\n",
    "        \n",
    "        return return_array.reshape(sequence_length, n_dim,-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_equal_sequence_doc_vectors_for_cnn(self, doc_ls, sequence_length = 10):\n",
    "        '''\n",
    "        복수 개의 문서(token list)를 Word2Vec으로 벡터화하여, CNN학습에 적합한 4D로 변환하는 함수입니다.\n",
    "\n",
    "        input\n",
    "        doc_ls : iterable, 토큰으로 구분된 array 형태의 문서가 저장된 리스트\n",
    "        sequence_length : int, 한 문서 당, 최대 토큰의 수\n",
    "        '''\n",
    "        \n",
    "        return np.array([self.generate_equal_sequence_doc_vector_for_cnn(doc, sequence_length) for doc in doc_ls])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def _init_weights(self,shape):\n",
    "        '''\n",
    "        CNN 학습을 위해, filter의 초기 weight를 주는 함수입니다.\n",
    "\n",
    "        inputs \n",
    "        shape = 4D-array, [batch, n_height, n_width, n_channel] \n",
    "        '''\n",
    "        return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 세팅을 시작합니다.\n",
      "사전에 학습된 Word2Vec 모델을 불러옵니다.\n",
      "학습된 Word2Vec 모델을 성공적으로 불러왔습니다.\n"
     ]
    }
   ],
   "source": [
    "w2v_cnn = Word2VecCNN('word2vec_model/w2v_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w2v_cnn.w2v_model = Word2Vec(min_count = 5, \n",
    "                             window = 5,\n",
    "                             sample = 1e-5,\n",
    "                             size = 100,\n",
    "                             workers = 4,\n",
    "                             iter = 30,\n",
    "                             sg = 1,\n",
    "                             hs = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_cnn.build_and_train_w2v_model(token_df['Token'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('지능', 0.8219226002693176),\n",
       " ('인공', 0.7812213897705078),\n",
       " ('음성인식', 0.7169848680496216),\n",
       " ('사물인터넷', 0.7135927677154541),\n",
       " ('어시스턴트', 0.7003586292266846),\n",
       " ('스피커', 0.6795388460159302),\n",
       " ('자연어', 0.6697943210601807),\n",
       " ('IoT', 0.6628202199935913),\n",
       " ('플랫폼', 0.6612290740013123),\n",
       " ('데이터', 0.6600569486618042)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_cnn.w2v_model.most_similar('AI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('도널드', 0.9513905048370361),\n",
       " ('행정부', 0.8262176513671875),\n",
       " ('백악관', 0.808573305606842),\n",
       " ('대통령', 0.7891108989715576),\n",
       " ('워싱턴포스트', 0.7868554592132568),\n",
       " ('WP', 0.7828512787818909),\n",
       " ('중간', 0.7472510933876038),\n",
       " ('이뤄졌다는데', 0.7459768652915955),\n",
       " ('미국', 0.7260775566101074),\n",
       " ('하원', 0.6922484636306763)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_cnn.w2v_model.most_similar('트럼프')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('정책', 0.8150776028633118),\n",
       " ('경제성장', 0.8124768733978271),\n",
       " ('기조', 0.7947652339935303),\n",
       " ('경제정책', 0.7556207180023193),\n",
       " ('보호무역', 0.7061445713043213),\n",
       " ('대외', 0.7056616544723511),\n",
       " ('심화', 0.695213794708252),\n",
       " ('경제위기', 0.6881315112113953),\n",
       " ('홍장표', 0.6862689256668091),\n",
       " ('포용', 0.6845958232879639)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_cnn.w2v_model.most_similar('경제')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 저장\n",
    "w2v_cnn.w2v_model.save('word2vec_model/w2v_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습을 위한 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = round(len(token_df) * 0.8)\n",
    "np.random.seed(0)\n",
    "train_index_ls = np.random.choice(token_df.index, train_size, replace = False)\n",
    "test_index_ls = [x for x in token_df.index if not x in train_index_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33134, 4) (8284, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df = token_df.loc[train_index_ls]\n",
    "test_df = token_df.loc[test_index_ls]\n",
    "\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'bio & tech': 1739,\n",
       "         'business': 4863,\n",
       "         'culture & art': 4102,\n",
       "         'economy': 2608,\n",
       "         'estate': 3932,\n",
       "         'financial': 746,\n",
       "         'it': 1742,\n",
       "         'politics': 3775,\n",
       "         'society': 3356,\n",
       "         'stock': 2508,\n",
       "         'world': 3763})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_df['Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_ls = train_df['Token'].tolist()\n",
    "train_label_ls = train_df['Section'].tolist()\n",
    "\n",
    "test_token_ls = test_df['Token'].tolist()\n",
    "test_label_ls = test_df['Section'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_ls, train_label_ls = nlp.oversample_batch(train_token_ls, train_label_ls, 3000)\n",
    "test_token_ls, test_label_ls = nlp.undersample_batch(test_token_ls, test_label_ls, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'bio & tech': 3000,\n",
       "         'business': 3000,\n",
       "         'culture & art': 3000,\n",
       "         'economy': 3000,\n",
       "         'estate': 3000,\n",
       "         'financial': 3000,\n",
       "         'it': 3000,\n",
       "         'politics': 3000,\n",
       "         'society': 3000,\n",
       "         'stock': 3000,\n",
       "         'world': 3000})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_label_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'bio & tech': 100,\n",
       "         'business': 100,\n",
       "         'culture & art': 100,\n",
       "         'economy': 100,\n",
       "         'estate': 100,\n",
       "         'financial': 100,\n",
       "         'it': 100,\n",
       "         'politics': 100,\n",
       "         'society': 100,\n",
       "         'stock': 100,\n",
       "         'world': 100})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(test_label_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec으로 문서 벡터화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = w2v_cnn.generate_equal_sequence_doc_vectors_for_cnn(train_token_ls, sequence_length=sequence_length)\n",
    "test_X = w2v_cnn.generate_equal_sequence_doc_vectors_for_cnn(test_token_ls, sequence_length=sequence_length)\n",
    "\n",
    "train_Y = pd.get_dummies(train_label_ls).values.astype('float32')\n",
    "test_Y = pd.get_dummies(test_label_ls).values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33000, 100, 100, 1), (1100, 100, 100, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = train_X.shape[2]\n",
    "n_class = len(set(test_label_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = sequence_length\n",
    "n_class = train_Y.shape[1]\n",
    "\n",
    "batch_size = 100\n",
    "test_size = 300\n",
    "\n",
    "num_filter = 100\n",
    "n_fc = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_filter_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.1))\n",
    "\n",
    "def init_weights(shape, cnt):\n",
    "    return tf.get_variable('W%s'%cnt, shape = shape, initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def graph(X,\n",
    "          p_keep_conv, \n",
    "          p_keep_hidden, \n",
    "          filter_size_ls = [1,2,3,4,5], \n",
    "          num_filter = num_filter, \n",
    "          sequence_length = sequence_length,\n",
    "          n_fc = n_fc, \n",
    "          n_class= n_class,\n",
    "          n_dim = n_dim):\n",
    "\n",
    "    max_pool_result_ls = []\n",
    "    \n",
    "    # 각각의 사이즈의 필터를 num_filter개 생성하여 convolution & max_pool\n",
    "    for filter_size in filter_size_ls:\n",
    "\n",
    "        filter_ = init_filter_weights([filter_size, n_dim, 1, num_filter])\n",
    "        \n",
    "        \n",
    "        # l1_conv shape=(?, sequence_length - filter_size + 1, n_dim, num_filter)\n",
    "        l1_conv = tf.nn.relu(tf.nn.conv2d(input = X,\n",
    "                                      filter = filter_,  \n",
    "                                      strides=[1, 1, 1, 1], \n",
    "                                      padding='VALID')) \n",
    "        \n",
    "        # l1_pool shape=(?, 1, 1, num_filter)\n",
    "        l1_pool = tf.nn.max_pool(l1_conv, \n",
    "                            ksize=[1, sequence_length - filter_size + 1, 1, 1], \n",
    "                            strides=[1, 1, 1, 1], \n",
    "                            padding='VALID')\n",
    "\n",
    "        l1_pool = tf.nn.dropout(l1_pool, p_keep_conv)\n",
    "\n",
    "        max_pool_result_ls.append(l1_pool)\n",
    "\n",
    "    # 각기 다른 종류의 필터를 거쳐 conv-pool한 결과를 concat\n",
    "    num_filter_total = num_filter * len(filter_size_ls)\n",
    "    max_pool_concat = tf.concat(max_pool_result_ls, 3)\n",
    "    max_pool_concat_flat = tf.reshape(max_pool_concat, [-1, num_filter_total])\n",
    "\n",
    "    # fully-connect\n",
    "    w_fc = init_weights([num_filter_total, n_fc], 1)\n",
    "    w_output = init_weights([n_fc, n_class], 2)\n",
    "\n",
    "    logit = tf.nn.relu(tf.matmul(max_pool_concat_flat, w_fc))\n",
    "    logit = tf.nn.dropout(logit, p_keep_hidden)\n",
    "    logit = tf.matmul(logit, w_output) + b\n",
    "    \n",
    "    # softmax\n",
    "    hypothesis = tf.nn.softmax(logit)\n",
    "#    print(h)\n",
    "    return logit, hypothesis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, sequence_length, w2v_cnn.w2v_model.vector_size, 1])\n",
    "Y = tf.placeholder(\"float\", [None, n_class])\n",
    "\n",
    "b = init_filter_weights([n_class])\n",
    "p_keep_conv = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "logit, hypothesis = graph(X, p_keep_conv, p_keep_hidden, sequence_length= sequence_length, n_class = n_class)\n",
    "\n",
    "lamb = 0.0001\n",
    "learning_rate = 0.0005\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y)) #+ lamb * tf.reduce_sum(tf.square(w_output))\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "predict = tf.argmax(hypothesis, 1)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y,1), predict), dtype= tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epoch = 500\n",
    "\n",
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "    train_cost_ls = []\n",
    "    test_cost_ls = []\n",
    "    \n",
    "    for i in range(training_epoch):\n",
    "        training_batch = zip(range(0, len(train_X), batch_size),\n",
    "                             range(batch_size, len(train_X)+1, batch_size))\n",
    "        \n",
    "        for start, end in training_batch:\n",
    "            sess.run(train, \n",
    "                     feed_dict={X: train_X[start:end], \n",
    "                                Y: train_Y[start:end],\n",
    "                                p_keep_conv: 0.7, \n",
    "                                p_keep_hidden: 0.7})\n",
    "        \n",
    "        \n",
    "        test_indices = np.arange(len(test_X)) # Get A Test Batch\n",
    "        np.random.shuffle(test_indices)\n",
    "        test_indices = test_indices[0:test_size]\n",
    "        \n",
    "        train_dict = {X: train_X[test_indices],\n",
    "                      Y: train_Y[test_indices],\n",
    "                      p_keep_conv: 1.0,\n",
    "                      p_keep_hidden : 1.0}\n",
    "        \n",
    "        test_dict = {X: test_X,\n",
    "                     Y: test_Y,\n",
    "                     p_keep_conv : 1.0,\n",
    "                     p_keep_hidden : 1.0}\n",
    "        \n",
    "        train_cost = sess.run(cost, feed_dict= train_dict)\n",
    "        train_cost_ls.append(train_cost)\n",
    "        \n",
    "        test_cost, acc = sess.run([cost, accuracy], feed_dict = test_dict)\n",
    "        test_cost_ls.append(test_cost)\n",
    "        \n",
    "        print('Epoch : %s, train_cost : %s, test_cost : %s'%(i,round(train_cost, 3), round(test_cost,3)))\n",
    "        print('Accuracy : %s'%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(train_cost_ls, label = 'train cost')\n",
    "plt.plot(test_cost_ls, label = 'test cost')\n",
    "\n",
    "plt.legend(loc = 'best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
