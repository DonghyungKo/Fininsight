{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dong\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\requests\\__init__.py:80: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "C:\\Users\\Dong\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from ko_text import *\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TF-IDF classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Data/Train_final.csv', encoding = 'cp949')\n",
    "test_df = pd.read_csv('Data/Test_final.csv', encoding = 'cp949')\n",
    "\n",
    "# 용량을 줄이기 위해 '단어 단어' 꼴로 묶어둔 token을 ['단어', '단어'] 꼴로 풀기\n",
    "train_df['Token'] = [token.split() for token in train_df['Token']]\n",
    "test_df['Token'] = [token.split() for token in test_df['Token']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=50000,\n",
    "                              min_df = 3)),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior=None))),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    #'tfidf__min_df': (2),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__estimator__alpha': (1e-2, 1e-3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Fitting 2 folds for each of 18 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed: 19.4min\n",
      "[Parallel(n_jobs=2)]: Done  36 out of  36 | elapsed: 25.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...assifier(estimator=MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       "          n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=2,\n",
       "       param_grid={'tfidf__max_df': (0.25, 0.5, 0.75), 'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)], 'clf__estimator__alpha': (0.01, 0.001)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% time\n",
    "\n",
    "train_corpus = [' '.join(doc) for doc in train_df['Token']]\n",
    "test_corpus = [' '.join(doc) for doc in test_df['Token']]\n",
    "\n",
    "y_train = train_df['Section'].tolist()\n",
    "y_test = test_df['Section'].tolist()\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(train_corpus, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=50000, min_df=3,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('clf', OneVsRestClassifier(estimator=MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True),\n",
      "          n_jobs=1))]\n",
      "Applying best classifier on test data:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.25, max_features=50000, min_df=3,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,...ssifier(estimator=MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True),\n",
      "          n_jobs=1))])\n",
      "0.7048658118977463\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)\n",
    "\n",
    "# measuring performance on test set\n",
    "print(\"Applying best classifier on test data:\")\n",
    "best_clf = grid_search_tune.best_estimator_\n",
    "print(best_clf)\n",
    "\n",
    "print(grid_search_tune.best_score_)\n",
    "#predictions = best_clf.predict(test_df['Token'].tolist())\n",
    "\n",
    "#print(classification_report(test_df['Section'].tolist(), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=50000,\n",
    "                              min_df = 3)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC())),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    \"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "    \"clf__estimator__class_weight\": ['balanced', None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 998 µs\n",
      "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed: 21.8min\n",
      "[Parallel(n_jobs=2)]: Done 108 out of 108 | elapsed: 83.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...lti_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "          n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=2,\n",
       "       param_grid={'tfidf__max_df': (0.25, 0.5, 0.75), 'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)], 'clf__estimator__C': [0.01, 0.1, 1], 'clf__estimator__class_weight': ['balanced', None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% time\n",
    "\n",
    "train_corpus = [' '.join(doc) for doc in train_df['Token']]\n",
    "test_corpus = [' '.join(doc) for doc in test_df['Token']]\n",
    "\n",
    "y_train = train_df['Section'].tolist()\n",
    "y_test = test_df['Section'].tolist()\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(train_corpus, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=50000, min_df=3,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "          n_jobs=1))]\n",
      "Applying best classifier on test data:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=50000, min_df=3,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,...lti_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0),\n",
      "          n_jobs=1))])\n",
      "0.7416694160343121\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)\n",
    "\n",
    "# measuring performance on test set\n",
    "print(\"Applying best classifier on test data:\")\n",
    "best_clf = grid_search_tune.best_estimator_\n",
    "print(best_clf)\n",
    "\n",
    "print(grid_search_tune.best_score_)\n",
    "#predictions = best_clf.predict(test_df['Token'].tolist())\n",
    "\n",
    "#print(classification_report(test_df['Section'].tolist(), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=30000,\n",
    "                              min_df = 5)),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'))),\n",
    "])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    \"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "    \"clf__estimator__class_weight\": ['balanced', None],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed: 19.5min\n",
      "[Parallel(n_jobs=2)]: Done 108 out of 108 | elapsed: 83.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=30000, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "..._state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=2,\n",
       "       param_grid={'tfidf__max_df': (0.25, 0.5, 0.75), 'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)], 'clf__estimator__C': [0.01, 0.1, 1], 'clf__estimator__class_weight': ['balanced', None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% time\n",
    "\n",
    "train_corpus = [' '.join(doc) for doc in train_df['Token']]\n",
    "test_corpus = [' '.join(doc) for doc in test_df['Token']]\n",
    "\n",
    "y_train = train_df['Section'].tolist()\n",
    "y_test = test_df['Section'].tolist()\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(train_corpus, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=30000, min_df=5,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LogisticRegression(C=1, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='sag', tol=0.0001, verbose=0, warm_start=False),\n",
      "          n_jobs=1))]\n",
      "Applying best classifier on test data:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=30000, min_df=5,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,..._state=None,\n",
      "          solver='sag', tol=0.0001, verbose=0, warm_start=False),\n",
      "          n_jobs=1))])\n",
      "0.7364588903172959\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)\n",
    "\n",
    "# measuring performance on test set\n",
    "print(\"Applying best classifier on test data:\")\n",
    "best_clf = grid_search_tune.best_estimator_\n",
    "print(best_clf)\n",
    "\n",
    "print(grid_search_tune.best_score_)\n",
    "#predictions = best_clf.predict(test_df['Token'].tolist())\n",
    "\n",
    "#print(classification_report(test_df['Section'].tolist(), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
