{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ko_text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data는 e9t(Lucy Park)님께서 github에 공유해주신 네이버 영화평점 데이터를 사용하였습니다.\n",
    "# https://github.com/e9t/nsmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('ratings_train.txt', sep='\\t').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149995, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75170</td>\n",
       "      <td>75170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74825</td>\n",
       "      <td>74825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  document\n",
       "label                 \n",
       "0      75170     75170\n",
       "1      74825     74825"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('ratings_test.txt', sep = '\\t').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6270596</td>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9274899</td>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8544678</td>\n",
       "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6825595</td>\n",
       "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6723715</td>\n",
       "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           document  label\n",
       "0  6270596                                                굳 ㅋ      1\n",
       "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
       "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
       "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
       "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49997, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24826</td>\n",
       "      <td>24826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25171</td>\n",
       "      <td>25171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  document\n",
       "label                 \n",
       "0      24826     24826\n",
       "1      25171     25171"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = NLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_ls = nlp.extract_tokens_for_all_document_FAST_VERSION(train_df['document'])\n",
    "test_token_ls = nlp.extract_tokens_for_all_document_FAST_VERSION(test_df['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['포스터', '보고', '초딩', '영화', '오버', '연기', '조차', '가볍지', '않구나']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token_ls[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['token'] = train_token_ls\n",
    "test_df['token'] = test_token_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['len'] = [len(x) for x in train_token_ls]\n",
    "test_df['len'] = [len(x) for x in test_token_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>token</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "      <td>[더빙, 진짜, 짜증나네요, 목소리]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "      <td>[포스터, 보고, 초딩, 영화, 오버, 연기, 조차, 가볍지, 않구나]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "      <td>[무재, 밓었, 다그, 래서, 보는것을, 추천]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "      <td>[교도소, 이야기, 구먼, 솔직히, 재미, 없다, 평점, 조정]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[사이, 몬페, 익살스런, 연기, 돋보였던, 영화, 스파이더맨, 에서, 늙어, 보이...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label  \\\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0   \n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1   \n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0   \n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0   \n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1   \n",
       "\n",
       "                                               token  len  \n",
       "0                               [더빙, 진짜, 짜증나네요, 목소리]    4  \n",
       "1            [포스터, 보고, 초딩, 영화, 오버, 연기, 조차, 가볍지, 않구나]    9  \n",
       "2                         [무재, 밓었, 다그, 래서, 보는것을, 추천]    6  \n",
       "3                [교도소, 이야기, 구먼, 솔직히, 재미, 없다, 평점, 조정]    8  \n",
       "4  [사이, 몬페, 익살스런, 연기, 돋보였던, 영화, 스파이더맨, 에서, 늙어, 보이...   16  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token의 수가 0개인 리뷰도 있다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>149995.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.421821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.265146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>67.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 len\n",
       "count  149995.000000\n",
       "mean        8.421821\n",
       "std         7.265146\n",
       "min         0.000000\n",
       "25%         4.000000\n",
       "50%         6.000000\n",
       "75%        10.000000\n",
       "max        67.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[['len']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>token</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>7348295</td>\n",
       "      <td>아</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>7781028</td>\n",
       "      <td>10점</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>6081437</td>\n",
       "      <td>굳굳</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>4913424</td>\n",
       "      <td>처ㅝ주</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>7679615</td>\n",
       "      <td>잼</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>4221289</td>\n",
       "      <td>What is this movie for?</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>9509970</td>\n",
       "      <td>Yesterday when i was young</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>10147571</td>\n",
       "      <td>once upon a dream</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>9659875</td>\n",
       "      <td>ㄵ 5점대asfgsdlgkbjsjvb</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>3978509</td>\n",
       "      <td>즐~~~~~</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                    document  label token  len\n",
       "151   7348295                           아      0    []    0\n",
       "159   7781028                         10점      1    []    0\n",
       "287   6081437                          굳굳      1    []    0\n",
       "319   4913424                         처ㅝ주      1    []    0\n",
       "384   7679615                           잼      1    []    0\n",
       "404   4221289     What is this movie for?      0    []    0\n",
       "412   9509970  Yesterday when i was young      1    []    0\n",
       "470  10147571           once upon a dream      1    []    0\n",
       "485   9659875        ㄵ 5점대asfgsdlgkbjsjvb      0    []    0\n",
       "489   3978509                      즐~~~~~      0    []    0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[train_df['len'] == 0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df = 3,\n",
    "                        max_features = 50000,\n",
    "                        max_df = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tfidf.fit_transform([' '.join(doc) for doc in train_token_ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149995, 30532)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf.transform([' '.join(doc) for doc in test_token_ls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49997, 30532)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8353101186071165\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, train_label_ls)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy : ', accuracy_score(y_pred, test_label_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8294697681860912\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, train_label_ls)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy : ', accuracy_score(y_pred, test_label_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Doc2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "nlp.make_Doc2Vec_model(window = 3, \n",
    "                       vector_size= 30, \n",
    "                       min_count= 3)\n",
    "nlp.build_and_train_Doc2Vec_model(train_token_ls, train_label_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = nlp.infer_vectors_with_Doc2Vec(train_token_ls)\n",
    "X_test = nlp.infer_vectors_with_Doc2Vec(test_token_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7483849030941857\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, train_label_ls)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy : ', accuracy_score(y_pred, test_label_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7486049162949777\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(X_train, train_label_ls)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy : ', accuracy_score(y_pred, test_label_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MLP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/donghyungko/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nn = np.array(X_train).astype('float32')\n",
    "y_train_nn = pd.get_dummies(train_label_ls).values.astype('float32')\n",
    "\n",
    "\n",
    "X_test_nn = np.array(X_test).astype('float32')\n",
    "y_test_nn = pd.get_dummies(test_label_ls).values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149995, 30) (149995, 2) (49997, 30) (49997, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_nn.shape, y_train_nn.shape, X_test_nn.shape, y_test_nn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset graphs\n",
    "tf.reset_default_graph() \n",
    "\n",
    "n_class = y_train_nn.shape[1]\n",
    "\n",
    "# mini-batches\n",
    "batch_size = X_train_nn.shape[0] // 10\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train_nn, y_train_nn))\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# building placeholder\n",
    "X = tf.placeholder(tf.float32, shape = [None, nlp.Doc2Vec_model.vector_size])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, n_class])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# building layers\n",
    "n_neuron = 100\n",
    "\n",
    "W1 = tf.get_variable('W1', shape = ([nlp.Doc2Vec_model.vector_size, n_neuron]), initializer = tf.contrib.layers.xavier_initializer())\n",
    "W2 = tf.get_variable('W2', shape = ([n_neuron, n_neuron]), initializer = tf.contrib.layers.xavier_initializer())\n",
    "W3 = tf.get_variable('W3', shape = ([n_neuron, n_neuron]), initializer = tf.contrib.layers.xavier_initializer())\n",
    "W4 = tf.get_variable('W4', shape = ([n_neuron, n_neuron]), initializer = tf.contrib.layers.xavier_initializer())\n",
    "W5 = tf.get_variable('W5', shape = ([n_neuron, n_class]), initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([n_neuron]))\n",
    "b2 = tf.Variable(tf.random_normal([n_neuron]))\n",
    "b3 = tf.Variable(tf.random_normal([n_neuron]))\n",
    "b4 = tf.Variable(tf.random_normal([n_neuron]))\n",
    "b5 = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
    "\n",
    "L3 = tf.nn.relu(tf.matmul(L2,W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
    "\n",
    "L4 = tf.nn.relu(tf.matmul(L3,W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob = keep_prob)\n",
    "\n",
    "logit = tf.matmul(L4,W5) + b5\n",
    "hypothesis = tf.nn.softmax(tf.matmul(L4,W5) + b5)\n",
    "\n",
    "\n",
    "# cost : cross - entropy cost \n",
    "lamb = 0.0001\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit, labels = Y)) + lamb * tf.reduce_sum(tf.square(W5))\n",
    "\n",
    "# optimize\n",
    "learning_rate = 0.0001\n",
    "train = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# prediction\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y,1), prediction), dtype= tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "# restore results\n",
    "train_cost_list = []\n",
    "train_acc_list = []\n",
    "\n",
    "test_cost_list = []\n",
    "test_acc_list = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Run**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **mini-batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dict = {X: X_train, Y: y_train}\n",
    "test_dict = {X: X_test_nn, Y: y_test_nn, keep_prob : 1}\n",
    "\n",
    "training_epochs = 1500\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "f, l = iterator.get_next()\n",
    "\n",
    "# launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    \n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        # iterator initialize\n",
    "        sess.run(iterator.initializer)\n",
    "        avg_cost = 0\n",
    "\n",
    "        while True:\n",
    "            # mini-batch\n",
    "            try:\n",
    "                batch_x,  batch_y = sess.run([f, l])\n",
    "                feed_dict = {X : batch_x, Y: batch_y, keep_prob : 0.7}\n",
    "                \n",
    "                c, _ = sess.run([cost, train], feed_dict = feed_dict)\n",
    "                avg_cost += c\n",
    "            \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "        \n",
    "        acc, _, test_cost = sess.run([accuracy, prediction, cost], feed_dict = test_dict)\n",
    "        \n",
    "        train_cost_list.append(avg_cost)\n",
    "        test_cost_list.append(test_cost)\n",
    "            \n",
    "        if (epoch+1) % (100) == 0 :\n",
    "            \n",
    "            test_acc_list.append(acc)\n",
    "            \n",
    "            print('Epoch : %s'%(epoch+1), 'cost :',test_cost)\n",
    "            print('Accuracy :', acc)\n",
    "            \n",
    "        \n",
    "    \n",
    "    acc, y_pred, test_cost = sess.run([accuracy, prediction, cost], feed_dict = test_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(train_cost_list, label = 'train_cost')\n",
    "plt.plot(test_cost_list, label = 'test_cost')\n",
    "plt.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Word2Vec + CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Word2VecCNN():\n",
    "    \n",
    "    def __init__(self, path_to_word2vec_model = ''):\n",
    "        print('초기 세팅을 시작합니다.')\n",
    "        print('사전에 학습된 Word2Vec 모델을 불러옵니다.')\n",
    "        try: \n",
    "            self.w2v_model = Word2Vec.load(path_to_word2vec_model)\n",
    "            print('학습된 Word2Vec 모델을 성공적으로 불러왔습니다.')\n",
    "            \n",
    "        except : \n",
    "            self.w2v_model = Word2Vec(min_count = 1)\n",
    "            print('Word2Vec 모델을 불러오는데 실패하였습니다.')\n",
    "            print('=================================================================================')\n",
    "            print('Default 세팅의 Word2Vec 모델을 새롭게 생성합니다.')\n",
    "            print('Process 진행에 앞서, Word2Vec 모델의 학습이 필요합니다.')\n",
    "            print('bulid_and_train_w2v_model 함수를 사용하여, word2vec 모델을 학습하시기 바랍니다')\n",
    "            print('=================================================================================')\n",
    "            print('Word2Vec의 Hyperparameter 튜닝을 원하신다면 self.w2v_model을 새롭게 생성한 모델로 덮어 쓰시면 됩니다. ')\n",
    "    \n",
    "    \n",
    "    def build_and_train_w2v_model(self, token_ls):\n",
    "        '''\n",
    "        Word2Vec 모델을 학습하는 함수입니다.\n",
    "        \n",
    "        inputs\n",
    "        \n",
    "        token_ls : iterable, 토큰이 리스트 형태로 저장된 리스트\n",
    "        ex: [['핀인사이트','화이팅'], ['텍스트','분석','화이팅']]\n",
    "        \n",
    "        return\n",
    "        모델 학습\n",
    "        '''\n",
    "        import logging\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "        self.w2v_model.build_vocab(token_ls)\n",
    "\n",
    "        self.w2v_model.train(token_ls,\n",
    "                            total_examples = self.w2v_model.corpus_count,\n",
    "                            epochs = 10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_equal_sequence_doc_vector_for_cnn(self, doc, sequence_length = 10):\n",
    "        '''\n",
    "        한 개의 문서(token list)를 Word2Vec으로 벡터화하여, CNN학습에 적합한 4D로 변환하는 함수입니다.\n",
    "\n",
    "        input\n",
    "        doc : iterable, 토큰으로 구분된 array 형태의 문서\n",
    "        w2v_model : word2vec_model, 개별 토큰을 벡터화하기 위한 word2vec 모델\n",
    "        sequence_length : int, 한 문서 당, 최대 토큰의 수\n",
    "        '''\n",
    "        if len(doc) < 1:\n",
    "            return np.zeros((sequence_length,self.w2v_model.vector_size)).reshape(sequence_length, self.w2v_model.vector_size, -1)\n",
    "\n",
    "        elif len(doc) < sequence_length:\n",
    "            # 해당 단어가 w2v 모델에 있으면, 해당 벡터 값으로, 없으면 0벡터로 변환\n",
    "            return_array = np.array([self.w2v_model.wv.__getitem__(token) if self.w2v_model.wv.__contains__(token) else np.zeros((1,n_dim)) for token in doc])\n",
    "\n",
    "            # 길이가 짧은 문서는 0백터로 max_len의 크기에 맞도록 패딩을 해준다.\n",
    "            n_padding = sequence_length - len(doc)\n",
    "            return_array = np.concatenate((return_array, np.zeros((n_padding, self.w2v_model.vector_size))))\n",
    "\n",
    "\n",
    "        # 문서의 길이가 max_length보다 길면 앞에서 max_length의 토큰까지 짜른다.\n",
    "        elif len(doc) >= sequence_length:\n",
    "            # 해당 단어가 w2v 모델에 있으면, 해당 벡터 값으로, 없으면 0벡터로 변환\n",
    "            return_array = np.array([self.w2v_model.wv.__getitem__(token) if self.w2v_model.wv.__contains__(token) else np.zeros((1,n_dim)) for token in doc[:sequence_length]])\n",
    "\n",
    "        return return_array.reshape(sequence_length, self.w2v_model.vector_size,-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_equal_sequence_doc_vectors_for_cnn(self, doc_ls, sequence_length = 10):\n",
    "        '''\n",
    "        복수 개의 문서(token list)를 Word2Vec으로 벡터화하여, CNN학습에 적합한 4D로 변환하는 함수입니다.\n",
    "\n",
    "        input\n",
    "        doc_ls : iterable, 토큰으로 구분된 array 형태의 문서가 저장된 리스트\n",
    "        sequence_length : int, 한 문서 당, 최대 토큰의 수\n",
    "        '''\n",
    "        \n",
    "        return np.array([self.generate_equal_sequence_doc_vector_for_cnn(doc, sequence_length) for doc in doc_ls])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def _init_weights(self,shape):\n",
    "        '''\n",
    "        CNN 학습을 위해, filter의 초기 weight를 주는 함수입니다.\n",
    "\n",
    "        inputs \n",
    "        shape = 4D-array, [batch, n_height, n_width, n_channel] \n",
    "        '''\n",
    "        return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 세팅을 시작합니다.\n",
      "사전에 학습된 Word2Vec 모델을 불러옵니다.\n",
      "Word2Vec 모델을 불러오는데 실패하였습니다.\n",
      "=================================================================================\n",
      "Default 세팅의 Word2Vec 모델을 새롭게 생성합니다.\n",
      "Process 진행에 앞서, Word2Vec 모델의 학습이 필요합니다.\n",
      "bulid_and_train_w2v_model 함수를 사용하여, word2vec 모델을 학습하시기 바랍니다\n",
      "=================================================================================\n",
      "Word2Vec의 Hyperparameter 튜닝을 원하신다면 self.w2v_model을 새롭게 생성한 모델로 덮어 쓰시면 됩니다. \n"
     ]
    }
   ],
   "source": [
    "w2v_cnn = Word2VecCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-27 18:43:36,815 : INFO : collecting all words and their counts\n",
      "2018-11-27 18:43:36,816 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-11-27 18:43:36,847 : INFO : PROGRESS: at sentence #10000, processed 85827 words, keeping 19987 word types\n",
      "2018-11-27 18:43:36,880 : INFO : PROGRESS: at sentence #20000, processed 169423 words, keeping 30435 word types\n",
      "2018-11-27 18:43:36,911 : INFO : PROGRESS: at sentence #30000, processed 254677 words, keeping 38961 word types\n",
      "2018-11-27 18:43:36,945 : INFO : PROGRESS: at sentence #40000, processed 340593 words, keeping 46346 word types\n",
      "2018-11-27 18:43:36,977 : INFO : PROGRESS: at sentence #50000, processed 424305 words, keeping 52595 word types\n",
      "2018-11-27 18:43:37,008 : INFO : PROGRESS: at sentence #60000, processed 508088 words, keeping 58313 word types\n",
      "2018-11-27 18:43:37,039 : INFO : PROGRESS: at sentence #70000, processed 591049 words, keeping 63628 word types\n",
      "2018-11-27 18:43:37,069 : INFO : PROGRESS: at sentence #80000, processed 675212 words, keeping 68644 word types\n",
      "2018-11-27 18:43:37,103 : INFO : PROGRESS: at sentence #90000, processed 760537 words, keeping 73599 word types\n",
      "2018-11-27 18:43:37,137 : INFO : PROGRESS: at sentence #100000, processed 843800 words, keeping 78087 word types\n",
      "2018-11-27 18:43:37,173 : INFO : PROGRESS: at sentence #110000, processed 927058 words, keeping 82423 word types\n",
      "2018-11-27 18:43:37,208 : INFO : PROGRESS: at sentence #120000, processed 1011790 words, keeping 86500 word types\n",
      "2018-11-27 18:43:37,244 : INFO : PROGRESS: at sentence #130000, processed 1096039 words, keeping 90486 word types\n",
      "2018-11-27 18:43:37,276 : INFO : PROGRESS: at sentence #140000, processed 1179611 words, keeping 94369 word types\n",
      "2018-11-27 18:43:37,309 : INFO : PROGRESS: at sentence #150000, processed 1263250 words, keeping 98016 word types\n",
      "2018-11-27 18:43:37,343 : INFO : PROGRESS: at sentence #160000, processed 1346832 words, keeping 101553 word types\n",
      "2018-11-27 18:43:37,376 : INFO : PROGRESS: at sentence #170000, processed 1430908 words, keeping 105036 word types\n",
      "2018-11-27 18:43:37,409 : INFO : PROGRESS: at sentence #180000, processed 1515842 words, keeping 108461 word types\n",
      "2018-11-27 18:43:37,442 : INFO : PROGRESS: at sentence #190000, processed 1600869 words, keeping 111918 word types\n",
      "2018-11-27 18:43:37,475 : INFO : collected 115189 word types from a corpus of 1686266 raw words and 199992 sentences\n",
      "2018-11-27 18:43:37,476 : INFO : Loading a fresh vocabulary\n",
      "2018-11-27 18:43:37,968 : INFO : effective_min_count=1 retains 115189 unique words (100% of original 115189, drops 0)\n",
      "2018-11-27 18:43:37,969 : INFO : effective_min_count=1 leaves 1686266 word corpus (100% of original 1686266, drops 0)\n",
      "2018-11-27 18:43:38,243 : INFO : deleting the raw counts dictionary of 115189 items\n",
      "2018-11-27 18:43:38,246 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2018-11-27 18:43:38,247 : INFO : downsampling leaves estimated 1572175 word corpus (93.2% of prior 1686266)\n",
      "2018-11-27 18:43:38,629 : INFO : estimated required memory for 115189 words and 100 dimensions: 149745700 bytes\n",
      "2018-11-27 18:43:38,629 : INFO : resetting layer weights\n",
      "2018-11-27 18:43:39,620 : INFO : training model with 3 workers on 115189 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-27 18:43:40,644 : INFO : EPOCH 1 - PROGRESS: at 52.75% examples, 820529 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-27 18:43:41,488 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-27 18:43:41,496 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-27 18:43:41,504 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-27 18:43:41,506 : INFO : EPOCH - 1 : training on 1686266 raw words (1572545 effective words) took 1.9s, 839755 effective words/s\n",
      "2018-11-27 18:43:42,529 : INFO : EPOCH 2 - PROGRESS: at 53.37% examples, 829540 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-27 18:43:43,367 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-27 18:43:43,375 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-27 18:43:43,376 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-27 18:43:43,376 : INFO : EPOCH - 2 : training on 1686266 raw words (1572387 effective words) took 1.9s, 846063 effective words/s\n",
      "2018-11-27 18:43:44,388 : INFO : EPOCH 3 - PROGRESS: at 52.75% examples, 825983 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-27 18:43:45,222 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-27 18:43:45,230 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-27 18:43:45,231 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-27 18:43:45,232 : INFO : EPOCH - 3 : training on 1686266 raw words (1572211 effective words) took 1.8s, 850565 effective words/s\n",
      "2018-11-27 18:43:46,245 : INFO : EPOCH 4 - PROGRESS: at 51.54% examples, 805990 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-27 18:43:47,093 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-27 18:43:47,105 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-27 18:43:47,105 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-27 18:43:47,106 : INFO : EPOCH - 4 : training on 1686266 raw words (1572141 effective words) took 1.9s, 842279 effective words/s\n",
      "2018-11-27 18:43:48,121 : INFO : EPOCH 5 - PROGRESS: at 52.15% examples, 813590 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-27 18:43:48,979 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-27 18:43:48,989 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-27 18:43:48,996 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-27 18:43:48,997 : INFO : EPOCH - 5 : training on 1686266 raw words (1572153 effective words) took 1.9s, 834245 effective words/s\n",
      "2018-11-27 18:43:50,015 : INFO : EPOCH 6 - PROGRESS: at 53.37% examples, 829993 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-27 18:43:50,838 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-27 18:43:50,846 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-27 18:43:50,853 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-27 18:43:50,854 : INFO : EPOCH - 6 : training on 1686266 raw words (1572200 effective words) took 1.8s, 850251 effective words/s\n",
      "2018-11-27 18:43:51,864 : INFO : EPOCH 7 - PROGRESS: at 52.75% examples, 827400 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-27 18:43:52,690 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-27 18:43:52,697 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-27 18:43:52,705 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-27 18:43:52,705 : INFO : EPOCH - 7 : training on 1686266 raw words (1572429 effective words) took 1.8s, 852679 effective words/s\n",
      "2018-11-27 18:43:53,718 : INFO : EPOCH 8 - PROGRESS: at 52.75% examples, 824598 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-27 18:43:54,543 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-27 18:43:54,551 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-27 18:43:54,561 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-27 18:43:54,562 : INFO : EPOCH - 8 : training on 1686266 raw words (1572480 effective words) took 1.8s, 850255 effective words/s\n",
      "2018-11-27 18:43:55,590 : INFO : EPOCH 9 - PROGRESS: at 53.37% examples, 827151 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-27 18:43:56,418 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-27 18:43:56,427 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-27 18:43:56,431 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-27 18:43:56,432 : INFO : EPOCH - 9 : training on 1686266 raw words (1572036 effective words) took 1.9s, 847135 effective words/s\n",
      "2018-11-27 18:43:57,445 : INFO : EPOCH 10 - PROGRESS: at 52.15% examples, 815393 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-27 18:43:58,301 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-27 18:43:58,309 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-27 18:43:58,311 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-27 18:43:58,311 : INFO : EPOCH - 10 : training on 1686266 raw words (1572434 effective words) took 1.9s, 840102 effective words/s\n",
      "2018-11-27 18:43:58,312 : INFO : training on a 16862660 raw words (15723016 effective words) took 18.7s, 841197 effective words/s\n"
     ]
    }
   ],
   "source": [
    "w2v_cnn.build_and_train_w2v_model(train_token_ls + test_token_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 20, 100, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_cnn.generate_equal_sequence_doc_vectors_for_cnn(train_token_ls[:3], sequence_length = 20).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습을 위한 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = w2v_cnn.generate_equal_sequence_doc_vectors_for_cnn(train_token_ls, sequence_length=sequence_length)\n",
    "test_X = w2v_cnn.generate_equal_sequence_doc_vectors_for_cnn(test_token_ls, sequence_length=sequence_length)\n",
    "\n",
    "train_Y = pd.get_dummies(train_df['label']).values.astype('float32')\n",
    "test_Y = pd.get_dummies(test_df['label']).values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def graph(X,\n",
    "          p_keep_conv, \n",
    "          p_keep_hidden, \n",
    "          filter_size_ls = [2,3,4], \n",
    "          num_filter = 2, \n",
    "          sequence_length = 10 ,\n",
    "          n_fc = 10, \n",
    "          n_class= 2,\n",
    "          n_dim = 100):\n",
    "\n",
    "    max_pool_result_ls = []\n",
    "    \n",
    "    # 각각의 사이즈의 필터를 num_filter개 생성하여 convolution & max_pool\n",
    "    for filter_size in filter_size_ls:\n",
    "\n",
    "        filter_ = init_weights([filter_size, n_dim, 1, num_filter])\n",
    "        \n",
    "        \n",
    "        # l1_conv shape=(?, sequence_length - filter_size + 1, n_dim, num_filter)\n",
    "        l1_conv = tf.nn.relu(tf.nn.conv2d(input = X,\n",
    "                                      filter = filter_,  \n",
    "                                      strides=[1, 1, 1, 1], \n",
    "                                      padding='VALID')) \n",
    "        # l1_pool shape=(?, 1, 1, num_filter)\n",
    "        l1_pool = tf.nn.max_pool(l1_conv, \n",
    "                            ksize=[1, sequence_length - filter_size + 1, 1, 1], \n",
    "                            strides=[1, 1, 1, 1], \n",
    "                            padding='VALID')\n",
    "\n",
    "        l1_pool = tf.nn.dropout(l1_pool, p_keep_conv)\n",
    "\n",
    "        max_pool_result_ls.append(l1_pool)\n",
    "\n",
    "    # 각기 다른 종류의 필터를 거쳐 conv-pool한 결과를 concat\n",
    "    num_filter_total = num_filter * len(filter_size_ls)\n",
    "    max_pool_concat = tf.concat(max_pool_result_ls, 3)\n",
    "    max_pool_concat_flat = tf.reshape(max_pool_concat, [-1, num_filter_total])\n",
    "\n",
    "    # fully-connect\n",
    "    w_fc = init_weights([num_filter_total, n_fc])\n",
    "    w_output = init_weights([n_fc, n_class])\n",
    "\n",
    "    l = tf.nn.relu(tf.matmul(max_pool_concat_flat, w_fc))\n",
    "    l = tf.nn.dropout(l, p_keep_hidden)\n",
    "\n",
    "    h = tf.matmul(l, w_output)\n",
    "    return h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = sequence_length\n",
    "n_class = train_Y.shape[1]\n",
    "\n",
    "batch_size = 100\n",
    "test_size = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\", [None, sequence_length, w2v_cnn.w2v_model.vector_size, 1])\n",
    "Y = tf.placeholder(\"float\", [None, n_class])\n",
    "\n",
    "p_keep_conv = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "h = graph(X, p_keep_conv, p_keep_hidden, sequence_length= sequence_length)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=h, labels=Y))\n",
    "train = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "predict = tf.argmax(h, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(10):\n",
    "        training_batch = zip(range(0, len(train_X), batch_size),\n",
    "                             range(batch_size, len(train_X)+1, batch_size))\n",
    "        \n",
    "        for start, end in training_batch:\n",
    "            sess.run(train, \n",
    "                     feed_dict={X: train_X[start:end], \n",
    "                                Y: train_Y[start:end],\n",
    "                                p_keep_conv: 0.8, \n",
    "                                p_keep_hidden: 0.5})\n",
    "\n",
    "        test_indices = np.arange(len(test_X)) # Get A Test Batch\n",
    "        np.random.shuffle(test_indices)\n",
    "        test_indices = test_indices[0:test_size]\n",
    "\n",
    "        print(i, np.mean(np.argmax(test_Y[test_indices], axis=1) ==\n",
    "                         sess.run(predict, \n",
    "                                  feed_dict={X: test_X[test_indices],\n",
    "                                                 Y: test_Y[test_indices],\n",
    "                                                 p_keep_conv: 1.0,\n",
    "                                                 p_keep_hidden: 1.0})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
