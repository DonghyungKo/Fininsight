{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unset PYTHONPATH first\n",
    "from ko_text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = NLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = pd.read_csv('Data/meta_morphs_final.csv', encoding = 'utf-8')\n",
    "\n",
    "# 용량을 줄이기 위해 '단어 단어' 꼴로 묶어둔 token을 ['단어', '단어'] 꼴로 풀기\n",
    "token_df['Token'] = [token.split() for token in token_df['Token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Text</th>\n",
       "      <th>Token</th>\n",
       "      <th>Num of Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>financial</td>\n",
       "      <td>\\n\\n\\n텀블벅에서 크라우드 펀딩이 이뤄지고 있는 `아침달 시집`.\\n\\n    ...</td>\n",
       "      <td>[텀블벅, 크라, 우드, 펀딩, 이뤄지고, 아침, 시집, 많지, 않은, 금액, 으로...</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>economy</td>\n",
       "      <td>\\n\\n\\n[사진 제공: 연합뉴스]\\n\\n                     유류...</td>\n",
       "      <td>[유류, 인하, 국제, 유가, 급락, 입어, 국내, 휘발유, 경유, 하락, 특히, ...</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>financial</td>\n",
       "      <td>부득이한 사정으로 매월 내는 보험료가 부담이 될 때 계약은 그대로 유지하면서 보험...</td>\n",
       "      <td>[부득이, 사정, 매월, 내는, 보험료, 부담, 계약, 그대로, 유지, 보험료, 부...</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>estate</td>\n",
       "      <td>한때 `미분양의 늪`으로 통하던 경기도 파주시 부동산 시장이 달라지고 있다. 지난해...</td>\n",
       "      <td>[한때, 미분, 하던, 경기도, 파주시, 부동산, 시장, 달라지고, 분양, 파주, ...</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>economy</td>\n",
       "      <td>\\n\\n\\n인디고뱅크의 `미키인서울` 컬래버 맨투맨  &lt;사진제공=월트디즈니코리아&gt;\\...</td>\n",
       "      <td>[인디고, 뱅크, 미키, 서울, 컬래버, 투맨, 월트디즈니, 사의, 마스코트, 미키...</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Section                                               Text  \\\n",
       "0  financial  \\n\\n\\n텀블벅에서 크라우드 펀딩이 이뤄지고 있는 `아침달 시집`.\\n\\n    ...   \n",
       "1    economy  \\n\\n\\n[사진 제공: 연합뉴스]\\n\\n                     유류...   \n",
       "2  financial   부득이한 사정으로 매월 내는 보험료가 부담이 될 때 계약은 그대로 유지하면서 보험...   \n",
       "3     estate  한때 `미분양의 늪`으로 통하던 경기도 파주시 부동산 시장이 달라지고 있다. 지난해...   \n",
       "4    economy  \\n\\n\\n인디고뱅크의 `미키인서울` 컬래버 맨투맨  <사진제공=월트디즈니코리아>\\...   \n",
       "\n",
       "                                               Token  Num of Tokens  \n",
       "0  [텀블벅, 크라, 우드, 펀딩, 이뤄지고, 아침, 시집, 많지, 않은, 금액, 으로...            263  \n",
       "1  [유류, 인하, 국제, 유가, 급락, 입어, 국내, 휘발유, 경유, 하락, 특히, ...            166  \n",
       "2  [부득이, 사정, 매월, 내는, 보험료, 부담, 계약, 그대로, 유지, 보험료, 부...            314  \n",
       "3  [한때, 미분, 하던, 경기도, 파주시, 부동산, 시장, 달라지고, 분양, 파주, ...            165  \n",
       "4  [인디고, 뱅크, 미키, 서울, 컬래버, 투맨, 월트디즈니, 사의, 마스코트, 미키...            196  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41418, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습을 위한 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = round(len(token_df) * 0.8)\n",
    "np.random.seed(0)\n",
    "train_index_ls = np.random.choice(token_df.index, train_size, replace = False)\n",
    "test_index_ls = [x for x in token_df.index if not x in train_index_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33134, 4) (8284, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df = token_df.loc[train_index_ls]\n",
    "test_df = token_df.loc[test_index_ls]\n",
    "\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'bio & tech': 1739,\n",
       "         'business': 4863,\n",
       "         'culture & art': 4102,\n",
       "         'economy': 2608,\n",
       "         'estate': 3932,\n",
       "         'financial': 746,\n",
       "         'it': 1742,\n",
       "         'politics': 3775,\n",
       "         'society': 3356,\n",
       "         'stock': 2508,\n",
       "         'world': 3763})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_df['Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_ls = train_df['Token'].tolist()\n",
    "train_label_ls = train_df['Section'].tolist()\n",
    "\n",
    "test_token_ls = test_df['Token'].tolist()\n",
    "test_label_ls = test_df['Section'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_ls, train_label_ls = nlp.oversample_batch(train_token_ls, train_label_ls, 3000)\n",
    "test_token_ls, test_label_ls = nlp.undersample_batch(test_token_ls, test_label_ls, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'bio & tech': 3000,\n",
       "         'business': 3000,\n",
       "         'culture & art': 3000,\n",
       "         'economy': 3000,\n",
       "         'estate': 3000,\n",
       "         'financial': 3000,\n",
       "         'it': 3000,\n",
       "         'politics': 3000,\n",
       "         'society': 3000,\n",
       "         'stock': 3000,\n",
       "         'world': 3000})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_label_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'bio & tech': 100,\n",
       "         'business': 100,\n",
       "         'culture & art': 100,\n",
       "         'economy': 100,\n",
       "         'estate': 100,\n",
       "         'financial': 100,\n",
       "         'it': 100,\n",
       "         'politics': 100,\n",
       "         'society': 100,\n",
       "         'stock': 100,\n",
       "         'world': 100})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(test_label_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_txt(path_to_file):\n",
    "    txt_ls = []\n",
    "    label_ls = []\n",
    "\n",
    "    with open(path_to_file) as f:\n",
    "        for i, line in enumerate(f.readlines()[1:]):\n",
    "            id_num, txt, label = line.split('\\t')\n",
    "            txt_ls.append(txt)\n",
    "            label_ls.append(int(label.replace('\\n','')))\n",
    "    return txt_ls, label_ls\n",
    "\n",
    "\n",
    "def convert_word_to_idx(sents):\n",
    "    for sent in sents:\n",
    "        yield [w2i_dict[word] for word in sent]\n",
    "    return\n",
    "\n",
    "def convert_label_to_idx(labels):\n",
    "    for label in labels:\n",
    "        yield l2i_dict[label]\n",
    "    return\n",
    "\n",
    "\n",
    "def add_padding(sents, max_len):\n",
    "    for i, sent in enumerate(sents):\n",
    "        if len(sent)< max_len:\n",
    "            sents[i] += [pad] * (max_len - len(sent))\n",
    "    \n",
    "        elif len(sent) > max_len:\n",
    "            sents[i] = sent[:max_len]\n",
    "    \n",
    "    return sents\n",
    "\n",
    "def convert_to_variable(w2i_ls):\n",
    "    \n",
    "    var = Variable(torch.LongTensor(w2i_ls))\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i_dict = defaultdict(lambda : len(w2i_dict))\n",
    "pad = w2i_dict['<PAD>']\n",
    "\n",
    "l2i_dict = defaultdict(lambda : len(l2i_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(convert_word_to_idx(train_token_ls))\n",
    "x_test = list(convert_word_to_idx(test_token_ls))\n",
    "\n",
    "y_train = list(convert_label_to_idx(train_label_ls))\n",
    "y_test = list(convert_label_to_idx(test_label_ls))\n",
    "\n",
    "i2w_dict = {val : key for key, val in w2i_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_to_variable(add_padding(x_train, 100))\n",
    "x_test = convert_to_variable(add_padding(x_test, 100))\n",
    "\n",
    "y_train = convert_to_variable(y_train).float()\n",
    "y_test = convert_to_variable(y_test).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN 모델 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-train된 Word2Vec은 사용하지 않았습니다.\n",
    "\n",
    "모든 embedding은 랜덤으로 초기화된 상태로 학습을 진행하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_text(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_words, embed_size, hid_size, drop_rate, kernel_size_ls, num_filter, n_category):\n",
    "        super(CNN_text, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.hid_size = hid_size\n",
    "        self.drop_rate = drop_rate\n",
    "        self.num_filter = num_filter\n",
    "        self.kernel_size_ls = kernel_size_ls\n",
    "        self.num_kernel = len(kernel_size_ls)\n",
    "        self.n_category = n_category\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_words, embed_size)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, num_filter, (kernel_size, embed_size)) for kernel_size in kernel_size_ls])\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(self.num_kernel*num_filter, hid_size), nn.ReLU(), \n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.Linear(hid_size, n_category),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x) # [batch_size, max_length, embed_size]\n",
    "        embed.unsqueeze_(1)  # [batch_size, 1, max_length, embed_size]\n",
    "        conved = [conv(embed).squeeze(3) for conv in self.convs] # [batch_size, num_filter, max_length -kernel_size +1]\n",
    "        pooled = [F.max_pool1d(conv, (conv.size(2))).squeeze(2) for conv in conved] # [batch_size, num_kernel, num_filter]\n",
    "        \n",
    "        concated = torch.cat(pooled, dim = 1) # [batch_size, num_kernel * num_filter]\n",
    "        logit = self.lin(concated)\n",
    "        \n",
    "        return logit\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(w2i_dict)\n",
    "EMBED_SIZE = 32\n",
    "HID_SIZE = 32\n",
    "DROP_RATE = 0.5\n",
    "KERNEL_SIZE_LS = [3,4,5,6]\n",
    "NUM_FILTER = 8\n",
    "N_CATEGORY = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_text(n_words = n_words, embed_size =EMBED_SIZE, drop_rate= DROP_RATE,\n",
    "                 hid_size=HID_SIZE, kernel_size_ls= KERNEL_SIZE_LS, num_filter=NUM_FILTER,\n",
    "                 n_category = N_CATEGORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_text(\n",
       "  (embedding): Embedding(125094, 32)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 32), stride=(1, 1))\n",
       "    (1): Conv2d(1, 8, kernel_size=(4, 32), stride=(1, 1))\n",
       "    (2): Conv2d(1, 8, kernel_size=(5, 32), stride=(1, 1))\n",
       "    (3): Conv2d(1, 8, kernel_size=(6, 32), stride=(1, 1))\n",
       "  )\n",
       "  (lin): Sequential(\n",
       "    (0): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=32, out_features=11, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch : 1,  loss : 80997.953125,  accuracy :0.100\n",
      "=================================================================================================\n",
      "Train epoch : 2,  loss : 79136.38330078125,  accuracy :0.111\n",
      "=================================================================================================\n",
      "Train epoch : 3,  loss : 78471.46728515625,  accuracy :0.142\n",
      "=================================================================================================\n",
      "Train epoch : 4,  loss : 77977.2451171875,  accuracy :0.164\n",
      "=================================================================================================\n",
      "Train epoch : 5,  loss : 77340.4248046875,  accuracy :0.171\n",
      "=================================================================================================\n",
      "Test Epoch : 5, Test Loss : 2567.579 , Test Accuracy : 0.234\n",
      "Train epoch : 6,  loss : 76403.77685546875,  accuracy :0.198\n",
      "=================================================================================================\n",
      "Train epoch : 7,  loss : 75206.50537109375,  accuracy :0.211\n",
      "=================================================================================================\n",
      "Train epoch : 8,  loss : 73732.93994140625,  accuracy :0.238\n",
      "=================================================================================================\n",
      "Train epoch : 9,  loss : 71792.05029296875,  accuracy :0.257\n",
      "=================================================================================================\n",
      "Train epoch : 10,  loss : 69572.42919921875,  accuracy :0.295\n",
      "=================================================================================================\n",
      "Test Epoch : 10, Test Loss : 2284.812 , Test Accuracy : 0.308\n",
      "Train epoch : 11,  loss : 67571.33544921875,  accuracy :0.310\n",
      "=================================================================================================\n",
      "Train epoch : 12,  loss : 65121.134765625,  accuracy :0.307\n",
      "=================================================================================================\n",
      "Train epoch : 13,  loss : 63025.75390625,  accuracy :0.347\n",
      "=================================================================================================\n",
      "Train epoch : 14,  loss : 60838.65283203125,  accuracy :0.362\n",
      "=================================================================================================\n",
      "Train epoch : 15,  loss : 58469.55029296875,  accuracy :0.417\n",
      "=================================================================================================\n",
      "Test Epoch : 15, Test Loss : 1958.891 , Test Accuracy : 0.414\n",
      "Train epoch : 16,  loss : 56727.3466796875,  accuracy :0.409\n",
      "=================================================================================================\n",
      "Train epoch : 17,  loss : 54717.08056640625,  accuracy :0.432\n",
      "=================================================================================================\n",
      "Train epoch : 18,  loss : 52949.83740234375,  accuracy :0.466\n",
      "=================================================================================================\n",
      "Train epoch : 19,  loss : 51268.52001953125,  accuracy :0.458\n",
      "=================================================================================================\n",
      "Train epoch : 20,  loss : 49493.0751953125,  accuracy :0.498\n",
      "=================================================================================================\n",
      "Test Epoch : 20, Test Loss : 1768.670 , Test Accuracy : 0.465\n",
      "Train epoch : 21,  loss : 48032.0263671875,  accuracy :0.517\n",
      "=================================================================================================\n",
      "Train epoch : 22,  loss : 46829.86572265625,  accuracy :0.505\n",
      "=================================================================================================\n",
      "Train epoch : 23,  loss : 45582.76416015625,  accuracy :0.544\n",
      "=================================================================================================\n",
      "Train epoch : 24,  loss : 44302.306396484375,  accuracy :0.541\n",
      "=================================================================================================\n",
      "Train epoch : 25,  loss : 42699.08984375,  accuracy :0.565\n",
      "=================================================================================================\n",
      "Test Epoch : 25, Test Loss : 1671.475 , Test Accuracy : 0.496\n",
      "Train epoch : 26,  loss : 41606.546630859375,  accuracy :0.577\n",
      "=================================================================================================\n",
      "Train epoch : 27,  loss : 40593.742919921875,  accuracy :0.588\n",
      "=================================================================================================\n",
      "Train epoch : 28,  loss : 39431.462158203125,  accuracy :0.606\n",
      "=================================================================================================\n",
      "Train epoch : 29,  loss : 38500.170166015625,  accuracy :0.612\n",
      "=================================================================================================\n",
      "Train epoch : 30,  loss : 37531.457763671875,  accuracy :0.629\n",
      "=================================================================================================\n",
      "Test Epoch : 30, Test Loss : 1641.176 , Test Accuracy : 0.510\n",
      "Train epoch : 31,  loss : 36303.276611328125,  accuracy :0.637\n",
      "=================================================================================================\n",
      "Train epoch : 32,  loss : 35248.731689453125,  accuracy :0.664\n",
      "=================================================================================================\n",
      "Train epoch : 33,  loss : 34299.3681640625,  accuracy :0.665\n",
      "=================================================================================================\n",
      "Train epoch : 34,  loss : 33307.9794921875,  accuracy :0.658\n",
      "=================================================================================================\n",
      "Train epoch : 35,  loss : 32404.87255859375,  accuracy :0.675\n",
      "=================================================================================================\n",
      "Test Epoch : 35, Test Loss : 1657.354 , Test Accuracy : 0.539\n",
      "Train epoch : 36,  loss : 31531.207275390625,  accuracy :0.683\n",
      "=================================================================================================\n",
      "Train epoch : 37,  loss : 30824.60107421875,  accuracy :0.681\n",
      "=================================================================================================\n",
      "Train epoch : 38,  loss : 29951.159423828125,  accuracy :0.705\n",
      "=================================================================================================\n",
      "Train epoch : 39,  loss : 29324.611572265625,  accuracy :0.722\n",
      "=================================================================================================\n",
      "Train epoch : 40,  loss : 28342.983154296875,  accuracy :0.734\n",
      "=================================================================================================\n",
      "Test Epoch : 40, Test Loss : 1692.075 , Test Accuracy : 0.533\n",
      "Train epoch : 41,  loss : 27646.436279296875,  accuracy :0.719\n",
      "=================================================================================================\n",
      "Train epoch : 42,  loss : 26752.458984375,  accuracy :0.731\n",
      "=================================================================================================\n",
      "Train epoch : 43,  loss : 26192.26171875,  accuracy :0.742\n",
      "=================================================================================================\n",
      "Train epoch : 44,  loss : 25380.59619140625,  accuracy :0.757\n",
      "=================================================================================================\n",
      "Train epoch : 45,  loss : 24728.032958984375,  accuracy :0.749\n",
      "=================================================================================================\n",
      "Test Epoch : 45, Test Loss : 1778.009 , Test Accuracy : 0.529\n",
      "Train epoch : 46,  loss : 24090.76513671875,  accuracy :0.764\n",
      "=================================================================================================\n",
      "Train epoch : 47,  loss : 23433.136474609375,  accuracy :0.769\n",
      "=================================================================================================\n",
      "Train epoch : 48,  loss : 22868.8232421875,  accuracy :0.775\n",
      "=================================================================================================\n",
      "Train epoch : 49,  loss : 22405.412963867188,  accuracy :0.780\n",
      "=================================================================================================\n",
      "Train epoch : 50,  loss : 21682.628662109375,  accuracy :0.785\n",
      "=================================================================================================\n",
      "Test Epoch : 50, Test Loss : 1861.116 , Test Accuracy : 0.540\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "lr = 0.001\n",
    "batch_size = 3000\n",
    "\n",
    "train_idx = np.arange(x_train.size(0))\n",
    "test_idx = np.arange(x_test.size(0))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "loss_ls = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # input 데이터 순서 섞기\n",
    "    random.shuffle(train_idx)\n",
    "    x_train = x_train[train_idx]\n",
    "    y_train = y_train[train_idx]\n",
    "    train_loss = 0\n",
    "\n",
    "    for start_idx, end_idx in zip(range(0, x_train.size(0), batch_size),\n",
    "                                  range(batch_size, x_train.size(0)+1, batch_size)):\n",
    "        x_batch = x_train[start_idx : end_idx]\n",
    "        y_batch = y_train[start_idx : end_idx].long()\n",
    "        \n",
    "        scores = model(x_batch)\n",
    "        predict = F.softmax(scores, dim = 1).argmax(dim = 1)\n",
    "        \n",
    "        acc = (predict == y_batch).sum().item() / batch_size\n",
    "        \n",
    "        loss = criterion(scores, y_batch)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Train epoch : %s,  loss : %s,  accuracy :%.3f'%(epoch+1, train_loss, acc))\n",
    "    print('=================================================================================================')\n",
    "    \n",
    "    loss_ls.append(train_loss)\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        model.eval()\n",
    "        scores = model(x_test)\n",
    "        predict = F.softmax(scores, dim=1).argmax(dim = 1)\n",
    "        \n",
    "        acc = (predict == y_test.long()).sum().item() / len(y_test)\n",
    "        loss = criterion(scores, y_test.long())\n",
    "        \n",
    "        print('Test Epoch : %s, Test Loss : %.03f , Test Accuracy : %.03f'%(epoch+1, loss.item(), acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
