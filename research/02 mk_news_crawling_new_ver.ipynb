{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **meta tag의 세부 카테고리를 수집한 결과**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from multiprocessing import Process,Queue, Pool\n",
    "import functools\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import time\n",
    "\n",
    "class MKNewsCrawler(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 기존에 데이터가 없으면 새로 만들어서 크롤링\n",
    "\n",
    "        # 자료가 저장되는 형태\n",
    "        self.item = OrderedDict(\\\n",
    "                       {'Title' : [],\n",
    "                        'Text' : [],\n",
    "                        'Date' : [],\n",
    "                        'Section' : [],\n",
    "                        'Link' : [],\n",
    "                       })\n",
    "\n",
    "        #self._pool = Pool(processes = 8)\n",
    "\n",
    "\n",
    "\n",
    "    def make_link_map(self, start_num, end_num, year = 2018):\n",
    "        '''\n",
    "        크롤링할 link map을 만드는 함수입니다.\n",
    "\n",
    "        inputs\n",
    "        ======================\n",
    "        start_num : int,\n",
    "            크롤링을 시작하는 article_number\n",
    "\n",
    "        end_num : int,\n",
    "            크롤링을 마치는 article_number\n",
    "\n",
    "        year : int,\n",
    "            크롤링을 수행하는 기사년도\n",
    "\n",
    "        return\n",
    "        ==========================\n",
    "        link_map : list,\n",
    "            크롤링을 수행할 link가 저장된 리스트\n",
    "        '''\n",
    "\n",
    "        link_map_ls = []\n",
    "\n",
    "        for article_num in range(start_num, end_num):\n",
    "            url = 'http://news.mk.co.kr/newsRead.php?&year=%s&no=%s'%(year,article_num)\n",
    "            link_map_ls.append(url)\n",
    "\n",
    "        return link_map_ls\n",
    "\n",
    "\n",
    "\n",
    "    def _crawl_one_article(self, link):\n",
    "        '''\n",
    "        해당 링크에 request를 보내, html에서 css태그로 원하는 정보를 추출하는 함수입니다.\n",
    "\n",
    "        inputs\n",
    "        ====================\n",
    "        link : str\n",
    "         - 기사 원문이 있는 url\n",
    "        '''\n",
    "        try:\n",
    "            req = requests.get(link)\n",
    "            soup = BeautifulSoup(req.content.decode('euc-kr','replace'), 'html.parser')\n",
    "\n",
    "            title = soup.select('#top_header > div > div > h1')[0].get_text()\n",
    "            text = soup.select('#article_body > div')[0].get_text()\n",
    "            date = int(soup.find('meta', {'property' :'article:published'})['content'].replace('-',''))\n",
    "\n",
    "            # section의 분류의 경우,\n",
    "            # html을 뜯어보면 세부 카테고리가 존재함..\n",
    "            section = soup.find('meta', {'name' :'classification'})['content']\n",
    "            return title, text, date, section, link\n",
    "        except:\n",
    "            return '','','','',''\n",
    "\n",
    "\n",
    "\n",
    "    def crawl_process(self, link_map, queue = False):\n",
    "        '''\n",
    "        입력받은 link_map에서 개별 링크에서 정보를 수집하는 함수입니다.\n",
    "\n",
    "        inputs\n",
    "        ======================\n",
    "        link_map : list,\n",
    "            크롤링할 링크가 담겨있는 리스트\n",
    "\n",
    "        queue : Queue, [optional]\n",
    "            병렬처리를 위한 Queue\n",
    "\n",
    "        return\n",
    "        ======================\n",
    "\n",
    "        '''\n",
    "        temp_dict = self.item.copy()\n",
    "\n",
    "        # 링크 맵을 돌면서 기사를 하나씩 수집\n",
    "        for link in link_map:\n",
    "            title, text, date, section, link = self._crawl_one_article(link)\n",
    "\n",
    "            if not title == '':\n",
    "                temp_dict['Title'].append(title)\n",
    "                temp_dict['Text'].append(text)\n",
    "                temp_dict['Date'].append(date)\n",
    "                temp_dict['Section'].append(section)\n",
    "                temp_dict['Link'].append(link)\n",
    "\n",
    "        # 병렬처리를 위해 queue에 쌓음\n",
    "        if queue:\n",
    "            queue.put(temp_dict)\n",
    "\n",
    "        print('batch done!')\n",
    "        return temp_dict\n",
    "\n",
    "\n",
    "\n",
    "    def multiprocess_crawling(self, link_map):\n",
    "        '''\n",
    "        입력받은 link_map에서 개별 링크에서 정보를 수집하는 함수입니다.\n",
    "        병렬처리가 적용되었습니다. [os.core_count() * 2]\n",
    "\n",
    "        inputs\n",
    "        ======================\n",
    "        link_map : list,\n",
    "            크롤링할 링크가 담겨있는 리스트\n",
    "\n",
    "        queue : Queue, [optional]\n",
    "            병렬처리를 위한 Queue\n",
    "\n",
    "        return\n",
    "        ======================\n",
    "\n",
    "        '''\n",
    "        queue_ls = []\n",
    "        procs = []\n",
    "\n",
    "        result_dict = self.item.copy()\n",
    "\n",
    "        n_batch = 100\n",
    "        if n_batch < len(link_map):\n",
    "            batch_size = len(link_map)// (n_batch)\n",
    "        else:\n",
    "            batch_size = 1\n",
    "        print('batch size : %s'%batch_size)\n",
    "\n",
    "        # process에 작업들을 할당\n",
    "        for i, idx in enumerate(range(0, len(link_map), batch_size)):\n",
    "            try:\n",
    "                batch_link_map = link_map[idx : idx + batch_size]\n",
    "            except:\n",
    "                batch_link_map = link_map[idx :]\n",
    "\n",
    "            queue_ls.append(Queue())\n",
    "            proc = Process(target=functools.partial(\n",
    "                                                    self.crawl_process,\n",
    "                                                    queue = queue_ls[i],\n",
    "                                                    link_map= batch_link_map))\n",
    "            procs.append(proc)\n",
    "            proc.start()\n",
    "\n",
    "        for queue in queue_ls:\n",
    "            temp_result_dict = queue.get()\n",
    "\n",
    "            for key in result_dict.keys():\n",
    "                result_dict[key] += temp_result_dict[key]\n",
    "\n",
    "        for proc in procs:\n",
    "            proc.join()\n",
    "            #proc.close()\n",
    "\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    start_time = time.time()\n",
    "\n",
    "    mk_crawler = MKNewsCrawler()\n",
    "    start_num = int(input('크롤링을 시작할 article number를 입력하세요 :  '))\n",
    "    end_num = int(input('크롤링을 마칠 article number를 입력하세요 :  '))\n",
    "    year = int(input('크롤링할 년도를 입력하세요 (YYYY) :  '))\n",
    "\n",
    "    link_map = mk_crawler.make_link_map(start_num, end_num, year = year)\n",
    "    result_dict = mk_crawler.multiprocess_crawling(link_map)\n",
    "    \n",
    "    data = pd.DataFrame(result_dict)\n",
    "    data.to_csv('../data/MK_%s_No_%s_to_%s.csv'%(year,start_num,end_num))\n",
    "\n",
    "    print('총 소요시간 : %s'%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Section</th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20181202</td>\n",
       "      <td>economy</td>\n",
       "      <td>\\n\\n\\n\\n                     영조의 탕평비에는 `주이불비 군...</td>\n",
       "      <td>반기업·친노동 지나쳐 `탕평경제`로 전환해야</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20181202</td>\n",
       "      <td>economy</td>\n",
       "      <td>\\n\\n\\n\\n                     한국 경제의 버팀목인 반도체 수...</td>\n",
       "      <td>반도체 수출증가율 1년새 `5분의1토막`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20181202</td>\n",
       "      <td>economy</td>\n",
       "      <td>우리나라 성인 5명 중 한 명은 \"과거와 현재, 그리고 미래가 모두 불행할 것\"이라...</td>\n",
       "      <td>성인 5명중 1명 \"현재 불행…미래도 희망없다\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20181202</td>\n",
       "      <td>financial</td>\n",
       "      <td>\\n\\n\\n\\n                     한국은행이 기준금리를 0.25%...</td>\n",
       "      <td>은행 대출금리 인상 본격 스타트</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20181202</td>\n",
       "      <td>financial</td>\n",
       "      <td>금융당국이 외부 전문가들에게 의뢰한 조사에서 우리나라 은행업이 과당경쟁 상황은 아니...</td>\n",
       "      <td>정부, 인터넷은행 신규설립 `군불 때기`</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date    Section                                               Text  \\\n",
       "0  20181202    economy  \\n\\n\\n\\n                     영조의 탕평비에는 `주이불비 군...   \n",
       "1  20181202    economy  \\n\\n\\n\\n                     한국 경제의 버팀목인 반도체 수...   \n",
       "2  20181202    economy  우리나라 성인 5명 중 한 명은 \"과거와 현재, 그리고 미래가 모두 불행할 것\"이라...   \n",
       "3  20181202  financial  \\n\\n\\n\\n                     한국은행이 기준금리를 0.25%...   \n",
       "4  20181202  financial  금융당국이 외부 전문가들에게 의뢰한 조사에서 우리나라 은행업이 과당경쟁 상황은 아니...   \n",
       "\n",
       "                        Title  \n",
       "0    반기업·친노동 지나쳐 `탕평경제`로 전환해야  \n",
       "1      반도체 수출증가율 1년새 `5분의1토막`  \n",
       "2  성인 5명중 1명 \"현재 불행…미래도 희망없다\"  \n",
       "3           은행 대출금리 인상 본격 스타트  \n",
       "4      정부, 인터넷은행 신규설립 `군불 때기`  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214829"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(result_dict['Title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224993, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('business', 27150),\n",
       " ('politics', 25117),\n",
       " ('estate', 24954),\n",
       " ('world', 24370),\n",
       " ('society', 22165),\n",
       " ('economy', 20410),\n",
       " ('culture', 19616),\n",
       " ('stock', 13035),\n",
       " ('it', 10919),\n",
       " ('health', 6545),\n",
       " ('retail', 5764),\n",
       " ('financial', 4516),\n",
       " ('broadcasting', 4091),\n",
       " ('', 3908),\n",
       " ('patent', 3503),\n",
       " ('performance', 2680),\n",
       " ('electronics', 2552),\n",
       " ('autos', 1493),\n",
       " ('chemistry', 833),\n",
       " ('movie', 615),\n",
       " ('heavy_industries', 455),\n",
       " ('entertainment', 297),\n",
       " ('people', 5)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(data['Section']).most_common()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
