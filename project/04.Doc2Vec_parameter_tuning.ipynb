{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unset PYTHONPATH first\n",
    "from ko_text import *\n",
    "from ko_crawler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = NLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Data/morphs/train_morphs_final.csv', encoding = 'utf-8')\n",
    "test_df = pd.read_csv('Data/morphs/test_morphs_final.csv', encoding = 'utf-8')\n",
    "\n",
    "# 용량을 줄이기 위해 '단어 단어' 꼴로 묶어둔 token을 ['단어', '단어'] 꼴로 풀기\n",
    "train_df['Token'] = [token.split() for token in train_df['Token']]\n",
    "test_df['Token'] = [token.split() for token in test_df['Token']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_df = pd.read_csv('Data/nouns/train_nouns_final.csv', encoding = 'utf-8')\n",
    "test_df = pd.read_csv('Data/nouns/test_nouns_final.csv', encoding = 'utf-8')\n",
    "\n",
    "# 용량을 줄이기 위해 '단어 단어' 꼴로 묶어둔 token을 ['단어', '단어'] 꼴로 풀기\n",
    "train_df['Token'] = [token.split() for token in train_df['Token']]\n",
    "test_df['Token'] = [token.split() for token in test_df['Token']]'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82963, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'IT과학': 9836,\n",
       "         '경제': 8050,\n",
       "         '국제': 9677,\n",
       "         '기업': 9604,\n",
       "         '문화': 9313,\n",
       "         '부동산': 9714,\n",
       "         '사회': 9325,\n",
       "         '정치': 8875,\n",
       "         '증권': 8569})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_df['Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Section</th>\n",
       "      <th>Num of Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[농협, 금융, 지주, 김광수, 회장, 서대문, 본사, 에서, 농협, 금융, 자회사...</td>\n",
       "      <td>경제</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[코스피, 붕괴하면서, 달러, 환율, 으로, 뛰었다, 서울, 외환시장, 에서, 달러...</td>\n",
       "      <td>경제</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[정의당, 국회, 롯데, 갑질, 피해자, 간담, 개최, 김상조, 정거, 위원장, 공...</td>\n",
       "      <td>경제</td>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[코스피, 무역, 전쟁, 내외, 악재, 힘없이, 내줬다, 코스피, 거래, 일보, 포...</td>\n",
       "      <td>경제</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[제주항공, 호텔, 개점, 분석, 고객, 자유, 여행객, 제주항공, 올해, 서울, ...</td>\n",
       "      <td>경제</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Token Section  Num of Tokens\n",
       "0  [농협, 금융, 지주, 김광수, 회장, 서대문, 본사, 에서, 농협, 금융, 자회사...      경제            103\n",
       "1  [코스피, 붕괴하면서, 달러, 환율, 으로, 뛰었다, 서울, 외환시장, 에서, 달러...      경제            218\n",
       "2  [정의당, 국회, 롯데, 갑질, 피해자, 간담, 개최, 김상조, 정거, 위원장, 공...      경제            226\n",
       "3  [코스피, 무역, 전쟁, 내외, 악재, 힘없이, 내줬다, 코스피, 거래, 일보, 포...      경제            172\n",
       "4  [제주항공, 호텔, 개점, 분석, 고객, 자유, 여행객, 제주항공, 올해, 서울, ...      경제            169"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 label마다 학습할 단어의 수\n",
    "train_size_for_each_label = 1000\n",
    "test_size_for_each_label = 300\n",
    "\n",
    "n_class = len(train_df['Section'].unique())\n",
    "\n",
    "\n",
    "# Doc2Vec으로 vector를 추정하기 위한 split 과정\n",
    "train_batch_size = n_class * train_size_for_each_label\n",
    "test_batch_size = n_class * test_size_for_each_label\n",
    "\n",
    "train_token_ls_split, train_tag_ls_split = nlp.extract_a_equally_splited_batch(train_df['Token'], train_df['Section'], train_batch_size)\n",
    "test_token_ls_split, test_tag_ls_split =  nlp.extract_a_equally_splited_batch(test_df['Token'],test_df['Section'], test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 잘 뽑혔는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'증권 디스플레이 대해 중소 유기발광다이오드 실적 불확실 존재 한다며 목표 주가 에서 으로 내린다고 밝혔다 김동원 연구원 올해 부터 디스플레이 플라스틱 패널 신규 공급 시작 하기 위해 파주 공장 에서 중소 신규 라인 가동 이라고 예상 했다 이어 이는 내년 하반기 북미 전략 고객 신제품 수요 대응 하고 부터 시장 확대 예상 되는 더블 스마트폰 시장 전략 으로 대응 하기 으로 판단 된다고 덧붙였다 연구원 다만 향후 중소 라인 개선 강도 연간 규모 감가상각비 으로 실적 개선 불확실 상존 한다고 설명 했다'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_token_ls_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'기업'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tag_ls_split[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Parameter 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **set ALPHA as default**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result_dict = {\n",
    "                'corpus_count' : [],\n",
    "               'min_count' : [],\n",
    "               'vector_size' : [],\n",
    "               'window' : [],\n",
    "               'n_epochs' : [],\n",
    "               'accuracy' : [],\n",
    "               'sample' : [],\n",
    "               'dm' : [],\n",
    "              }\n",
    "\n",
    "testing_section_ls = ['경제','기업','사회','국제','부동산','증권','정치','IT과학','문화']\n",
    "\n",
    "# 하이퍼 파라미터 튜닝 작업 수행\n",
    "\n",
    "# 언더샘플링으로 학습\n",
    "undersampling_size = Counter(train_df['Section']).most_common()[-1][1]\n",
    "\n",
    "for dm in [1, 0]:\n",
    "    for size in [10000 * 9]:\n",
    "        x_split, y_split = nlp.extract_a_equally_splited_batch(train_df['Token'].tolist(), train_df['Section'].tolist(), size)\n",
    "        for sample in [1e-04, 1e-05, 1e-06]:\n",
    "            for min_count in [10, 50, 100]:\n",
    "                for vector_size in [100,300]:\n",
    "                    for window in [5,15]:\n",
    "                        for n_epochs in [10]:\n",
    "\n",
    "                            # Doc2Vec 모델 생성\n",
    "                            nlp.make_Doc2Vec_model(dm = dm,\n",
    "                                                   min_count = min_count,\n",
    "                                                   sample = sample,\n",
    "                                                   vector_size = vector_size,\n",
    "                                                   window = window,\n",
    "                                                   dm_mean = 0,\n",
    "                                                   dm_concat = 0)\n",
    "\n",
    "                            nlp.build_and_train_Doc2Vec_model(x_split,\n",
    "                                                              y_split,\n",
    "                                                              n_epochs = n_epochs)\n",
    "\n",
    "\n",
    "                            model_name = 'Doc2Vec_dm=%s&cc=%s&vs=%s&win=%s&min=%s&sample=%s&epochs=%s'%(\\\n",
    "                                                                                                                 nlp.Doc2Vec_model.dm,\n",
    "                                                                                                                   nlp.Doc2Vec_model.corpus_count,\n",
    "                                                                                                                   nlp.Doc2Vec_model.vector_size,\n",
    "                                                                                                                   nlp.Doc2Vec_model.window,\n",
    "                                                                                                                   nlp.Doc2Vec_model.min_count,\n",
    "                                                                                                                   nlp.Doc2Vec_model.sample,\n",
    "                                                                                                                   nlp.Doc2Vec_model.epochs)\n",
    "                            # Doc2Vec 모델 저장\n",
    "                            #nlp.Doc2Vec_model.save('Doc2Vec_model/'+model_name)\n",
    "\n",
    "\n",
    "                            '''\n",
    "\n",
    "                            X =nlp.infer_vectors_with_Doc2Vec(train_token_for_display[0])\n",
    "\n",
    "                            from sklearn.decomposition import PCA\n",
    "\n",
    "                            pca = PCA(n_components=2)\n",
    "                            X_pca = pca.fit_transform(X)\n",
    "                            scatter_df = pd.DataFrame(X_pca,\n",
    "                                                      index = train_tag_for_display[0],\n",
    "                                                      columns = ['x','y'])\n",
    "\n",
    "                            plt.figure(figsize = (15, 15))\n",
    "\n",
    "                            for section in testing_section_ls:\n",
    "                                temp_df = scatter_df[scatter_df.index == section]\n",
    "                                plt.scatter(temp_df['x'].values, temp_df['y'].values, label = section, c = np.random.rand(3,))\n",
    "\n",
    "                            plt.legend(loc = 'best')\n",
    "                            plt.savefig('Doc2Vec_model/images/'+model_name)\n",
    "\n",
    "                            '''\n",
    "                            # 각 레이블별 1000개씩 학습, \n",
    "\n",
    "                            # LR\n",
    "                            X_train = nlp.infer_vectors_with_Doc2Vec(train_token_ls_split)\n",
    "                            y_train = train_tag_ls_split\n",
    "\n",
    "                            X_test = nlp.infer_vectors_with_Doc2Vec(test_token_ls_split)\n",
    "                            y_test = test_tag_ls_split\n",
    "\n",
    "\n",
    "                            clf = LogisticRegression(solver = 'sag',\n",
    "                                                     multi_class = 'multinomial')\n",
    "\n",
    "\n",
    "                            clf.fit(X_train, y_train)\n",
    "                            y_pred = clf.predict(X_test)\n",
    "                            \n",
    "                            result_dict['dm'].append(nlp.Doc2Vec_model.dm)\n",
    "                            result_dict['corpus_count'].append(nlp.Doc2Vec_model.corpus_count)\n",
    "                            result_dict['min_count'].append(nlp.Doc2Vec_model.min_count)\n",
    "                            result_dict['vector_size'].append(nlp.Doc2Vec_model.vector_size)\n",
    "                            result_dict['window'].append(nlp.Doc2Vec_model.window)\n",
    "                            result_dict['n_epochs'].append(nlp.Doc2Vec_model.epochs)\n",
    "                            result_dict['sample'].append(nlp.Doc2Vec_model.sample)\n",
    "                            result_dict['accuracy'].append(accuracy_score(y_pred, y_test))\n",
    "\n",
    "                            print(accuracy_score(y_pred, y_test))\n",
    "\n",
    "\n",
    "            pd.DataFrame(result_dict).to_csv('Parameter_tuning_result.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.Doc2Vec_model.docvecs.vectors_docs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
