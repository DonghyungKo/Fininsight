{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unset PYTHONPATH first\n",
    "from ko_text import *\n",
    "from ko_crawler import *\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = NLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = pd.read_csv('Data/meta_morphs_final.csv', encoding = 'utf-8')\n",
    "\n",
    "# 용량을 줄이기 위해 '단어 단어' 꼴로 묶어둔 token을 ['단어', '단어'] 꼴로 풀기\n",
    "token_df['Token'] = [token.split() for token in token_df['Token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Text</th>\n",
       "      <th>Token</th>\n",
       "      <th>Num of Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>financial</td>\n",
       "      <td>\\n\\n\\n텀블벅에서 크라우드 펀딩이 이뤄지고 있는 `아침달 시집`.\\n\\n    ...</td>\n",
       "      <td>[텀블벅, 크라, 우드, 펀딩, 이뤄지고, 아침, 시집, 많지, 않은, 금액, 으로...</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>economy</td>\n",
       "      <td>\\n\\n\\n[사진 제공: 연합뉴스]\\n\\n                     유류...</td>\n",
       "      <td>[유류, 인하, 국제, 유가, 급락, 입어, 국내, 휘발유, 경유, 하락, 특히, ...</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>financial</td>\n",
       "      <td>부득이한 사정으로 매월 내는 보험료가 부담이 될 때 계약은 그대로 유지하면서 보험...</td>\n",
       "      <td>[부득이, 사정, 매월, 내는, 보험료, 부담, 계약, 그대로, 유지, 보험료, 부...</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>estate</td>\n",
       "      <td>한때 `미분양의 늪`으로 통하던 경기도 파주시 부동산 시장이 달라지고 있다. 지난해...</td>\n",
       "      <td>[한때, 미분, 하던, 경기도, 파주시, 부동산, 시장, 달라지고, 분양, 파주, ...</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>economy</td>\n",
       "      <td>\\n\\n\\n인디고뱅크의 `미키인서울` 컬래버 맨투맨  &lt;사진제공=월트디즈니코리아&gt;\\...</td>\n",
       "      <td>[인디고, 뱅크, 미키, 서울, 컬래버, 투맨, 월트디즈니, 사의, 마스코트, 미키...</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Section                                               Text  \\\n",
       "0  financial  \\n\\n\\n텀블벅에서 크라우드 펀딩이 이뤄지고 있는 `아침달 시집`.\\n\\n    ...   \n",
       "1    economy  \\n\\n\\n[사진 제공: 연합뉴스]\\n\\n                     유류...   \n",
       "2  financial   부득이한 사정으로 매월 내는 보험료가 부담이 될 때 계약은 그대로 유지하면서 보험...   \n",
       "3     estate  한때 `미분양의 늪`으로 통하던 경기도 파주시 부동산 시장이 달라지고 있다. 지난해...   \n",
       "4    economy  \\n\\n\\n인디고뱅크의 `미키인서울` 컬래버 맨투맨  <사진제공=월트디즈니코리아>\\...   \n",
       "\n",
       "                                               Token  Num of Tokens  \n",
       "0  [텀블벅, 크라, 우드, 펀딩, 이뤄지고, 아침, 시집, 많지, 않은, 금액, 으로...            263  \n",
       "1  [유류, 인하, 국제, 유가, 급락, 입어, 국내, 휘발유, 경유, 하락, 특히, ...            166  \n",
       "2  [부득이, 사정, 매월, 내는, 보험료, 부담, 계약, 그대로, 유지, 보험료, 부...            314  \n",
       "3  [한때, 미분, 하던, 경기도, 파주시, 부동산, 시장, 달라지고, 분양, 파주, ...            165  \n",
       "4  [인디고, 뱅크, 미키, 서울, 컬래버, 투맨, 월트디즈니, 사의, 마스코트, 미키...            196  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41418, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Word2Vec + CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "class Word2VecCNN():\n",
    "    \n",
    "    def __init__(self, path_to_word2vec_model = ''):\n",
    "        print('초기 세팅을 시작합니다.')\n",
    "        print('사전에 학습된 Word2Vec 모델을 불러옵니다.')\n",
    "        try: \n",
    "            self.w2v_model = Word2Vec.load(path_to_word2vec_model)\n",
    "            print('학습된 Word2Vec 모델을 성공적으로 불러왔습니다.')\n",
    "            \n",
    "        except : \n",
    "            self.w2v_model = Word2Vec(min_count = 1)\n",
    "            print('Word2Vec 모델을 불러오는데 실패하였습니다.')\n",
    "            print('=================================================================================')\n",
    "            print('Default 세팅의 Word2Vec 모델을 새롭게 생성합니다.')\n",
    "            print('Process 진행에 앞서, Word2Vec 모델의 학습이 필요합니다.')\n",
    "            print('bulid_and_train_w2v_model 함수를 사용하여, word2vec 모델을 학습하시기 바랍니다')\n",
    "            print('=================================================================================')\n",
    "            print('Word2Vec의 Hyper-parameter 튜닝을 원하신다면 self.w2v_model을 새롭게 생성한 모델로 덮어 쓰시면 됩니다. ')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def build_and_train_w2v_model(self, token_ls):\n",
    "        '''\n",
    "        Word2Vec 모델을 학습하는 함수입니다.\n",
    "        \n",
    "        #inputs   \n",
    "        token_ls : iterable, 문서가 토큰으로 구부된 형태로 저장된 리스트\n",
    "        \n",
    "        #return\n",
    "        모델 학습\n",
    "        '''\n",
    "        import logging\n",
    "        #logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "        self.w2v_model.build_vocab(token_ls)\n",
    "\n",
    "        self.w2v_model.train(token_ls,\n",
    "                            total_examples = self.w2v_model.corpus_count,\n",
    "                            epochs = 10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_w2v_model(self, path_to_model):\n",
    "        '''\n",
    "        사전에 학습한 Word2Vec 모델을 불러오는 함수입니다.\n",
    "        '''\n",
    "    \n",
    "        self.w2v_model = Word2Vec.load(path_to_model)\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_equal_sequence_doc_vector_for_cnn(self, doc, sequence_length = 10):\n",
    "        '''\n",
    "        CNN 학습을 위해, 모든 문서의 token 길이를 동일한 값(sequence_length)로 맞춰주는 함수입니다,\n",
    "        한 개의 문서(token list)를 Word2Vec으로 벡터화하여, CNN학습에 적합한 4D로 변환합니다.\n",
    "        문서의 토큰 수가 sequence_length보다 적은 경우, 부족한 만큼 zero padding을 추가합니다.\n",
    "\n",
    "        input\n",
    "        doc : iterable, 토큰으로 구분된 array 형태의 문서\n",
    "        w2v_model : word2vec_model, 개별 토큰을 벡터화하기 위한 word2vec 모델\n",
    "        sequence_length : int, 한 문서 당, 최대 토큰의 수\n",
    "        '''\n",
    "        n_dim = self.w2v_model.vector_size\n",
    "        \n",
    "        if len(doc) < 1:\n",
    "            return np.zeros((sequence_length,self.w2v_model.vector_size)).reshape(sequence_length, n_dim, -1)\n",
    "\n",
    "        elif len(doc) < sequence_length:\n",
    "            # 해당 단어가 w2v 모델에 있으면, 해당 벡터 값으로, 없으면 0벡터로 변환\n",
    "            return_array = np.array([self.w2v_model.wv.__getitem__(token) if self.w2v_model.wv.__contains__(token) else [0] * n_dim for token in doc])\n",
    "\n",
    "            # 길이가 짧은 문서는 0백터로 max_len의 크기에 맞도록 패딩을 해준다.\n",
    "            n_padding = sequence_length - len(doc)\n",
    "            return_array = np.concatenate((return_array, np.zeros((n_padding, n_dim))))\n",
    "\n",
    "\n",
    "        # 문서의 길이가 max_length보다 길면 앞에서 max_length의 토큰까지 짜른다.\n",
    "        elif len(doc) >= sequence_length:\n",
    "            # 해당 단어가 w2v 모델에 있으면, 해당 벡터 값으로, 없으면 0벡터로 변환\n",
    "            return_array = np.array([self.w2v_model.wv.__getitem__(token) if self.w2v_model.wv.__contains__(token) else [0] * n_dim for token in doc[:sequence_length]])\n",
    "        \n",
    "        return return_array.reshape(sequence_length, n_dim,-1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def generate_equal_sequence_doc_vectors_for_cnn(self, doc_ls, sequence_length = 10):\n",
    "        '''\n",
    "        복수 개의 문서(token list)를 Word2Vec으로 벡터화하여, CNN학습에 적합한 4D로 변환하는 함수입니다.\n",
    "\n",
    "        input\n",
    "        doc_ls : iterable, 토큰으로 구분된 array 형태의 문서가 저장된 리스트\n",
    "        sequence_length : int, 한 문서 당, 최대 토큰의 수\n",
    "        '''\n",
    "        \n",
    "        return np.array([self.generate_equal_sequence_doc_vector_for_cnn(doc, sequence_length) for doc in doc_ls])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def _init_weights(self,shape):\n",
    "        '''\n",
    "        CNN 학습을 위해, filter의 초기 weight를 주는 함수입니다.\n",
    "\n",
    "        inputs \n",
    "        shape = 4D-array, [batch, n_height, n_width, n_channel] \n",
    "        '''\n",
    "        return tf.Variable(tf.random_normal(shape, stddev=0.01))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "초기 세팅을 시작합니다.\n",
      "사전에 학습된 Word2Vec 모델을 불러옵니다.\n",
      "학습된 Word2Vec 모델을 성공적으로 불러왔습니다.\n"
     ]
    }
   ],
   "source": [
    "w2v_cnn = Word2VecCNN('word2vec_model/w2v_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w2v_cnn.w2v_model = Word2Vec(min_count = 5, \n",
    "                             window = 5,\n",
    "                             sample = 1e-5,\n",
    "                             size = 100,\n",
    "                             workers = 4,\n",
    "                             iter = 30,\n",
    "                             sg = 1,\n",
    "                             hs = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_cnn.build_and_train_w2v_model(token_df['Token'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('지능', 0.8219226002693176),\n",
       " ('인공', 0.7812213897705078),\n",
       " ('음성인식', 0.7169848680496216),\n",
       " ('사물인터넷', 0.7135927677154541),\n",
       " ('어시스턴트', 0.7003586292266846),\n",
       " ('스피커', 0.6795388460159302),\n",
       " ('자연어', 0.6697943210601807),\n",
       " ('IoT', 0.6628202199935913),\n",
       " ('플랫폼', 0.6612290740013123),\n",
       " ('데이터', 0.6600569486618042)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_cnn.w2v_model.most_similar('AI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('도널드', 0.9513905048370361),\n",
       " ('행정부', 0.8262176513671875),\n",
       " ('백악관', 0.808573305606842),\n",
       " ('대통령', 0.7891108989715576),\n",
       " ('워싱턴포스트', 0.7868554592132568),\n",
       " ('WP', 0.7828512787818909),\n",
       " ('중간', 0.7472510933876038),\n",
       " ('이뤄졌다는데', 0.7459768652915955),\n",
       " ('미국', 0.7260775566101074),\n",
       " ('하원', 0.6922484636306763)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_cnn.w2v_model.most_similar('트럼프')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('정책', 0.8150776028633118),\n",
       " ('경제성장', 0.8124768733978271),\n",
       " ('기조', 0.7947652339935303),\n",
       " ('경제정책', 0.7556207180023193),\n",
       " ('보호무역', 0.7061445713043213),\n",
       " ('대외', 0.7056616544723511),\n",
       " ('심화', 0.695213794708252),\n",
       " ('경제위기', 0.6881315112113953),\n",
       " ('홍장표', 0.6862689256668091),\n",
       " ('포용', 0.6845958232879639)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_cnn.w2v_model.most_similar('경제')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 저장\n",
    "w2v_cnn.w2v_model.save('word2vec_model/w2v_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습을 위한 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = round(len(token_df) * 0.8)\n",
    "np.random.seed(0)\n",
    "train_index_ls = np.random.choice(token_df.index, train_size, replace = False)\n",
    "test_index_ls = [x for x in token_df.index if not x in train_index_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33134, 4) (8284, 4)\n"
     ]
    }
   ],
   "source": [
    "train_df = token_df.loc[train_index_ls]\n",
    "test_df = token_df.loc[test_index_ls]\n",
    "\n",
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'bio & tech': 1739,\n",
       "         'business': 4863,\n",
       "         'culture & art': 4102,\n",
       "         'economy': 2608,\n",
       "         'estate': 3932,\n",
       "         'financial': 746,\n",
       "         'it': 1742,\n",
       "         'politics': 3775,\n",
       "         'society': 3356,\n",
       "         'stock': 2508,\n",
       "         'world': 3763})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_df['Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_ls = train_df['Token'].tolist()\n",
    "train_label_ls = train_df['Section'].tolist()\n",
    "\n",
    "test_token_ls = test_df['Token'].tolist()\n",
    "test_label_ls = test_df['Section'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_token_ls, train_label_ls = nlp.oversample_batch(train_token_ls, train_label_ls, 3000)\n",
    "test_token_ls, test_label_ls = nlp.undersample_batch(test_token_ls, test_label_ls, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'bio & tech': 3000,\n",
       "         'business': 3000,\n",
       "         'culture & art': 3000,\n",
       "         'economy': 3000,\n",
       "         'estate': 3000,\n",
       "         'financial': 3000,\n",
       "         'it': 3000,\n",
       "         'politics': 3000,\n",
       "         'society': 3000,\n",
       "         'stock': 3000,\n",
       "         'world': 3000})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_label_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'bio & tech': 100,\n",
       "         'business': 100,\n",
       "         'culture & art': 100,\n",
       "         'economy': 100,\n",
       "         'estate': 100,\n",
       "         'financial': 100,\n",
       "         'it': 100,\n",
       "         'politics': 100,\n",
       "         'society': 100,\n",
       "         'stock': 100,\n",
       "         'world': 100})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(test_label_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec으로 문서 벡터화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = w2v_cnn.generate_equal_sequence_doc_vectors_for_cnn(train_token_ls, sequence_length=sequence_length)\n",
    "test_X = w2v_cnn.generate_equal_sequence_doc_vectors_for_cnn(test_token_ls, sequence_length=sequence_length)\n",
    "\n",
    "train_Y = pd.get_dummies(train_label_ls).values.astype('float32')\n",
    "test_Y = pd.get_dummies(test_label_ls).values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33000, 100, 100, 1), (1100, 100, 100, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = train_X.shape[2]\n",
    "n_class = len(set(test_label_ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = sequence_length\n",
    "n_class = train_Y.shape[1]\n",
    "\n",
    "batch_size = 100\n",
    "test_size = 300\n",
    "\n",
    "num_filter = 100\n",
    "n_fc = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_filter_weights(shape):\n",
    "    return tf.Variable(tf.random_normal(shape, stddev=0.1))\n",
    "\n",
    "def init_weights(shape, cnt):\n",
    "    return tf.get_variable('W%s'%cnt, shape = shape, initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def graph(X,\n",
    "          p_keep_conv, \n",
    "          p_keep_hidden, \n",
    "          filter_size_ls = [1,2,3,4,5], \n",
    "          num_filter = num_filter, \n",
    "          sequence_length = sequence_length,\n",
    "          n_fc = n_fc, \n",
    "          n_class= n_class,\n",
    "          n_dim = n_dim):\n",
    "\n",
    "    max_pool_result_ls = []\n",
    "    \n",
    "    # 각각의 사이즈의 필터를 num_filter개 생성하여 convolution & max_pool\n",
    "    for filter_size in filter_size_ls:\n",
    "\n",
    "        filter_ = init_filter_weights([filter_size, n_dim, 1, num_filter])\n",
    "        \n",
    "        \n",
    "        # l1_conv shape=(?, sequence_length - filter_size + 1, n_dim, num_filter)\n",
    "        l1_conv = tf.nn.relu(tf.nn.conv2d(input = X,\n",
    "                                      filter = filter_,  \n",
    "                                      strides=[1, 1, 1, 1], \n",
    "                                      padding='VALID')) \n",
    "        \n",
    "        # l1_pool shape=(?, 1, 1, num_filter)\n",
    "        l1_pool = tf.nn.max_pool(l1_conv, \n",
    "                            ksize=[1, sequence_length - filter_size + 1, 1, 1], \n",
    "                            strides=[1, 1, 1, 1], \n",
    "                            padding='VALID')\n",
    "\n",
    "        l1_pool = tf.nn.dropout(l1_pool, p_keep_conv)\n",
    "\n",
    "        max_pool_result_ls.append(l1_pool)\n",
    "\n",
    "    # 각기 다른 종류의 필터를 거쳐 conv-pool한 결과를 concat\n",
    "    num_filter_total = num_filter * len(filter_size_ls)\n",
    "    max_pool_concat = tf.concat(max_pool_result_ls, 3)\n",
    "    max_pool_concat_flat = tf.reshape(max_pool_concat, [-1, num_filter_total])\n",
    "\n",
    "    # fully-connect\n",
    "    w_fc = init_weights([num_filter_total, n_fc], 1)\n",
    "    w_output = init_weights([n_fc, n_class], 2)\n",
    "\n",
    "    logit = tf.nn.relu(tf.matmul(max_pool_concat_flat, w_fc))\n",
    "    logit = tf.nn.dropout(logit, p_keep_hidden)\n",
    "    logit = tf.matmul(logit, w_output) + b\n",
    "    \n",
    "    # softmax\n",
    "    hypothesis = tf.nn.softmax(logit)\n",
    "#    print(h)\n",
    "    return logit, hypothesis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, sequence_length, w2v_cnn.w2v_model.vector_size, 1])\n",
    "Y = tf.placeholder(\"float\", [None, n_class])\n",
    "\n",
    "b = init_filter_weights([n_class])\n",
    "p_keep_conv = tf.placeholder(\"float\")\n",
    "p_keep_hidden = tf.placeholder(\"float\")\n",
    "logit, hypothesis = graph(X, p_keep_conv, p_keep_hidden, sequence_length= sequence_length, n_class = n_class)\n",
    "\n",
    "lamb = 0.0001\n",
    "learning_rate = 0.0005\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=Y)) #+ lamb * tf.reduce_sum(tf.square(w_output))\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "predict = tf.argmax(hypothesis, 1)\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y,1), predict), dtype= tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, train_cost : 3.784, test_cost : 3.022\n",
      "Accuracy : 0.10818182\n",
      "Epoch : 1, train_cost : 1.96, test_cost : 3.539\n",
      "Accuracy : 0.122727275\n",
      "Epoch : 2, train_cost : 2.473, test_cost : 4.022\n",
      "Accuracy : 0.16181818\n",
      "Epoch : 3, train_cost : 1.972, test_cost : 4.367\n",
      "Accuracy : 0.16727273\n",
      "Epoch : 4, train_cost : 1.774, test_cost : 4.257\n",
      "Accuracy : 0.20181818\n",
      "Epoch : 5, train_cost : 1.885, test_cost : 4.685\n",
      "Accuracy : 0.22090909\n",
      "Epoch : 6, train_cost : 1.461, test_cost : 4.059\n",
      "Accuracy : 0.26181817\n",
      "Epoch : 7, train_cost : 1.365, test_cost : 3.653\n",
      "Accuracy : 0.2718182\n",
      "Epoch : 8, train_cost : 1.382, test_cost : 3.642\n",
      "Accuracy : 0.28545454\n",
      "Epoch : 9, train_cost : 1.361, test_cost : 3.365\n",
      "Accuracy : 0.30272728\n",
      "Epoch : 10, train_cost : 0.997, test_cost : 3.727\n",
      "Accuracy : 0.31272727\n",
      "Epoch : 11, train_cost : 1.12, test_cost : 3.746\n",
      "Accuracy : 0.3018182\n",
      "Epoch : 12, train_cost : 1.26, test_cost : 3.95\n",
      "Accuracy : 0.31363636\n",
      "Epoch : 13, train_cost : 1.195, test_cost : 3.542\n",
      "Accuracy : 0.3181818\n",
      "Epoch : 14, train_cost : 1.393, test_cost : 4.43\n",
      "Accuracy : 0.30545455\n",
      "Epoch : 15, train_cost : 1.558, test_cost : 3.439\n",
      "Accuracy : 0.3309091\n",
      "Epoch : 16, train_cost : 0.853, test_cost : 3.826\n",
      "Accuracy : 0.3181818\n",
      "Epoch : 17, train_cost : 1.022, test_cost : 3.466\n",
      "Accuracy : 0.34636363\n",
      "Epoch : 18, train_cost : 1.077, test_cost : 3.749\n",
      "Accuracy : 0.32454544\n",
      "Epoch : 19, train_cost : 1.017, test_cost : 3.502\n",
      "Accuracy : 0.32454544\n",
      "Epoch : 20, train_cost : 1.13, test_cost : 3.294\n",
      "Accuracy : 0.3572727\n",
      "Epoch : 21, train_cost : 0.961, test_cost : 3.401\n",
      "Accuracy : 0.38272727\n",
      "Epoch : 22, train_cost : 1.462, test_cost : 3.712\n",
      "Accuracy : 0.33727273\n",
      "Epoch : 23, train_cost : 1.006, test_cost : 3.044\n",
      "Accuracy : 0.35363635\n",
      "Epoch : 24, train_cost : 0.83, test_cost : 2.604\n",
      "Accuracy : 0.4090909\n",
      "Epoch : 25, train_cost : 0.968, test_cost : 2.261\n",
      "Accuracy : 0.43272728\n",
      "Epoch : 26, train_cost : 0.999, test_cost : 2.62\n",
      "Accuracy : 0.40727273\n",
      "Epoch : 27, train_cost : 0.847, test_cost : 3.141\n",
      "Accuracy : 0.40363637\n",
      "Epoch : 28, train_cost : 0.676, test_cost : 2.546\n",
      "Accuracy : 0.41545454\n",
      "Epoch : 29, train_cost : 1.039, test_cost : 2.262\n",
      "Accuracy : 0.4709091\n",
      "Epoch : 30, train_cost : 0.752, test_cost : 2.159\n",
      "Accuracy : 0.45181817\n",
      "Epoch : 31, train_cost : 0.843, test_cost : 2.707\n",
      "Accuracy : 0.43727273\n",
      "Epoch : 32, train_cost : 0.651, test_cost : 2.519\n",
      "Accuracy : 0.44727272\n",
      "Epoch : 33, train_cost : 0.667, test_cost : 1.907\n",
      "Accuracy : 0.48090908\n",
      "Epoch : 34, train_cost : 0.767, test_cost : 1.898\n",
      "Accuracy : 0.47545454\n",
      "Epoch : 35, train_cost : 0.755, test_cost : 1.973\n",
      "Accuracy : 0.45636365\n",
      "Epoch : 36, train_cost : 0.839, test_cost : 1.943\n",
      "Accuracy : 0.4709091\n",
      "Epoch : 37, train_cost : 0.812, test_cost : 1.714\n",
      "Accuracy : 0.49545455\n",
      "Epoch : 38, train_cost : 0.858, test_cost : 1.433\n",
      "Accuracy : 0.5518182\n",
      "Epoch : 39, train_cost : 0.773, test_cost : 1.44\n",
      "Accuracy : 0.5627273\n",
      "Epoch : 40, train_cost : 0.727, test_cost : 1.462\n",
      "Accuracy : 0.56545454\n",
      "Epoch : 41, train_cost : 0.615, test_cost : 1.484\n",
      "Accuracy : 0.5690909\n",
      "Epoch : 42, train_cost : 0.595, test_cost : 1.657\n",
      "Accuracy : 0.55636364\n",
      "Epoch : 43, train_cost : 0.582, test_cost : 1.508\n",
      "Accuracy : 0.58\n",
      "Epoch : 44, train_cost : 0.547, test_cost : 1.299\n",
      "Accuracy : 0.61545455\n",
      "Epoch : 45, train_cost : 0.632, test_cost : 1.33\n",
      "Accuracy : 0.6136364\n",
      "Epoch : 46, train_cost : 0.625, test_cost : 1.367\n",
      "Accuracy : 0.6009091\n",
      "Epoch : 47, train_cost : 0.568, test_cost : 1.379\n",
      "Accuracy : 0.60818183\n",
      "Epoch : 48, train_cost : 0.666, test_cost : 1.407\n",
      "Accuracy : 0.6036364\n",
      "Epoch : 49, train_cost : 0.537, test_cost : 1.353\n",
      "Accuracy : 0.62363636\n",
      "Epoch : 50, train_cost : 0.54, test_cost : 1.37\n",
      "Accuracy : 0.6318182\n",
      "Epoch : 51, train_cost : 0.606, test_cost : 1.371\n",
      "Accuracy : 0.63272727\n",
      "Epoch : 52, train_cost : 0.617, test_cost : 1.371\n",
      "Accuracy : 0.64\n",
      "Epoch : 53, train_cost : 0.627, test_cost : 1.413\n",
      "Accuracy : 0.63545454\n",
      "Epoch : 54, train_cost : 0.661, test_cost : 1.399\n",
      "Accuracy : 0.6372727\n",
      "Epoch : 55, train_cost : 0.536, test_cost : 1.355\n",
      "Accuracy : 0.6409091\n",
      "Epoch : 56, train_cost : 0.464, test_cost : 1.361\n",
      "Accuracy : 0.6481818\n",
      "Epoch : 57, train_cost : 0.472, test_cost : 1.359\n",
      "Accuracy : 0.6545454\n",
      "Epoch : 58, train_cost : 0.406, test_cost : 1.382\n",
      "Accuracy : 0.63272727\n",
      "Epoch : 59, train_cost : 0.364, test_cost : 1.362\n",
      "Accuracy : 0.64272726\n",
      "Epoch : 60, train_cost : 0.413, test_cost : 1.334\n",
      "Accuracy : 0.6490909\n",
      "Epoch : 61, train_cost : 0.29, test_cost : 1.351\n",
      "Accuracy : 0.6454545\n",
      "Epoch : 62, train_cost : 0.277, test_cost : 1.386\n",
      "Accuracy : 0.6481818\n",
      "Epoch : 63, train_cost : 0.338, test_cost : 1.346\n",
      "Accuracy : 0.65\n",
      "Epoch : 64, train_cost : 0.34, test_cost : 1.35\n",
      "Accuracy : 0.65636367\n",
      "Epoch : 65, train_cost : 0.325, test_cost : 1.354\n",
      "Accuracy : 0.65272725\n",
      "Epoch : 66, train_cost : 0.334, test_cost : 1.391\n",
      "Accuracy : 0.6618182\n",
      "Epoch : 67, train_cost : 0.293, test_cost : 1.391\n",
      "Accuracy : 0.6618182\n",
      "Epoch : 68, train_cost : 0.291, test_cost : 1.371\n",
      "Accuracy : 0.66\n",
      "Epoch : 69, train_cost : 0.362, test_cost : 1.404\n",
      "Accuracy : 0.65909094\n",
      "Epoch : 70, train_cost : 0.422, test_cost : 1.4\n",
      "Accuracy : 0.66818184\n",
      "Epoch : 71, train_cost : 0.296, test_cost : 1.402\n",
      "Accuracy : 0.66818184\n",
      "Epoch : 72, train_cost : 0.286, test_cost : 1.444\n",
      "Accuracy : 0.66545457\n",
      "Epoch : 73, train_cost : 0.291, test_cost : 1.448\n",
      "Accuracy : 0.67545456\n",
      "Epoch : 74, train_cost : 0.267, test_cost : 1.453\n",
      "Accuracy : 0.6790909\n",
      "Epoch : 75, train_cost : 0.235, test_cost : 1.509\n",
      "Accuracy : 0.6645455\n",
      "Epoch : 76, train_cost : 0.25, test_cost : 1.508\n",
      "Accuracy : 0.67727274\n",
      "Epoch : 77, train_cost : 0.27, test_cost : 1.496\n",
      "Accuracy : 0.6736364\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-a3e84f6612d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                                 \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                 \u001b[0mp_keep_conv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                                 p_keep_hidden: 0.7})\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fininsight_python3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_epoch = 500\n",
    "\n",
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "    train_cost_ls = []\n",
    "    test_cost_ls = []\n",
    "    \n",
    "    for i in range(training_epoch):\n",
    "        training_batch = zip(range(0, len(train_X), batch_size),\n",
    "                             range(batch_size, len(train_X)+1, batch_size))\n",
    "        \n",
    "        for start, end in training_batch:\n",
    "            sess.run(train, \n",
    "                     feed_dict={X: train_X[start:end], \n",
    "                                Y: train_Y[start:end],\n",
    "                                p_keep_conv: 0.7, \n",
    "                                p_keep_hidden: 0.7})\n",
    "        \n",
    "        \n",
    "        test_indices = np.arange(len(test_X)) # Get A Test Batch\n",
    "        np.random.shuffle(test_indices)\n",
    "        test_indices = test_indices[0:test_size]\n",
    "        \n",
    "        train_dict = {X: train_X[test_indices],\n",
    "                      Y: train_Y[test_indices],\n",
    "                      p_keep_conv: 1.0,\n",
    "                      p_keep_hidden : 1.0}\n",
    "        \n",
    "        test_dict = {X: test_X,\n",
    "                     Y: test_Y,\n",
    "                     p_keep_conv : 1.0,\n",
    "                     p_keep_hidden : 1.0}\n",
    "        \n",
    "        train_cost = sess.run(cost, feed_dict= train_dict)\n",
    "        train_cost_ls.append(train_cost)\n",
    "        \n",
    "        test_cost, acc = sess.run([cost, accuracy], feed_dict = test_dict)\n",
    "        test_cost_ls.append(test_cost)\n",
    "        \n",
    "        print('Epoch : %s, train_cost : %s, test_cost : %s'%(i,round(train_cost, 3), round(test_cost,3)))\n",
    "        print('Accuracy : %s'%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(train_cost_ls, label = 'train cost')\n",
    "plt.plot(test_cost_ls, label = 'test cost')\n",
    "\n",
    "plt.legend(loc = 'best')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
