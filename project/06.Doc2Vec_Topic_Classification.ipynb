{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unset PYTHONPATH first\n",
    "from ko_text import *\n",
    "from ko_crawler import *\n",
    "\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = NLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Data/Train_final.csv', encoding = 'utf-8')\n",
    "test_df = pd.read_csv('Data/Test_final.csv', encoding = 'utf-8')\n",
    "\n",
    "# 용량을 줄이기 위해 '단어 단어' 꼴로 묶어둔 token을 ['단어', '단어'] 꼴로 풀기\n",
    "train_df['Token'] = [token.split() for token in train_df['Token']]\n",
    "test_df['Token'] = [token.split() for token in test_df['Token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87899, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'IT과학': 9996,\n",
       "         '경제': 9660,\n",
       "         '국제': 9844,\n",
       "         '기업': 9965,\n",
       "         '문화': 9846,\n",
       "         '부동산': 9986,\n",
       "         '사회': 9880,\n",
       "         '정치': 9295,\n",
       "         '증권': 9427})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_df['Section'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Doc2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Doc2Vec 모델 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-14 15:24:54,210 : INFO : loading Doc2Vec object from Doc2Vec_model/Doc2Vec_dm=True&cc=87899&vs=100&win=10&neg=5&min=30&sample=1e-05&epochs=10\n",
      "2018-11-14 15:24:54,963 : INFO : loading vocabulary recursively from Doc2Vec_model/Doc2Vec_dm=True&cc=87899&vs=100&win=10&neg=5&min=30&sample=1e-05&epochs=10.vocabulary.* with mmap=None\n",
      "2018-11-14 15:24:54,964 : INFO : loading wv recursively from Doc2Vec_model/Doc2Vec_dm=True&cc=87899&vs=100&win=10&neg=5&min=30&sample=1e-05&epochs=10.wv.* with mmap=None\n",
      "2018-11-14 15:24:54,966 : INFO : loading docvecs recursively from Doc2Vec_model/Doc2Vec_dm=True&cc=87899&vs=100&win=10&neg=5&min=30&sample=1e-05&epochs=10.docvecs.* with mmap=None\n",
      "2018-11-14 15:24:54,966 : INFO : loading trainables recursively from Doc2Vec_model/Doc2Vec_dm=True&cc=87899&vs=100&win=10&neg=5&min=30&sample=1e-05&epochs=10.trainables.* with mmap=None\n",
      "2018-11-14 15:24:54,967 : INFO : loaded Doc2Vec_model/Doc2Vec_dm=True&cc=87899&vs=100&win=10&neg=5&min=30&sample=1e-05&epochs=10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x7f1928d82e80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 성능이 좋았던 모델들 호출\n",
    "nlp.load_Doc2Vec_model('Doc2Vec_model/Doc2Vec_dm=True&cc=87899&vs=100&win=10&neg=5&min=30&sample=1e-05&epochs=10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Doc2Vec train** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''nlp.train_Doc2Vec_model(train_df['Token'],\n",
    "                        train_df['Section'],\n",
    "                        n_epochs = 10)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Doc2Vec 학습결과 확인**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **88000개 문서로 build하고 87000개 문서로 train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.Doc2Vec_model.train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('국가주석', 0.9011991620063782),\n",
       " ('주석', 0.8918991684913635),\n",
       " ('방중', 0.7639303207397461),\n",
       " ('리커창', 0.7269981503486633),\n",
       " ('리잔수', 0.7150949239730835),\n",
       " ('방북설', 0.7129294872283936),\n",
       " ('중국공산당', 0.711692750453949),\n",
       " ('심재훈', 0.6904332637786865),\n",
       " ('왕치산', 0.6902913451194763),\n",
       " ('공산당', 0.684836745262146)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.Doc2Vec_model.most_similar('시진핑')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('지능', 0.8594276905059814),\n",
       " ('인공', 0.8072700500488281),\n",
       " ('음성인식', 0.7785195112228394),\n",
       " ('자연어', 0.738886296749115),\n",
       " ('러닝', 0.7320957183837891),\n",
       " ('기계학습', 0.7101413011550903),\n",
       " ('빅데이터', 0.6975805759429932),\n",
       " ('왓슨', 0.6846472024917603),\n",
       " ('사물인터넷', 0.6809580326080322),\n",
       " ('CTO', 0.6744200587272644)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.Doc2Vec_model.most_similar('AI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'IT과학': 9996, '사회': 9880})"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################\n",
    "# 한 label마다 학습할 단어의 수\n",
    "train_size_for_each_label = 10000\n",
    "test_size_for_each_label = 100\n",
    "###################################\n",
    "\n",
    "\n",
    "# 분류기의 성능을 테스트하기 위해 선정된 section list\n",
    "#testing_section_ls = np.unique(train_df['Section'])\n",
    "testing_section_ls = ['사회','IT과학']\n",
    "\n",
    "# 전체를 모두 학습하면 시간이 오래걸림.\n",
    "# 분류기별 성능 비교를 위해, 부분만 학습하기 위한 전처리 작업\n",
    "train_df2 = train_df[train_df['Section'].isin(testing_section_ls)]\n",
    "train_df2.index = np.arange(0,len(train_df2))\n",
    "\n",
    "test_df2 = test_df[test_df['Section'].isin(testing_section_ls)]\n",
    "test_df2.index = np.arange(0,len(test_df2))\n",
    "\n",
    "n_class = len(test_df2['Section'].unique())\n",
    "\n",
    "\n",
    "# Doc2Vec으로 vector를 추정하기 위한 split 과정\n",
    "train_batch_size = n_class * train_size_for_each_label\n",
    "test_batch_size = n_class * test_size_for_each_label\n",
    "\n",
    "X_train, y_train = nlp.extract_a_equally_splited_batch(train_df2['Token'], train_df2['Section'], train_batch_size)\n",
    "X_test, y_test =  nlp.extract_a_equally_splited_batch(test_df2['Token'],test_df2['Section'], test_batch_size)\n",
    "\n",
    "print(1)\n",
    "\n",
    "X_train = nlp.infer_vectors_with_Doc2Vec(X_train)\n",
    "y_train = y_train\n",
    "\n",
    "X_test = nlp.infer_vectors_with_Doc2Vec(X_test)\n",
    "y_test = y_test\n",
    "\n",
    "from collections import Counter\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.935\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver = 'newton-cg',\n",
    "                         multi_class = 'multinomial')\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy : ', accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree 모델은 feature들이 각각 의미있는 변수들이면서, 독립적으로 사용될 수 있을 때 유용한 방법이다.\n",
    "\n",
    "\n",
    "각 변수별로 적절한 기준선을 찾아 공간을 나누기 때문.\n",
    "\n",
    "따라서 Doc2Vec과 같이 좌표평면상에서 벡터의 위치가 아무런 의미가 없는 경우, 학습 효과가 현저하게 떨어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.84\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy : ', accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.91\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100,  n_jobs = -1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "    \n",
    "print('Accuracy : ', accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neural Net**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nn = np.array(X_train).astype('float32')\n",
    "y_train_nn = pd.get_dummies(y_train).values.astype('float32')\n",
    "\n",
    "\n",
    "X_test_nn = np.array(X_test).astype('float32')\n",
    "y_test_nn = pd.get_dummies(y_test).values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19876, 100) (19876, 2) (200, 100) (200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_nn.shape, y_train_nn.shape, X_test_nn.shape, y_test_nn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset graphs\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# mini-batches\n",
    "batch_size = X_train_nn.shape[0] // 5\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train_nn, y_train_nn))\n",
    "dataset = dataset.batch(batch_size)\n",
    "\n",
    "# building placeholder\n",
    "X = tf.placeholder(tf.float32, shape = [None, nlp.Doc2Vec_model.vector_size])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, n_class])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# building layers\n",
    "n_neuron = 100\n",
    "\n",
    "W1 = tf.get_variable('W1', shape = ([nlp.Doc2Vec_model.vector_size, n_neuron]), initializer = tf.contrib.layers.xavier_initializer())\n",
    "W2 = tf.get_variable('W2', shape = ([n_neuron, n_neuron]), initializer = tf.contrib.layers.xavier_initializer())\n",
    "W3 = tf.get_variable('W3', shape = ([n_neuron, n_neuron]), initializer = tf.contrib.layers.xavier_initializer())\n",
    "W4 = tf.get_variable('W4', shape = ([n_neuron, n_neuron]), initializer = tf.contrib.layers.xavier_initializer())\n",
    "W5 = tf.get_variable('W5', shape = ([n_neuron, n_class]), initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([n_neuron]))\n",
    "b2 = tf.Variable(tf.random_normal([n_neuron]))\n",
    "b3 = tf.Variable(tf.random_normal([n_neuron]))\n",
    "b4 = tf.Variable(tf.random_normal([n_neuron]))\n",
    "b5 = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "L1 = tf.nn.relu(tf.matmul(X,W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob = keep_prob)\n",
    "\n",
    "L2 = tf.nn.relu(tf.matmul(L1,W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob = keep_prob)\n",
    "\n",
    "L3 = tf.nn.relu(tf.matmul(L2,W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob = keep_prob)\n",
    "\n",
    "L4 = tf.nn.relu(tf.matmul(L3,W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob = keep_prob)\n",
    "\n",
    "logit = tf.matmul(L4,W5) + b5\n",
    "hypothesis = tf.nn.softmax(tf.matmul(L4,W5) + b5)\n",
    "\n",
    "\n",
    "# cost : cross - entropy cost \n",
    "lamb = 0.0001\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit, labels = Y)) + lamb * tf.reduce_sum(tf.square(W5))\n",
    "\n",
    "# optimize\n",
    "learning_rate = 0.0001\n",
    "train = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "# prediction\n",
    "prediction = tf.argmax(hypothesis,1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y,1), prediction), dtype= tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "# restore results\n",
    "train_cost_list = []\n",
    "train_acc_list = []\n",
    "\n",
    "test_cost_list = []\n",
    "test_acc_list = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Run**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **mini-batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dict = {X: X_train, Y: y_train}\n",
    "test_dict = {X: X_test_nn, Y: y_test_nn, keep_prob : 1}\n",
    "\n",
    "training_epochs = 1500\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "f, l = iterator.get_next()\n",
    "\n",
    "# launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    \n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        # iterator initialize\n",
    "        sess.run(iterator.initializer)\n",
    "        avg_cost = 0\n",
    "\n",
    "        while True:\n",
    "            # mini-batch\n",
    "            try:\n",
    "                batch_x,  batch_y = sess.run([f, l])\n",
    "                feed_dict = {X : batch_x, Y: batch_y, keep_prob : 0.7}\n",
    "                \n",
    "                c, _ = sess.run([cost, train], feed_dict = feed_dict)\n",
    "                avg_cost += c\n",
    "            \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "        \n",
    "        acc, _, test_cost = sess.run([accuracy, prediction, cost], feed_dict = test_dict)\n",
    "        \n",
    "        train_cost_list.append(avg_cost)\n",
    "        test_cost_list.append(test_cost)\n",
    "            \n",
    "        if (epoch+1) % (100) == 0 :\n",
    "            \n",
    "            test_acc_list.append(acc)\n",
    "            \n",
    "            print('Epoch : %s'%(epoch+1), 'cost :',test_cost)\n",
    "            print('Accuracy :', acc)\n",
    "            \n",
    "        \n",
    "    \n",
    "    acc, y_pred, test_cost = sess.run([accuracy, prediction, cost], feed_dict = test_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(train_cost_list, label = 'train_cost')\n",
    "plt.plot(test_cost_list, label = 'test_cost')\n",
    "plt.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **full-batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dict = {X: X_train, Y: y_train}\n",
    "test_dict = {X: X_test_nn, Y: y_test_nn, keep_prob : 1}\n",
    "\n",
    "training_epochs = 1000\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "f, l = iterator.get_next()\n",
    "\n",
    "# launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())    \n",
    "    \n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        # iterator initialize\n",
    "        avg_cost = 0\n",
    "\n",
    "        while True:\n",
    "            # mini-batch\n",
    "            try:\n",
    "                feed_dict = {X : X_train_nn, Y: y_train_nn, keep_prob : 0.7}\n",
    "                \n",
    "                c, _ = sess.run([cost, train], feed_dict = feed_dict)\n",
    "                avg_cost += c\n",
    "            \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "        \n",
    "        acc, y_pred, test_cost = sess.run([accuracy, prediction, cost], feed_dict = test_dict)\n",
    "        \n",
    "        train_cost_list.append(avg_cost)\n",
    "        test_cost_list.append(test_cost)\n",
    "            \n",
    "        if (epoch+1) % (100) == 0 :\n",
    "            \n",
    "            test_acc_list.append(acc)\n",
    "            \n",
    "            print('Epoch : %s'%(epoch+1), 'cost :',test_cost)\n",
    "            print('Accuracy :', acc)\n",
    "        #print(sess.run(tf.confusion_matrix(labels = tf.reshape(Y, [-1]), predictions = tf.reshape(y_pred, [-1])), feed_dict = test_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(train_cost_list, label = 'train_cost')\n",
    "plt.plot(test_cost_list, label = 'test_cost')\n",
    "plt.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
