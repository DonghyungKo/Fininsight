{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 원본 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "req = requests.get('http://52.231.65.246:9200/test/news/_search/?size=10000')\n",
    "\n",
    "def read_json(req):\n",
    "    json_dict = json.loads(req.content.decode())\n",
    "\n",
    "    body_ls = []\n",
    "    category_ls = []\n",
    "    try:\n",
    "        for article in json_dict['hits']['hits']:\n",
    "            body = article['_source']['body'].replace('\\t',' ').replace('\\n',' ')\n",
    "            body_ls.append(body)\n",
    "\n",
    "            category = article['_source']['category']\n",
    "            category_ls.append(category)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return body_ls, category_ls\n",
    "\n",
    "body_ls, category_ls = read_json(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'IT': 156,\n",
       "         '경제': 1539,\n",
       "         '기타': 403,\n",
       "         '사회': 3399,\n",
       "         '생활': 819,\n",
       "         '세계': 738,\n",
       "         '스포츠': 633,\n",
       "         '연예': 433,\n",
       "         '오피니언': 619,\n",
       "         '정치': 1261})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(category_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news.txt', 'w') as f:\n",
    "    for category, body in zip(category_ls, body_ls):\n",
    "        f.write('%s\\t%s\\n'%(category, body))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 + 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(path_to_file):\n",
    "    body_ls = []\n",
    "    category_ls = []\n",
    "    \n",
    "    with open(path_to_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            try:\n",
    "                category, body = line.split('\\t')\n",
    "                category_ls.append(category)\n",
    "                body_ls.append(body)\n",
    "            except:\n",
    "                print(i)\n",
    "    return body_ls, category_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_ls, category_ls = read_txt('news.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리(클렌징)\n",
    "import re\n",
    "\n",
    "# 형태소 분석\n",
    "from konlpy.tag import Okt\n",
    "import konlpy\n",
    "\n",
    "# 병렬처리\n",
    "from multiprocessing import Process,Queue, Pool\n",
    "import functools\n",
    "from threading import Thread\n",
    "from konlpy import jvm\n",
    "import jpype\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        전처리를 위한 함수들이 저장된 클래스입니다.\n",
    "        '''\n",
    "\n",
    "        self.twit = Okt()# 한 개의 문서에서 명사(noun)만 추출하는 함수\n",
    "        \n",
    "        # 정규 표현식 리스트\n",
    "        self.regex_ls = [\n",
    "                    '[\\t\\n\\r\\f\\v]', #공백 제거,\n",
    "            '\\(.+?\\)', '\\[.+?\\]', '\\<.+?\\>',  '◀.+?▶',  '=.+=', #특수문자 사이에 오는 단어 제거\n",
    "            '(?<=▶).+', '(?<=▷).+', '(?<=※).+', #특수문자 다음으로 오는 단어 제거\n",
    "            #'(?<=\\xa0).+', # \\xa0(증권 기사) 다음으로 오는 단어 제거\n",
    "            '(?<=\\Copyrights).+', # Copyrights 다음에 오는 기사 제거\n",
    "            '[\\w]+@[a-zA-Z]+\\.[a-zA-Z]+[\\.]?[a-z]*', #이메일 제거\n",
    "            '[가-힣]+ 기자', '[가-힣]+ 선임기자',\n",
    "            '[\\{\\}\\[\\]\\/?,;·:“‘|\\)*~`!^\\-_+<>@○▲▶■◆\\#$┌─┐&\\\\\\=\\(\\'\\\"├┼┤│┬└┴┘|ⓒ]', #특수문자 제거\n",
    "            #'[0-9]+[년월분일시조억만천원]*' , #숫자 단위 제거\n",
    "        ]\n",
    "        \n",
    "        # 2. 제거 대상 단어 리스트\n",
    "        self.word_to_be_cleaned_ls = ['한기재','헬스경향','디지털뉴스팀', '\\u3000','Copyrights','xa0', \n",
    "                                      'googletagdisplay', 'windowjQuery', 'documentwrite']\n",
    "        \n",
    "        # 3. 불용어 리스트\n",
    "        self.stopword_ls = ['에서','으로','했다','하는','이다','있다','하고','있는','까지','이라고','에는',\n",
    "                            '한다','지난','관련','대한','됐다','부터','된다','위해','이번','통해','대해',\n",
    "                            '애게','했다고','보다','되는','에서는','있다고','한다고','하기','에도','때문',\n",
    "                            '하며','하지','해야','이어','하면','따라','하면서','라며','라고','되고','단지',\n",
    "                            '이라는','이나','한다는','있따는','했고','이를','있도록','있어','하게','있다며',\n",
    "                            '하기로','에서도','오는','라는','이런','하겠다고','만큼','이는','덧붙였다','있을',\n",
    "                            '이고','이었다','이라','있으며','있고','이며','했다며','됐다고','나타났다','한다며',\n",
    "                            '하도록','있지만','된다고','되면서','그러면서','그동안','해서는','에게','밝혔다', '한편',\n",
    "                            '최근', '있다는','보이','되지','정도','지난해','매년','오늘','되며','하기도', '지난달',\n",
    "                            '하겠다는','했다세라','올해', '바로', '바랍니다', '함께','이후','따르면','같은','오후','모두',\n",
    "                            '로부터','전날','면서','했다는','그리고','있던'\n",
    "                           ]\n",
    "\n",
    "    # 크롤링한 text에 정규표현식을 적용하는 함수입니다.\n",
    "    def clean_text(self,text):\n",
    "        try:\n",
    "            for regex in self.regex_ls:\n",
    "                text = re.sub(regex, '', text)\n",
    "        except:\n",
    "            text = ' '\n",
    "        return text\n",
    "\n",
    "    # 복수 개의 문서를 클렌징하는 함수입니다.\n",
    "    def clean_doc(self, doc_ls):\n",
    "        '''\n",
    "        정규표현식으로 문서를 전처리하는 함수입니다.\n",
    "        \n",
    "        input\n",
    "        doc_ls : list(iterable)\n",
    "            str형태의 text를 원소로 갖는 list type\n",
    "        '''\n",
    "        return [self.clean_text(doc) for doc in doc_ls]\n",
    "\n",
    "\n",
    "    # 불용어를 제거하는 함수입니다.\n",
    "    def remove_stopwords(self, token_doc_ls):\n",
    "        '''\n",
    "        불용어를 제거하는 함수입니다.\n",
    "\n",
    "        input\n",
    "        token_doc_ls : str, iterable\n",
    "            token 형태로 구분된 문서가 담긴 list 형식\n",
    "        '''\n",
    "\n",
    "        total_stopword_set = set(self.stopword_ls)\n",
    "\n",
    "        # input이 복수 개의 문서가 담긴 list라면, 개별 문서에 따라 단어를 구분하여 불용어 처리\n",
    "        return_ls = []\n",
    "\n",
    "        if token_doc_ls:\n",
    "            if type(token_doc_ls[0]) == list:\n",
    "                for doc in token_doc_ls:\n",
    "                    return_ls += [[token for token in doc if not token in total_stopword_set]]\n",
    "\n",
    "            elif type(token_doc_ls[0]) == str:\n",
    "                return_ls = [token for token in token_doc_ls if not token in total_stopword_set]\n",
    "\n",
    "        return return_ls  \n",
    "    \n",
    "    def _extract_nouns_for_single_doc(self, doc):\n",
    "        '''\n",
    "        명사 추출\n",
    "        '''\n",
    "        clean_doc = self.clean_text(doc) # 클렌징\n",
    "        token_ls = [x for x in self.twit.nouns(clean_doc) if len(x) > 1] # 토크나이징\n",
    "        return self.remove_stopwords(token_ls) # 불용어 제거\n",
    "\n",
    "\n",
    "    # 한 개의 문서에서 형태소(morphs)만 추출하는 함수\n",
    "    def _extract_morphs_for_single_doc(self, doc):\n",
    "        '''\n",
    "        형태소 추출\n",
    "        '''\n",
    "        clean_doc = self.clean_text(doc) # 클렌징\n",
    "        token_ls = [x for x in self.twit.morphs(clean_doc) if len(x) > 1] # 토크나이징\n",
    "        return self.remove_stopwords(token_ls) # 불용어 제거\n",
    "\n",
    "\n",
    "\n",
    "    # 모든 문서에서 명사(nouns)을 추출하는 함수.\n",
    "    def extract_nouns_for_all_document(self,doc_ls, stopword_ls = []):\n",
    "        '''\n",
    "        모든 문서에서 명사를 추출하는 함수입니다.\n",
    "        전처리를 적용하고 불용어를 제거한 결과를 반환합니다.\n",
    "\n",
    "        input\n",
    "        doc_ls : iterable, 원문이 str형태로 저장된 list\n",
    "\n",
    "        return\n",
    "        전처리 적용, 불용어 제거\n",
    "        list : 각각의 문서가 토크나이징 된 결과를 list형태로 반환\n",
    "        '''\n",
    "        jpype.attachThreadToJVM()\n",
    "        # 전처리\n",
    "        clean_doc_ls = self.clean_doc(doc_ls)\n",
    "\n",
    "        # 명사 추출\n",
    "        token_doc_ls = [self._extract_nouns_for_single_doc(doc) for doc in clean_doc_ls]\n",
    "\n",
    "        # 불용어 제거\n",
    "        return self.remove_stopwords(token_doc_ls)\n",
    "\n",
    "\n",
    "\n",
    "    # 모든 문서에서 형태소(morph)를 추출하는 함수.\n",
    "    def extract_morphs_for_all_document(self,doc_ls, stopword_ls = []):\n",
    "        '''\n",
    "        모든 문서에서 형태소(morph)를 추출하는 함수입니다.\n",
    "        전처리를 적용하고 불용어를 제거한 결과를 반환합니다.\n",
    "\n",
    "        input\n",
    "        doc_ls : iterable, 원문이 str형태로 저장된 list\n",
    "\n",
    "        return\n",
    "        list : 각각의 문서가 토크나이징 된 결과를 list형태로 반환\n",
    "        '''\n",
    "        jpype.attachThreadToJVM()\n",
    "        # 전처리\n",
    "        clean_doc_ls = self.clean_doc(doc_ls)\n",
    "\n",
    "        # 형태소(morph) 추출\n",
    "        token_doc_ls = [self._extract_morphs_for_single_doc(doc) for doc in clean_doc_ls]\n",
    "\n",
    "        # 불용어 제거\n",
    "        return self.remove_stopwords(token_doc_ls)\n",
    "\n",
    "    \n",
    "    \n",
    "    # 토크나이징을 병렬처리 하는데 사용되는 함수\n",
    "    def _extract_nouns_for_multiprocessing(self, tuple_ls):\n",
    "        jpype.attachThreadToJVM()\n",
    "        # 멀티프로세싱의 경우, 병렬처리시 순서가 뒤섞이는 것을 방지하기위해,\n",
    "        # [(idx, doc)] 형태의 tuple이 들어온다.\n",
    "        return [(idx, self._extract_nouns_for_single_doc(doc)) for idx, doc in tuple_ls]\n",
    "\n",
    "    # 토크나이징을 병렬처리 하는데 사용되는 함수\n",
    "    def _extract_morphs_for_multiprocessing(self, tuple_ls):\n",
    "        jpype.attachThreadToJVM()\n",
    "        # 멀티프로세싱의 경우, 병렬처리시 순서가 뒤섞이는 것을 방지하기위해,\n",
    "        # [(idx, doc)] 형태의 tuple이 들어온다.\n",
    "        return [(idx, self._extract_morphs_for_single_doc(doc)) for idx, doc in tuple_ls]\n",
    "\n",
    "    # 토크나이징을 병렬처리 하기 위해, 작업을 분할하는 함수\n",
    "    def split_list(self, ls, n):\n",
    "        '''\n",
    "        병렬처리를 위해, 리스트를 원하는 크기(n)로 분할해주는 함수입니다.\n",
    "        '''\n",
    "        result_ls = []\n",
    "        for i in range(0, len(ls), n):\n",
    "            result_ls += [ls[i:i+n]]\n",
    "        \n",
    "        return result_ls\n",
    "\n",
    "    def extract_morphs_for_all_document_FAST_VERSION(self,\n",
    "                                                     doc_ls,\n",
    "                                                     n_thread = 4):\n",
    "        jpype.attachThreadToJVM()\n",
    "\n",
    "        '''\n",
    "        멀티쓰레딩을 적용하여 속도가 개선된 버전입니다.\n",
    "        문서들을 전처리하고 불용어(stopwords)를 제거한 후, Tokenzing하는 함수입니다.\n",
    "\n",
    "        inputs\n",
    "        doc_ls : iterable, 원문이 str 형태로 담겨있는 list를 input으로 받습니다.\n",
    "        n_thread: int[default : 4], 사용하실 쓰레드의 갯수를 input으로 받습니다.\n",
    "        '''\n",
    "\n",
    "        # 텍스트 클렌징 작업 수행\n",
    "        # [(idx, clean_doc)] 형태로 저장 (나중에 sorting을 위해)\n",
    "        clean_tuple_ls = [(idx, clean_doc) for idx, clean_doc in zip(range(len(doc_ls)), self.clean_doc(doc_ls))]\n",
    "\n",
    "        # 멀티쓰레딩을 위한 작업(리스트)분할\n",
    "        length = len(clean_tuple_ls)\n",
    "        splited_clean_tuple_ls = self.split_list(clean_tuple_ls, length//n_thread)\n",
    "\n",
    "        que = queue.Queue()\n",
    "        thread_ls = []\n",
    "\n",
    "        for tuple_ls in splited_clean_tuple_ls:\n",
    "\n",
    "            temp_thread = Thread(target= lambda q, arg1: q.put(self._extract_morphs_for_multiprocessing(arg1)),  args = (que, tuple_ls))\n",
    "\n",
    "            temp_thread.start()\n",
    "            thread_ls.append(temp_thread)\n",
    "\n",
    "        for thread in thread_ls:\n",
    "            thread.join()\n",
    "\n",
    "        # 정렬을 위한 index_ls와 token_ls를 사용\n",
    "        index_ls = []\n",
    "        token_ls = []\n",
    "\n",
    "        # thread의 return 값을 결합\n",
    "        while not que.empty():\n",
    "\n",
    "            result = que.get() # [(idx, token), (idx, token)...] 형태를 반환\n",
    "            index_ls += [idx for idx, _ in result]\n",
    "            token_ls += [token for _, token in result]\n",
    "\n",
    "        token_ls = [token for idx, token in sorted(zip(index_ls, token_ls))]  \n",
    "        token_ls = [' '.join(tokens) for tokens in token_ls]\n",
    "    \n",
    "        return token_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 17s, sys: 16.1 s, total: 8min 33s\n",
      "Wall time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clean_body_ls = pre.extract_morphs_for_all_document_FAST_VERSION(body_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'파울루 벤투호에 패한 오스카 타바레스 우루과이 대표팀 감독이 한국 축구는 많이 성장했고 강해졌다”며 박수를 보냈다.벤투 감독이 이끄는 축구대표팀은 12일 오후 서울월드컵경기장에서 열린 우루과이와의 평가전에서 21로 승리했다. 후반 20분 황의조의 선제골과 후반 34분 정우영의 결승골이 승전고를 울렸다.경기 후 일흔이 넘은 노장 타바레스 감독은 좋은 경기였다. 전반전은 양팀이 서로 균형이 맞았다”며 결과를 가져오지 못해 아쉬움은 있으나 좋은 경기를 펼쳤다는 것에 만족한다”고 소감을 밝혔다.타바레스 감독은 우루과이와 한국은 12시간의 시차가 있고 또 장시간 비행으로 인해 체력적인 어려움도 있었다”고 정상적 컨디션이 아니었다는 것을 이야기하면서도 러시아 월드컵과 견줘 한국 축구는 많이 성장했고 강해졌다. 특히 손흥민은 톱 클래스의 플레이를 펼치고 있다. 한국은 더 성장할 여지가 있다”고 평가했다.벤투 감독에 대해서는 그와 한 시즌을 같이 생활했는데 당시에도 이미 높은 수준을 보여주고 있었기에 그를 잊을 수 없다”며 감독으로도 잘 성장하고 있다. 중국과 포르투갈 그리고 한국에서 커리어를 쌓고 있는데 이대로 걸어간다면 세계적인 명장이 될 수 있을 것”이라고 덕담했다.타바레스 감독은 199798시즌 스페인의 레알 오비에도를 이끌던 시절 선수 벤투와 연을 맺었다.타바레스 감독은 끝으로 우루과이는 세대교체 중이다. 오늘 경기만 가지고 과거의 우루과이와 비교하는 것은 무리다. 러시아 월드컵은 끝났고 이제 새로운 출발 앞에 서 있다”면서 지금은 카타르 월드컵에 집중하는 단계다. 잘 준비해 다음 주에 있을 일본전을 대비할 것”이라고 밝혔다.박태근  '"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 306\n",
    "body_ls[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'파울루 투호 오스카 타바레스 우루과이 대표팀 감독 한국 축구 많이 성장했고 강해졌다 박수 보냈다 감독 이끄는 축구 대표팀 12일 월드컵경기 열린 우루과이 와의 평가전 21 승리 후반 20분 황의조 선제골 후반 34분 정우영 결승골 승전 고를 울렸다 경기 일흔 넘은 노장 타바레스 감독 좋은 경기 였다 전반전 서로 균형 맞았다 결과 가져오지 아쉬움 있으나 좋은 경기 펼쳤다는 만족한다 소감 타바레스 감독 우루과이 한국 12시간 시차 장시간 비행 인해 체력 어려움 있었다 정상 컨디션 아니었다는 이야기 하면서도 러시아 월드컵 견줘 한국 축구 많이 성장했고 강해졌다 특히 손흥민 클래스 플레이 펼치고 한국 성장할 여지 평가 감독 서는 시즌 같이 생활 했는데 당시 이미 높은 수준 보여주고 있었기에 잊을 없다 감독 으로도 성장하고 중국 포르투갈 한국 커리어 쌓고 있는데 이대로 걸어간다면 세계 명장 덕담 타바레스 감독 199798시 스페인 레알 오비에도 이끌던 시절 선수 맺었다 타바레스 감독 우루과이 세대 교체 중이 경기 가지 과거 우루과이 비교 무리 러시아 월드컵 끝났고 이제 새로운 출발 지금 카타르 월드컵 집중 단계 준비 다음 본전 대비 박태'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_body_ls[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'스포츠'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_ls[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ls = [token for tokens in clean_body_ls for token in tokens.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('한국', 7604),\n",
       " ('미국', 6335),\n",
       " ('중국', 6064),\n",
       " ('서울', 5360),\n",
       " ('대통령', 5052),\n",
       " ('정부', 5049),\n",
       " ('사업', 4155),\n",
       " ('문제', 4124),\n",
       " ('지역', 4061),\n",
       " ('사람', 3958)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(token_ls).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news_tokenized.txt', 'w') as f:\n",
    "    for category, body in zip(category_ls, clean_body_ls):\n",
    "        f.write('%s\\t%s\\n'%(category, body))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 단어 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y 더미화\n",
    "from collections import defaultdict\n",
    "\n",
    "# CBOW 계산\n",
    "import numpy as np\n",
    "\n",
    "# 임베딩\n",
    "import os\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "\n",
    "# 분류기 학습 결과 계산\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# 결과 저장\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(path_to_file):\n",
    "    body_ls = []\n",
    "    category_ls = []\n",
    "    \n",
    "    with open(path_to_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            try:\n",
    "                category, body = line.split('\\t')\n",
    "                category_ls.append(category)\n",
    "                \n",
    "                body_ls.append(body)\n",
    "            except:\n",
    "                print(i)\n",
    "    return body_ls, category_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cbow(token_ls, word2vec):\n",
    "    cbow_ls = []\n",
    "\n",
    "    for tokens in token_ls:\n",
    "        cbow = np.zeros((word2vec.vector_size))\n",
    "\n",
    "        for token in tokens:\n",
    "            cbow += word2vec.wv.get_vector(token)\n",
    "\n",
    "        cbow_ls.append(cbow)\n",
    "    return cbow_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Word2Vec train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # data load\n",
    "    token_ls, category_ls = read_txt('news_tokenized.txt')\n",
    "    token_ls = [tokens.split() for tokens in token_ls]\n",
    "\n",
    "    # train test split\n",
    "    train_size = int(round(len(token_ls) * 0.8))\n",
    "\n",
    "    x_train = token_ls[:train_size]\n",
    "    y_train = category_ls[:train_size]\n",
    "\n",
    "    x_test = token_ls[train_size:]\n",
    "    y_test = category_ls[train_size:]\n",
    "    \n",
    "    # Y 더미화\n",
    "    cat2idx = defaultdict(lambda : len(cat2idx))\n",
    "    y_train_idx = [cat2idx[x] for x in y_train]\n",
    "    y_test_idx = [cat2idx[x] for x in y_test]\n",
    "\n",
    "    # train\n",
    "    result_dict = defaultdict(lambda: [])\n",
    "    best_loss = 1e10\n",
    "    \n",
    "    # training\n",
    "    for sg in [1,0]:\n",
    "        for sample in [1e-05, 1e-06]:\n",
    "            for min_count in [1, 5]:\n",
    "                for alpha in [0.025, 0.1, 0.25]:\n",
    "                    for vector_size in [100,300]:\n",
    "                        for negative in [5,15]:\n",
    "                            for n_epochs in [3,10,30]:\n",
    "                                print('==============================================================================')\n",
    "                                print('Model Training Started')\n",
    "                                \n",
    "                                # word2vec 모델 생성\n",
    "                                word2vec = Word2Vec(\n",
    "                                    sentences = token_ls,\n",
    "                                    sg = sg,\n",
    "                                    size = vector_size,\n",
    "                                    alpha = alpha,\n",
    "                                    min_count = min_count,\n",
    "                                    sample = sample,\n",
    "                                    workers = 4,\n",
    "                                    negative = negative,\n",
    "                                    iter = n_epochs,\n",
    "                                )\n",
    "\n",
    "                                # CBOW 생성\n",
    "                                X_train = make_cbow(x_train, word2vec)\n",
    "                                X_test = make_cbow(x_test, word2vec)\n",
    "                                \n",
    "                                # 분류기 학습\n",
    "                                clf = LogisticRegression(solver = 'sag', multi_class = 'multinomial')\n",
    "\n",
    "                                clf.fit(X_train, y_train_idx)\n",
    "                                y_prob = clf.predict_log_proba(X_test)\n",
    "                                y_pred = [prob.argmax() for prob in y_prob]\n",
    "\n",
    "                                acc = accuracy_score(y_test_idx, y_pred)\n",
    "                                loss = log_loss(y_test_idx, y_prob, normalize=False)\n",
    "\n",
    "                                print('Accuracy : ', acc)\n",
    "\n",
    "                                # 모델 저장할 폴더 생성\n",
    "                                if not os.path.exists('Word2Vec_model'):\n",
    "                                    os.mkdir('Word2Vec_model')\n",
    "\n",
    "                                # 분류 결과가 best인 모델만 저장\n",
    "                                if loss < best_loss:\n",
    "                                    best_loss = loss\n",
    "                                    word2vec.save('Word2Vec_model/best_w2v_model')\n",
    "                                \n",
    "                                # 결과 저장\n",
    "                                result_dict['sg'].append(sg)\n",
    "                                result_dict['corpus_count'].append(len(token_ls))\n",
    "                                result_dict['min_count'].append(min_count)\n",
    "                                result_dict['vector_size'].append(vector_size)\n",
    "                                result_dict['n_epochs'].append(n_epochs)\n",
    "                                result_dict['sample'].append(sample)\n",
    "                                result_dict['accuracy'].append(acc)\n",
    "\n",
    "                                # 분류 결과 저장\n",
    "                                pd.DataFrame(result_dict).to_csv('Word2Vec_tuning_result.csv', index=False)\n",
    "\n",
    "                                print('Model Training Finished')\n",
    "    return\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class D2V(object):\n",
    "    #####################################################\n",
    "    ##################   Doc2Vec   ######################\n",
    "    #####################################################\n",
    "\n",
    "    def __init__(self, path_to_model = ''):\n",
    "        '''\n",
    "        기존에 학습된 모델을 불러옵니다.\n",
    "\n",
    "        Inputs\n",
    "        =========================\n",
    "        path_to_model : str,\n",
    "            학습된 모델이 저장된 path\n",
    "        '''\n",
    "        try:\n",
    "            self.Doc2Vec_model = Doc2Vec.load(path_to_model)\n",
    "            print('학습된 모델을 성공적으로 불러왔습니다.')\n",
    "            return\n",
    "        except:\n",
    "            print('모델을 불러오지 못하였습니다.')\n",
    "            print('새로운 모델을 생성해주시기 바랍니다.')\n",
    "            return\n",
    "\n",
    "\n",
    "    def make_Doc2Vec_model(self,\n",
    "                           dm = 1,\n",
    "                           dbow_words = 0,\n",
    "                           window = 15,\n",
    "                           vector_size = 100,\n",
    "                           sample = 1e-5,\n",
    "                           min_count = 5,\n",
    "                           hs = 0,\n",
    "                           negative = 5,\n",
    "                           dm_mean = 0,\n",
    "                           dm_concat = 0):\n",
    "        '''\n",
    "        Doc2Vec 모델의 초기설정을 입력하는 함수입니다.\n",
    "        기존에 만들어진 모델을 load하여 사용할 수 있습니다. (load_Doc2Vec_model 함수를 사용)\n",
    "\n",
    "        Inputs\n",
    "         - dm : PV-DBOW / default 1\n",
    "         - dbow_words : w2v simultaneous with DBOW d2v / default 0\n",
    "         - window : distance between the predicted word and context words\n",
    "         - vector size : vector_size\n",
    "         - min_count : ignore with freq lower\n",
    "         - workers : multi cpu\n",
    "         - hs : hierarchical softmax / default 0\n",
    "         - negative : negative sampling / default 5\n",
    "\n",
    "        Return\n",
    "         - None\n",
    "        '''\n",
    "        cores = multiprocessing.cpu_count()\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "        self.Doc2Vec_model = Doc2Vec(\n",
    "            dm= dm,                     # PV-DBOW / default 1\n",
    "            dbow_words= dbow_words,     # w2v simultaneous with DBOW d2v / default 0\n",
    "            window= window,             # distance between the predicted word and context words\n",
    "            vector_size= vector_size,   # vector size\n",
    "            sample = sample,\n",
    "            min_count = min_count,      # ignore with freq lower\n",
    "            workers= cores,             # multi cpu\n",
    "            hs = hs,                    # hierarchical softmax / default 0\n",
    "            negative = negative,        # negative sampling / default 5\n",
    "            dm_mean = dm_mean,\n",
    "            dm_concat = dm_concat,\n",
    "        )\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "\n",
    "    def build_and_train_Doc2Vec_model(self,\n",
    "                                      train_doc_ls,\n",
    "                                      train_tag_ls,\n",
    "                                      n_epochs = 10,\n",
    "                                      if_tokenized = True,\n",
    "                                      if_morphs = True):\n",
    "\n",
    "        '''\n",
    "        Doc2Vec 모델을 생성 혹은 Load한 다음 작업으로, Doc2Vec을 build하고 학습을 수행합니다.\n",
    "\n",
    "        Inputs\n",
    "        =================================\n",
    "        train_doc_ls : iterable,\n",
    "            documents(tokenized or not tokenized)\n",
    "\n",
    "        train_tag_ls : iterable,\n",
    "            tags of each documents\n",
    "\n",
    "        n_epochs : int\n",
    "            numbers of iteration\n",
    "\n",
    "        '''\n",
    "\n",
    "\n",
    "        # tokenized된 input데이터를 받으면 tokenizing skip\n",
    "        if if_tokenized :\n",
    "            train_token_ls = train_doc_ls\n",
    "        else:\n",
    "            clean_train_doc_ls = self.clean_doc(train_doc_ls)\n",
    "            if if_morphs:\n",
    "                train_token_ls = self.extract_tokens_for_all_document_FAST_VERSION(clean_train_doc_ls, if_morphs = True)\n",
    "            else:\n",
    "                train_token_ls = self.extract_tokens_for_all_document_FAST_VERSION(clean_train_doc_ls, if_morphs = False)\n",
    "\n",
    "        # train_tag_ls를 리스트 형태로 변환 (Series 형태로 들어올 경우)\n",
    "        try:  train_tag_ls = train_tag_ls.tolist()\n",
    "        except:   pass\n",
    "\n",
    "        # words와 tags로 구성된 namedtuple 형태로 데이터 변환 (tagging 작업)\n",
    "        tagged_train_doc_ls = [TaggedDocument(tuple_[0], [tuple_[1]]) for i, tuple_ in enumerate(zip(train_token_ls, train_tag_ls))]\n",
    "\n",
    "        # Doc2Vec 모델에 단어 build작업 수행\n",
    "        self.Doc2Vec_model.build_vocab(tagged_train_doc_ls)\n",
    "\n",
    "        # 학습 수행\n",
    "        self.Doc2Vec_model.train(tagged_train_doc_ls,\n",
    "                                 total_examples= self.Doc2Vec_model.corpus_count,\n",
    "                                 epochs= n_epochs)\n",
    "        return\n",
    "\n",
    "\n",
    "    def train_Doc2Vec_model(self,\n",
    "                            train_doc_ls,\n",
    "                            train_tag_ls,\n",
    "                            n_epochs = 10,\n",
    "                            if_tokenized = True,\n",
    "                            if_morphs = True,\n",
    "                            ):\n",
    "        '''\n",
    "        built된 Doc2Vec 모델에 추가적인 학습을 수행합니다.\n",
    "\n",
    "        Inputs\n",
    "         - train_doc_ls : iterable, \n",
    "            documents(tokenized or not tokenized)\n",
    "\n",
    "         - train_tag_ls : iterable, \n",
    "            tags of each documents\n",
    "\n",
    "         - n_epochs : int \n",
    "            numbers of iteration\n",
    "\n",
    "         - if_morphs :  boolean\n",
    "            원문에 대한 tokenizing을 수행할 때, morphs를 추출 (defaulf = True),\n",
    "\n",
    "         - if_tokenized : boolean \n",
    "            True if input document is tokenized [default = True]\n",
    "\n",
    "         - if_morphs : boolean,\n",
    "            True : if not tokenized, tokenized with morphs,\n",
    "            False : if not tokenized, tokenized with nouns.\n",
    "\n",
    "        Return\n",
    "         - None\n",
    "        '''\n",
    "\n",
    "\n",
    "        # tokenized된 input데이터를 받으면 tokenizing skip\n",
    "        if if_tokenized :\n",
    "            train_token_ls = train_doc_ls\n",
    "        else:\n",
    "            clean_train_doc_ls = self.clean_doc(train_doc_ls)\n",
    "            if if_morphs:\n",
    "                train_token_ls = self.extract_tokens_for_all_document_FAST_VERSION(clean_train_doc_ls, if_morphs = True)\n",
    "            else :\n",
    "                train_token_ls = self.extract_tokens_for_all_document_FAST_VERSION(clean_train_doc_ls, if_morphs = False)\n",
    "\n",
    "        # train_tag_ls를 리스트 형태로 변환 (Series 형태로 들어올 경우)\n",
    "        try:  train_tag_ls = train_tag_ls.tolist()\n",
    "        except:   pass\n",
    "\n",
    "        # words와 tags로 구성된 namedtuple 형태로 데이터 변환 (tagging 작업)\n",
    "        tagged_train_doc_ls = [TaggedDocument(tuple_[0], [tuple_[1]]) for i, tuple_ in enumerate(zip(train_token_ls, train_tag_ls))]\n",
    "\n",
    "        # 학습 수행\n",
    "        self.Doc2Vec_model.train(tagged_train_doc_ls,\n",
    "                                 total_examples= self.Doc2Vec_model.corpus_count,\n",
    "                                 epochs= n_epochs)\n",
    "        return\n",
    "\n",
    "\n",
    "    # 문서 벡터를 추정하기 위한 함수 (병렬처리에 사용)\n",
    "    def _infer_vector(self,\n",
    "            doc_ls,\n",
    "            alpha=0.1,\n",
    "            steps=30,\n",
    "            queue = False):\n",
    "\n",
    "        '''\n",
    "        Doc2Vec을 사용하여, documents를 vectorize하는 함수입니다.\n",
    "        본 함수는 병렬처리를 위해 사용합니다.\n",
    "\n",
    "        Inputs\n",
    "        doc_ls : iterable or str\n",
    "            array of tokenized documents\n",
    "\n",
    "        alpha : int\n",
    "        steps : int\n",
    "\n",
    "        return\n",
    "         - matrix of documents inferred by Doc2Vec\n",
    "        '''\n",
    "        return_ls = []\n",
    "\n",
    "        # 문서 1개가 들어온 경우,\n",
    "        if type(doc_ls) == str:\n",
    "            return self.Doc2Vec_model.infer_vector(doc_ls,\n",
    "                                                   alpha = alpha,\n",
    "                                                   min_alpha = self.Doc2Vec_model.min_alpha,\n",
    "                                                   steps = steps)\n",
    "\n",
    "        # 복수 개의 문서가 input으로 들어온 경우,\n",
    "        else:\n",
    "            return [self.Doc2Vec_model.infer_vector(doc,\n",
    "                                                    alpha = alpha,\n",
    "                                                    min_alpha = self.Doc2Vec_model.min_alpha,\n",
    "                                                    steps = steps) \\\n",
    "                    for doc in doc_ls]\n",
    "\n",
    "\n",
    "    ###################################################\n",
    "    ################### 병렬처리 적용 ####################\n",
    "    ###################################################\n",
    "    def _multiprocessing_queue_put(self, func, queue, **kwargs):\n",
    "        queue.put(func(**kwargs))\n",
    "        return\n",
    "\n",
    "\n",
    "    def infer_vectors_multiprocessing(self, doc_ls):\n",
    "        queue_ls = []\n",
    "        procs = []\n",
    "        result_ls = []\n",
    "        batch_size = len(doc_ls) // 10\n",
    "\n",
    "        # process에 작업들을 할당\n",
    "        for i, idx in enumerate(range(0, len(doc_ls), batch_size)):\n",
    "            try:\n",
    "                batch_ls = doc_ls[idx : idx + batch_size]\n",
    "            except:\n",
    "                batch_ls = doc_ls[idx :]\n",
    "\n",
    "            queue_ls.append(Queue())\n",
    "            proc = Process(\n",
    "                    target= self._multiprocessing_queue_put,\n",
    "                    kwargs = {\n",
    "                        'func' : self._infer_vector,\n",
    "                        'queue' : queue_ls[i],\n",
    "                        'doc_ls' : batch_ls})\n",
    "\n",
    "            procs.append(proc)\n",
    "            proc.start()\n",
    "\n",
    "        for queue in queue_ls:\n",
    "            temp_result_ls = queue.get()\n",
    "            queue.close()\n",
    "            del queue\n",
    "\n",
    "            result_ls += temp_result_ls\n",
    "\n",
    "        for proc in procs:\n",
    "            proc.join()\n",
    "            del proc\n",
    "\n",
    "        return np.array(result_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_arr = np.array(token_ls)\n",
    "category_arr = np.array(category_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_size = int(round(len(token_ls) * 0.8))\n",
    "\n",
    "x_train, x_test = token_ls[:train_size], token_ls[train_size:]\n",
    "y_train, y_test = category_ls[:train_size], category_ls[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y 더미화\n",
    "cat2idx = defaultdict(lambda : len(cat2idx))\n",
    "y_train_idx = [cat2idx[x] for x in y_train]\n",
    "y_test_idx = [cat2idx[x] for x in y_test]\n",
    "\n",
    "# 모델 저장할 폴더 생성\n",
    "if not os.path.exists('Doc2Vec_model'):\n",
    "    os.mkdir('Doc2Vec_model')\n",
    "\n",
    "# 기존에 학습하던 결과가 있으면 불러온다.\n",
    "if os.path.isfile('Doc2Vec_model/Doc2Vec_tuning_result.csv'):\n",
    "    result_dict = pd.read_csv('Doc2Vec_model/Doc2Vec_tuning_result.csv').to_dict('list')\n",
    "    print('기존의 튜닝 결과를 불러옵니다.')\n",
    "else:\n",
    "    result_dict = defaultdict(lambda : [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = defaultdict(lambda : [])\n",
    "# training\n",
    "for dm in [1]:\n",
    "    for sample in [1e-05, 1e-06]:\n",
    "        for min_count in [1, 5, 15]:\n",
    "            for vector_size in [100,300]:\n",
    "                for negative in [5,10,15]:\n",
    "                    for n_epochs in [10,30]:\n",
    "                        print('==============================================================================')\n",
    "                        print('Model Training Started')\n",
    "\n",
    "                        # Doc2Vec 모델 생성\n",
    "                        d2v.make_Doc2Vec_model(\n",
    "                            dm = dm,\n",
    "                            min_count = min_count,\n",
    "                            sample = sample,\n",
    "                            vector_size = vector_size)\n",
    "\n",
    "                        # build and train Doc2Vec\n",
    "                        d2v.build_and_train_Doc2Vec_model(x_train, category_ls, n_epochs = n_epochs)\n",
    "                        \n",
    "                        '''\n",
    "                        # 단어 벡터 추정량 분포 시각화\n",
    "                        X =d2v.infer_vectors_multiprocessing(x_test)\n",
    "\n",
    "                        tsne= TSNE(n_components=2)\n",
    "                        X_tsne = tsne.fit_transform(X)\n",
    "                        scatter_df = pd.DataFrame(X_tsne,\n",
    "                                                  index = y_test,\n",
    "                                                  columns = ['x','y'])\n",
    "\n",
    "                        plt.figure(figsize = (10, 10))\n",
    "\n",
    "                        for i,section in enumerate(set(category_ls)):\n",
    "                            temp_df = scatter_df[scatter_df.index == section]\n",
    "                            plt.scatter(temp_df['x'].values, temp_df['y'].values, label = section, c = np.random.rand(3,))\n",
    "\n",
    "                        plt.legend(loc = 'best')\n",
    "                        plt.savefig('추정된 벡터 분포 t-sne ver')\n",
    "\n",
    "                        '''\n",
    "                        \n",
    "                        # 벡터 추정 후 학습 및 성과 평가\n",
    "                        X_train = d2v.infer_vectors_multiprocessing(x_train)\n",
    "                        X_test = d2v.infer_vectors_multiprocessing(x_test)\n",
    "\n",
    "                        # 분류기 학습\n",
    "                        clf = LogisticRegression(solver = 'sag', multi_class = 'multinomial')\n",
    "\n",
    "                        clf.fit(X_train, y_train_idx)\n",
    "                        y_prob = clf.predict_log_proba(X_test)\n",
    "                        y_pred = [prob.argmax() for prob in y_prob]\n",
    "\n",
    "                        acc = accuracy_score(y_test_idx, y_pred)\n",
    "                        loss = log_loss(y_test_idx, y_prob, normalize=False)\n",
    "\n",
    "                        print('Accuracy : ', acc)\n",
    "\n",
    "                        # 분류 결과가 best인 모델만 저장\n",
    "                        if loss < best_loss:\n",
    "                            best_loss = loss\n",
    "                            d2v.Doc2Vec_model.save('Doc2Vec_model/best_d2v_model')\n",
    "                        \n",
    "                        # 결과 저장\n",
    "                        result_dict['dm'].append(dm)\n",
    "                        result_dict['corpus_count'].append(len(token_ls))\n",
    "                        result_dict['min_count'].append(min_count)\n",
    "                        result_dict['vector_size'].append(vector_size)\n",
    "                        result_dict['negative'].append(negative)\n",
    "                        result_dict['n_epochs'].append(n_epochs)\n",
    "                        result_dict['sample'].append(sample)\n",
    "                        result_dict['accuracy'].append(acc)\n",
    "                        result_dict['loss'].append(loss)\n",
    "\n",
    "                        # 분류 결과 저장\n",
    "                        pd.DataFrame(result_dict).to_csv('Doc2Vec_model/Doc2Vec_tuning_result.csv', index=False)\n",
    "\n",
    "                        print('Model Training Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
