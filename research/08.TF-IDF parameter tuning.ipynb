{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ko_text import *\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = pd.read_csv('Data/meta_morphs_final.csv', encoding = 'utf-8')\n",
    "\n",
    "# 용량을 줄이기 위해 '단어 단어' 꼴로 묶어둔 token을 ['단어', '단어'] 꼴로 풀기\n",
    "token_df['Token'] = [token.split() for token in token_df['Token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Section</th>\n",
       "      <th>Text</th>\n",
       "      <th>Token</th>\n",
       "      <th>Num of Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>financial</td>\n",
       "      <td>\\n\\n\\n텀블벅에서 크라우드 펀딩이 이뤄지고 있는 `아침달 시집`.\\n\\n    ...</td>\n",
       "      <td>[텀블벅, 크라, 우드, 펀딩, 이뤄지고, 아침, 시집, 많지, 않은, 금액, 으로...</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>economy</td>\n",
       "      <td>\\n\\n\\n[사진 제공: 연합뉴스]\\n\\n                     유류...</td>\n",
       "      <td>[유류, 인하, 국제, 유가, 급락, 입어, 국내, 휘발유, 경유, 하락, 특히, ...</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>financial</td>\n",
       "      <td>부득이한 사정으로 매월 내는 보험료가 부담이 될 때 계약은 그대로 유지하면서 보험...</td>\n",
       "      <td>[부득이, 사정, 매월, 내는, 보험료, 부담, 계약, 그대로, 유지, 보험료, 부...</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>estate</td>\n",
       "      <td>한때 `미분양의 늪`으로 통하던 경기도 파주시 부동산 시장이 달라지고 있다. 지난해...</td>\n",
       "      <td>[한때, 미분, 하던, 경기도, 파주시, 부동산, 시장, 달라지고, 분양, 파주, ...</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>economy</td>\n",
       "      <td>\\n\\n\\n인디고뱅크의 `미키인서울` 컬래버 맨투맨  &lt;사진제공=월트디즈니코리아&gt;\\...</td>\n",
       "      <td>[인디고, 뱅크, 미키, 서울, 컬래버, 투맨, 월트디즈니, 사의, 마스코트, 미키...</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Section                                               Text  \\\n",
       "0  financial  \\n\\n\\n텀블벅에서 크라우드 펀딩이 이뤄지고 있는 `아침달 시집`.\\n\\n    ...   \n",
       "1    economy  \\n\\n\\n[사진 제공: 연합뉴스]\\n\\n                     유류...   \n",
       "2  financial   부득이한 사정으로 매월 내는 보험료가 부담이 될 때 계약은 그대로 유지하면서 보험...   \n",
       "3     estate  한때 `미분양의 늪`으로 통하던 경기도 파주시 부동산 시장이 달라지고 있다. 지난해...   \n",
       "4    economy  \\n\\n\\n인디고뱅크의 `미키인서울` 컬래버 맨투맨  <사진제공=월트디즈니코리아>\\...   \n",
       "\n",
       "                                               Token  Num of Tokens  \n",
       "0  [텀블벅, 크라, 우드, 펀딩, 이뤄지고, 아침, 시집, 많지, 않은, 금액, 으로...            263  \n",
       "1  [유류, 인하, 국제, 유가, 급락, 입어, 국내, 휘발유, 경유, 하락, 특히, ...            166  \n",
       "2  [부득이, 사정, 매월, 내는, 보험료, 부담, 계약, 그대로, 유지, 보험료, 부...            314  \n",
       "3  [한때, 미분, 하던, 경기도, 파주시, 부동산, 시장, 달라지고, 분양, 파주, ...            165  \n",
       "4  [인디고, 뱅크, 미키, 서울, 컬래버, 투맨, 월트디즈니, 사의, 마스코트, 미키...            196  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41418, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = NLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_corpus = [' '.join(doc) for doc in token_df['Token']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = round(len(token_df) * 0.8)\n",
    "np.random.seed(0)\n",
    "train_index_ls = np.random.choice(token_df.index, train_size, replace = False)\n",
    "test_index_ls = [x for x in token_df.index if not x in train_index_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33865, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = token_df.loc[train_index_ls]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8466, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = token_df.loc[test_index_ls]\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'bio & tech': 1730,\n",
       "         'business': 5055,\n",
       "         'culture & art': 4105,\n",
       "         'economy': 2611,\n",
       "         'estate': 3958,\n",
       "         'financial': 763,\n",
       "         'it': 2125,\n",
       "         'politics': 3810,\n",
       "         'society': 3374,\n",
       "         'stock': 2560,\n",
       "         'world': 3774})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_df['Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_df['Section']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TF-IDF classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=100000,\n",
    "                              min_df = 3)),\n",
    "    ('clf', OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior=None, alpha= 1e-2))),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    #'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__min_df': (1, 5, 10),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2),],\n",
    "    #'clf__estimator__alpha': (1e-2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% time\n",
    "\n",
    "train_corpus = [' '.join(doc) for doc in train_df['Token']]\n",
    "test_corpus = [' '.join(doc) for doc in test_df['Token']]\n",
    "\n",
    "y_train = train_df['Section'].tolist()\n",
    "y_test = test_df['Section'].tolist()\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(train_corpus, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)\n",
    "\n",
    "# measuring performance on test set\n",
    "print(\"Applying best classifier on test data:\")\n",
    "best_clf = grid_search_tune.best_estimator_\n",
    "print(best_clf)\n",
    "\n",
    "print(grid_search_tune.best_score_)\n",
    "#predictions = best_clf.predict(test_df['Token'].tolist())\n",
    "\n",
    "#print(classification_report(test_df['Section'].tolist(), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=50000,\n",
    "                              min_df = 3)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC())),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    \"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "    \"clf__estimator__class_weight\": ['balanced', None],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 998 µs\n",
      "Fitting 2 folds for each of 54 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed: 21.8min\n",
      "[Parallel(n_jobs=2)]: Done 108 out of 108 | elapsed: 83.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=50000, min_df=3,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "...lti_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0),\n",
       "          n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=2,\n",
       "       param_grid={'tfidf__max_df': (0.25, 0.5, 0.75), 'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)], 'clf__estimator__C': [0.01, 0.1, 1], 'clf__estimator__class_weight': ['balanced', None]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% time\n",
    "\n",
    "train_corpus = [' '.join(doc) for doc in train_df['Token']]\n",
    "test_corpus = [' '.join(doc) for doc in test_df['Token']]\n",
    "\n",
    "y_train = train_df['Section'].tolist()\n",
    "y_test = test_df['Section'].tolist()\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(train_corpus, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)\n",
    "\n",
    "# measuring performance on test set\n",
    "print(\"Applying best classifier on test data:\")\n",
    "best_clf = grid_search_tune.best_estimator_\n",
    "print(best_clf)\n",
    "\n",
    "print(grid_search_tune.best_score_)\n",
    "#predictions = best_clf.predict(test_df['Token'].tolist())\n",
    "\n",
    "#print(classification_report(test_df['Section'].tolist(), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_features=30000,\n",
    "                              min_df = 3)),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'))),\n",
    "])\n",
    "parameters = {\n",
    "    #'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    \"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "    \"clf__estimator__class_weight\": ['balanced', None],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% time\n",
    "\n",
    "train_corpus = [' '.join(doc) for doc in train_df['Token']]\n",
    "test_corpus = [' '.join(doc) for doc in test_df['Token']]\n",
    "\n",
    "y_train = train_df['Section'].tolist()\n",
    "y_test = test_df['Section'].tolist()\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(train_corpus, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set:\n",
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=30000, min_df=5,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('clf', OneVsRestClassifier(estimator=LogisticRegression(C=1, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
      "          solver='sag', tol=0.0001, verbose=0, warm_start=False),\n",
      "          n_jobs=1))]\n",
      "Applying best classifier on test data:\n",
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=30000, min_df=5,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,..._state=None,\n",
      "          solver='sag', tol=0.0001, verbose=0, warm_start=False),\n",
      "          n_jobs=1))])\n",
      "0.7364588903172959\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)\n",
    "\n",
    "# measuring performance on test set\n",
    "print(\"Applying best classifier on test data:\")\n",
    "best_clf = grid_search_tune.best_estimator_\n",
    "print(best_clf)\n",
    "\n",
    "print(grid_search_tune.best_score_)\n",
    "#predictions = best_clf.predict(test_df['Token'].tolist())\n",
    "\n",
    "#print(classification_report(test_df['Section'].tolist(), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
