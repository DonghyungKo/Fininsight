{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/raw/MK_2018_No_0_to_10000.csv')\n",
    "\n",
    "with open('stock_name_ls.pickle', 'rb') as f:\n",
    "    stock_name_ls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_uselsess_data(data):\n",
    "    useless_keyword_ls = ['신년사]','[인사]','[포토]','포토','MK포토']\n",
    "    \n",
    "    index_ls = data.index\n",
    "    title_ls = data['Title'].tolist()\n",
    "    \n",
    "    drop_index_ls = []\n",
    "    for idx, title in zip(index_ls, title_ls):\n",
    "        if any(keyword in title for keyword in useless_keyword_ls):\n",
    "            drop_index_ls.append(idx)\n",
    "            \n",
    "    return data.drop(drop_index_ls)\n",
    "\n",
    "def reclassify_categories(data, input_category, output_category):\n",
    "    '''\n",
    "    input_category에 해당하는 카테고리를 output_category로 변환하는 함수입니다.\n",
    "    \n",
    "    inputs\n",
    "    =================================\n",
    "    data : pandas.DataFrame\n",
    "        크롤링을 마친 raw data 상태의 DataFrame\n",
    "    \n",
    "    input_category : str, list\n",
    "        재분류 전 카테고리\n",
    "        \n",
    "    output_category : str, list\n",
    "        재분류 후 카테고리\n",
    "    '''\n",
    "    if type(input_category) == str:\n",
    "        input_category = [input_category]\n",
    "    \n",
    "    # category reclassification\n",
    "    data.loc[data['Section'].isin(input_category), 'Section'] = output_category\n",
    "    return data\n",
    "\n",
    "\n",
    "def to_business(data, stock_name_ls):\n",
    "    '''\n",
    "    economy, special_edition, health의 일부 기사를 business로 재분류 \n",
    "    retail, it, financial, electronics, autos, chemistry, heavy_industries의 모든 기사를 기업(business)로 재분류\n",
    "    \n",
    "    inputs\n",
    "    =================================\n",
    "    data : pandas.DataFrame\n",
    "        크롤링을 마친 raw data 상태의 DataFrame\n",
    "    \n",
    "    stock_name_ls : list,\n",
    "        기업명이 str형태로 저장된 list\n",
    "    '''\n",
    "    \n",
    "    # economy와 health 기사 제목에 상장종목 명이 포함된 경우 business로 재분류\n",
    "    section_ls = ['economy','special_edition', 'health']\n",
    "    temp_df = data.loc[data['Section'].isin(section_ls)]\n",
    "\n",
    "    index_ls = temp_df.index\n",
    "    title_ls = temp_df['Title'].tolist()\n",
    "        \n",
    "    reclassification_idx_ls = []    \n",
    "    \n",
    "    for idx, title in zip(index_ls, title_ls):\n",
    "        if any(stock_name in title for stock_name in stock_name_ls):\n",
    "            reclassification_idx_ls.append(idx)\n",
    "            \n",
    "    data.loc[reclassification_idx_ls, 'Section'] = 'business'\n",
    "    \n",
    "    \n",
    "    \n",
    "    # retail, it, financial, electronics, autos, chemistry, heavy_industries는 전부 다 business로 재분류\n",
    "    input_category_ls = ['retail','it','financial', 'electronics', 'autos', 'chemistry', 'heavy_industries']\n",
    "    \n",
    "    data = reclassify_categories(data, input_category_ls, 'business')\n",
    "    return data\n",
    "\n",
    "def to_stock(data):\n",
    "    '''\n",
    "    경제, 기업, health, special_edition 기사의 일부를\n",
    "    증권(stock)기사로 재분류 하는 함수입니다.\n",
    "    \n",
    "    inputs\n",
    "    =================================\n",
    "    data : pandas.DataFrame\n",
    "        크롤링을 마친 raw data 상태의 DataFrame\n",
    "    '''\n",
    "    section_ls = ['economy', 'business','health','special_edition']\n",
    "    temp_df = data.loc[data['Section'].isin(section_ls)]\n",
    "    \n",
    "    index_ls = temp_df.index\n",
    "    title_ls = temp_df['Title'].tolist()\n",
    "        \n",
    "    to_stock_idx_ls = []\n",
    "    keyword_ls = ['코스피', '코스닥', '증시','주가','주식','목표가','상장','특징주', '증자', \n",
    "                  '영업익', '공시', '지분', '매출','이익', 'Hot-Line', '펀드',\n",
    "                  '키움증권','NH투자','KB증권','미래에셋대우','신한금투','대신증권', 'KTB투자증권',\n",
    "                  '한투증권', '현대차투자증권', '유안타증권', '유진투자', '메리츠종금']\n",
    "\n",
    "    for idx, title in zip(index_ls, title_ls):\n",
    "        if any(keyword in title for keyword in keyword_ls):\n",
    "            to_stock_idx_ls.append(idx) \n",
    "            \n",
    "    data.loc[to_stock_idx_ls, 'Section'] = 'stock'\n",
    "    return data\n",
    "\n",
    "def to_economy(data):\n",
    "    '''\n",
    "    기업, 증권 기사에서 경제(economy)기사로 재분류 하는 함수입니다.\n",
    "    \n",
    "    inputs\n",
    "    =================================\n",
    "    data : pandas.DataFrame\n",
    "        크롤링을 마친 raw data 상태의 DataFrame\n",
    "    '''\n",
    "    section_ls = ['stock', 'business', 'special_edition']\n",
    "    temp_df = data.loc[data['Section'].isin(section_ls)]\n",
    "    \n",
    "    index_ls = temp_df.index\n",
    "    title_ls = temp_df['Title'].tolist()\n",
    "        \n",
    "    reclassification_ls = []\n",
    "    keyword_ls = ['경제', '업종','환율','핀테크','산업혁명','가상화폐','비트코인', '금리','유가',\n",
    "                  '임금',]\n",
    "\n",
    "    for idx, title in zip(index_ls, title_ls):\n",
    "        if any(keyword in title for keyword in keyword_ls):\n",
    "            reclassification_ls.append(idx)\n",
    "    \n",
    "    data.loc[reclassification_ls, 'Section'] = 'stock'\n",
    "    return data\n",
    "\n",
    "def reclassify_culture(data):\n",
    "    '''\n",
    "    entertainment와 culture 기사에서 culture & art로 재분류 하는 함수입니다.\n",
    "    inputs\n",
    "    =================================\n",
    "    data : pandas.DataFrame\n",
    "        크롤링을 마친 raw data 상태의 DataFrame\n",
    "    '''\n",
    "    # 1. 일기예보(weather-forecast) 분류\n",
    "    temp_df = data[data['Section'] == 'culture']\n",
    "    index_ls = temp_df.index\n",
    "    title_ls = temp_df['Title'].tolist()\n",
    "    \n",
    "    keyword_ls = ['기온', '날씨','온도', '영하', '한파', '눈', '추위', '폭설', '적설량', '대설', \n",
    "                  '영상', '낮','폭염', '비','더위','폭우','강수량', '장마',\n",
    "                  '쌀쌀','맑고','구름','미세먼지','안개','바람','찜통']\n",
    "    \n",
    "    reclassification_index_ls = []\n",
    "    for idx, title in zip(index_ls, title_ls):\n",
    "        if any(keyword in title for keyword in keyword_ls):\n",
    "            reclassification_index_ls.append(idx)\n",
    "    \n",
    "    data.loc[reclassification_index_ls, 'Section'] = 'weather-forecast'\n",
    "    \n",
    "    \n",
    "    # 2. culture & art 분류\n",
    "    section_ls = ['entertainment','culture']\n",
    "    temp_df = data[data['Section'].isin(section_ls)]\n",
    "    \n",
    "    keyword_ls = ['박물관','미술','전시','신간','작품','피아니스트','바이올','아트',\n",
    "              '예술','유물','展','소설','수필','문학','에세이','발간','출간','사진',\n",
    "              '뮤지컬','영화','개봉','완간']\n",
    "    \n",
    "    index_ls = temp_df.index\n",
    "    title_ls = temp_df['Title'].tolist()\n",
    "    \n",
    "    reclassfication_ls = []\n",
    "    for idx, title in zip(index_ls, title_ls):\n",
    "        if any(keyword in title for keyword in keyword_ls):\n",
    "            reclassfication_ls.append(idx)\n",
    "    \n",
    "    data.loc[reclassfication_ls, 'Section'] = 'culture & art'\n",
    "    \n",
    "    return data.drop(data[data['Section'] == 'culture'].index)\n",
    "\n",
    "def drop_categories(data, drop_category_ls):\n",
    "    '''\n",
    "    ' ', people, opinion, special_edition 카테고리 및, 전체 비중의 0.1% 이하를 차지하는 기사를 전부 제거\n",
    "    \n",
    "    inputs\n",
    "    =================================\n",
    "    data : pandas.DataFrame\n",
    "        크롤링을 마친 raw data 상태의 DataFrame\n",
    "        \n",
    "    drop_category_ls : str, list\n",
    "        제거 대상 category 목록\n",
    "    '''\n",
    "    \n",
    "    # 원하지 않는 카테고리는 제거\n",
    "    if type(drop_category_ls) == str:\n",
    "        drop_category_ls = [drop_category_ls]\n",
    "    \n",
    "    drop_index_ls = data[data['Section'].isin(drop_category_ls)].index\n",
    "    data.drop(drop_index_ls, inplace = True)\n",
    "    \n",
    "    \n",
    "    # 전체 비중의 0.1% 이하의 카테고리 제거\n",
    "    ratio_huddle = len(data) // 1000\n",
    "    \n",
    "    total_category_ls = list(set(data['Section']))\n",
    "    counter = Counter(data['Section'])\n",
    "    \n",
    "    useful_category_ls = []\n",
    "    for category in total_category_ls:\n",
    "        if counter[category] > ratio_huddle:\n",
    "            useful_category_ls.append(category)\n",
    "    \n",
    "    return data.loc[data['Section'].isin(useful_category_ls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = drop_uselsess_data(data)\n",
    "\n",
    "data = reclassify_categories(data, \n",
    "                            ['tv_broadcasting', 'entertainment_topic', 'broadcasting', 'hot_issue', 'music', 'overseas_etn'], \n",
    "                            'entertainment')\n",
    "\n",
    "data = reclassify_categories(data, 'golf', 'sports')\n",
    "data = reclassify_categories(data, ['movie','performance'], 'culture & art')\n",
    "data = reclassify_categories(data, 'patent', 'technology')\n",
    "\n",
    "data = to_business(data, stock_name_ls)\n",
    "\n",
    "data = to_stock(data)\n",
    "\n",
    "data = to_economy(data)\n",
    "\n",
    "data = reclassify_culture(data)\n",
    "\n",
    "data = drop_categories(data, [' ','opinion','people','special_edition'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(data['Section']).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stock', 1520),\n",
       " ('society', 1202),\n",
       " ('entertainment', 994),\n",
       " ('business', 846),\n",
       " ('politics', 708),\n",
       " ('world', 667),\n",
       " ('sports', 468),\n",
       " ('economy', 404),\n",
       " ('estate', 164),\n",
       " ('culture & art', 134),\n",
       " ('travel', 93),\n",
       " ('weather-forecast', 81),\n",
       " ('health', 48),\n",
       " ('technology', 41)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7370, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ko_text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = NLP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 클렌징을 위한 정규표현식 or 구문 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_ls = ['여기를 누르시면 크게 보실 수 있습니다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_regex(regex_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 불용어(stopwords) 사전 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_ls = ['googletagdisplay',\n",
    "               'windowjQuery',\n",
    "               'documentwrite',\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_stopwords(stopword_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_doc_ls = nlp.extract_morphs_for_all_document_FAST_VERSION(data['Text'].tolist(),\n",
    "                                                                n_thread = 4)\n",
    "\n",
    "data = data\n",
    "\n",
    "data['Token'] = token_doc_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter([token for doc in token_doc_ls for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for token in np.random.choice(data['Token'],100):\n",
    "    input('아무 키나 ')\n",
    "    print(' '.join(token))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 중복되는 기사는 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index 초기화\n",
    "data.index = np.arange(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ls = []\n",
    "unique_idx_ls = []\n",
    "\n",
    "for idx, text in enumerate(data['Text']):\n",
    "    if not text in text_ls:\n",
    "        text_ls.append(text)\n",
    "        unique_idx_ls.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7370 7247\n"
     ]
    }
   ],
   "source": [
    "print(len(data), len(unique_idx_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[unique_idx_ls]\n",
    "data.index = np.arange(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰 길이 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_ls = []\n",
    "\n",
    "for token in data['Token']:\n",
    "    len_ls.append(len(token))\n",
    "\n",
    "data['num_token'] = len_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Section</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>843.0</td>\n",
       "      <td>161.621590</td>\n",
       "      <td>122.209004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.00</td>\n",
       "      <td>132.0</td>\n",
       "      <td>219.50</td>\n",
       "      <td>762.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>culture &amp; art</th>\n",
       "      <td>134.0</td>\n",
       "      <td>143.768657</td>\n",
       "      <td>128.294534</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.25</td>\n",
       "      <td>113.0</td>\n",
       "      <td>194.00</td>\n",
       "      <td>726.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economy</th>\n",
       "      <td>403.0</td>\n",
       "      <td>198.307692</td>\n",
       "      <td>158.117658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.00</td>\n",
       "      <td>171.0</td>\n",
       "      <td>255.50</td>\n",
       "      <td>2058.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>994.0</td>\n",
       "      <td>97.725352</td>\n",
       "      <td>103.180383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>86.0</td>\n",
       "      <td>139.00</td>\n",
       "      <td>971.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>estate</th>\n",
       "      <td>164.0</td>\n",
       "      <td>189.469512</td>\n",
       "      <td>108.964081</td>\n",
       "      <td>22.0</td>\n",
       "      <td>121.25</td>\n",
       "      <td>170.0</td>\n",
       "      <td>228.00</td>\n",
       "      <td>849.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>48.0</td>\n",
       "      <td>241.604167</td>\n",
       "      <td>176.790866</td>\n",
       "      <td>54.0</td>\n",
       "      <td>123.75</td>\n",
       "      <td>197.0</td>\n",
       "      <td>274.25</td>\n",
       "      <td>760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politics</th>\n",
       "      <td>652.0</td>\n",
       "      <td>199.694785</td>\n",
       "      <td>132.153290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.00</td>\n",
       "      <td>166.0</td>\n",
       "      <td>268.75</td>\n",
       "      <td>884.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>society</th>\n",
       "      <td>1187.0</td>\n",
       "      <td>152.967144</td>\n",
       "      <td>105.868765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.00</td>\n",
       "      <td>128.0</td>\n",
       "      <td>200.00</td>\n",
       "      <td>836.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>468.0</td>\n",
       "      <td>159.326923</td>\n",
       "      <td>105.403609</td>\n",
       "      <td>2.0</td>\n",
       "      <td>86.75</td>\n",
       "      <td>136.0</td>\n",
       "      <td>208.00</td>\n",
       "      <td>755.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock</th>\n",
       "      <td>1483.0</td>\n",
       "      <td>73.679703</td>\n",
       "      <td>105.173486</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>16.0</td>\n",
       "      <td>97.50</td>\n",
       "      <td>1284.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technology</th>\n",
       "      <td>41.0</td>\n",
       "      <td>190.487805</td>\n",
       "      <td>90.333582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>129.00</td>\n",
       "      <td>194.0</td>\n",
       "      <td>222.00</td>\n",
       "      <td>451.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>travel</th>\n",
       "      <td>93.0</td>\n",
       "      <td>181.559140</td>\n",
       "      <td>116.363791</td>\n",
       "      <td>33.0</td>\n",
       "      <td>91.00</td>\n",
       "      <td>153.0</td>\n",
       "      <td>237.00</td>\n",
       "      <td>588.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather-forecast</th>\n",
       "      <td>77.0</td>\n",
       "      <td>107.220779</td>\n",
       "      <td>79.665357</td>\n",
       "      <td>22.0</td>\n",
       "      <td>63.00</td>\n",
       "      <td>87.0</td>\n",
       "      <td>129.00</td>\n",
       "      <td>545.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>world</th>\n",
       "      <td>660.0</td>\n",
       "      <td>190.092424</td>\n",
       "      <td>101.668414</td>\n",
       "      <td>3.0</td>\n",
       "      <td>120.00</td>\n",
       "      <td>171.0</td>\n",
       "      <td>234.00</td>\n",
       "      <td>724.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   count        mean         std   min     25%    50%     75%  \\\n",
       "Section                                                                         \n",
       "business           843.0  161.621590  122.209004   0.0   79.00  132.0  219.50   \n",
       "culture & art      134.0  143.768657  128.294534   0.0   56.25  113.0  194.00   \n",
       "economy            403.0  198.307692  158.117658   0.0  105.00  171.0  255.50   \n",
       "entertainment      994.0   97.725352  103.180383   0.0    5.00   86.0  139.00   \n",
       "estate             164.0  189.469512  108.964081  22.0  121.25  170.0  228.00   \n",
       "health              48.0  241.604167  176.790866  54.0  123.75  197.0  274.25   \n",
       "politics           652.0  199.694785  132.153290   0.0  104.00  166.0  268.75   \n",
       "society           1187.0  152.967144  105.868765   0.0   83.00  128.0  200.00   \n",
       "sports             468.0  159.326923  105.403609   2.0   86.75  136.0  208.00   \n",
       "stock             1483.0   73.679703  105.173486   0.0    7.00   16.0   97.50   \n",
       "technology          41.0  190.487805   90.333582   0.0  129.00  194.0  222.00   \n",
       "travel              93.0  181.559140  116.363791  33.0   91.00  153.0  237.00   \n",
       "weather-forecast    77.0  107.220779   79.665357  22.0   63.00   87.0  129.00   \n",
       "world              660.0  190.092424  101.668414   3.0  120.00  171.0  234.00   \n",
       "\n",
       "                     max  \n",
       "Section                   \n",
       "business           762.0  \n",
       "culture & art      726.0  \n",
       "economy           2058.0  \n",
       "entertainment      971.0  \n",
       "estate             849.0  \n",
       "health             760.0  \n",
       "politics           884.0  \n",
       "society            836.0  \n",
       "sports             755.0  \n",
       "stock             1284.0  \n",
       "technology         451.0  \n",
       "travel             588.0  \n",
       "weather-forecast   545.0  \n",
       "world              724.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('Section')['num_token'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰의 길이가 30개 이상인 표본만 추출\n",
    "data = data[data['num_token'] > 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = round(len(token_df) * 0.8)\n",
    "np.random.seed(0)\n",
    "train_index_ls = np.random.choice(token_df.index, train_size, replace = False)\n",
    "test_index_ls = [x for x in token_df.index if not x in train_index_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4648, 8)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = token_df.loc[train_index_ls]\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1162, 8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = token_df.loc[test_index_ls]\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'business': 600,\n",
       "         'culture & art': 89,\n",
       "         'economy': 311,\n",
       "         'entertainment': 533,\n",
       "         'estate': 134,\n",
       "         'health': 37,\n",
       "         'politics': 498,\n",
       "         'society': 885,\n",
       "         'sports': 348,\n",
       "         'stock': 535,\n",
       "         'technology': 31,\n",
       "         'travel': 72,\n",
       "         'weather-forecast': 67,\n",
       "         'world': 508})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_df['Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'business': 155,\n",
       "         'culture & art': 24,\n",
       "         'economy': 67,\n",
       "         'entertainment': 117,\n",
       "         'estate': 29,\n",
       "         'health': 11,\n",
       "         'politics': 132,\n",
       "         'society': 231,\n",
       "         'sports': 77,\n",
       "         'stock': 133,\n",
       "         'technology': 9,\n",
       "         'travel': 21,\n",
       "         'weather-forecast': 9,\n",
       "         'world': 147})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(test_df['Section'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sampling for training classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 label마다 학습할 단어의 수\n",
    "train_batch_size = 300\n",
    "test_batch_size = 30\n",
    "\n",
    "train_token_ls_split, train_tag_ls_split = nlp.oversample_batch(train_df['Token'], train_df['Section'], train_batch_size)\n",
    "test_token_ls_split, test_tag_ls_split =  nlp.oversample_batch(test_df['Token'],test_df['Section'], test_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'business': 300,\n",
       "         'culture & art': 300,\n",
       "         'economy': 300,\n",
       "         'entertainment': 300,\n",
       "         'estate': 300,\n",
       "         'health': 300,\n",
       "         'politics': 300,\n",
       "         'society': 300,\n",
       "         'sports': 300,\n",
       "         'stock': 300,\n",
       "         'technology': 300,\n",
       "         'travel': 300,\n",
       "         'weather-forecast': 300,\n",
       "         'world': 300})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_token_ls_split))\n",
    "Counter(train_tag_ls_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'business': 30,\n",
       "         'culture & art': 30,\n",
       "         'economy': 30,\n",
       "         'entertainment': 30,\n",
       "         'estate': 30,\n",
       "         'health': 30,\n",
       "         'politics': 30,\n",
       "         'society': 30,\n",
       "         'sports': 30,\n",
       "         'stock': 30,\n",
       "         'technology': 30,\n",
       "         'travel': 30,\n",
       "         'weather-forecast': 30,\n",
       "         'world': 30})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(test_tag_ls_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 잘 뽑혔는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이병철 KTB 투자 증권 부회장 성문 회장 전량 매수 최대 주주 올라서게 이로써 불거진 KTB 투자 증권 경영 분쟁 논란 부회장 승리 맺음 금융투자 업계 부회장 측은 회장 최대 주주 변경 논의 회장 보유 전량 매수 합의 부회장 KTB 투자 증권 최대 주주 올라서게 회장 의결권 주식 기준 가운데 사들이기로 계약금 입금 완료 아울러 회장 추가 매입 나머지 역시 회장 요구 대로 주당 이자 더해 분할 매수 했다이밖에 회장 요구 회장 비서실 임원 직원 여명 고용 보장 합의 합의 부회장 주주 에서 주주 올라서게 회장 경영 일선 에서 물러날 전망'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_token_ls_split[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stock'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tag_ls_split[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Parameter 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **set ALPHA as default**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ls = train_df['Token'].tolist() + test_df['Token'].tolist()\n",
    "label_ls = train_df['Section'].tolist() + test_df['Section'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5810, 5810)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_ls), len(label_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result_dict = {\n",
    "                'corpus_count' : [],\n",
    "               'min_count' : [],\n",
    "               'vector_size' : [],\n",
    "               'window' : [],\n",
    "               'n_epochs' : [],\n",
    "               'accuracy' : [],\n",
    "               'sample' : [],\n",
    "               'dm' : [],\n",
    "              }\n",
    "\n",
    "testing_section_ls = ['경제','기업','사회','국제','부동산','증권','정치','IT과학','문화']\n",
    "\n",
    "# 하이퍼 파라미터 튜닝 작업 수행\n",
    "\n",
    "for dm in [1]:\n",
    "    for doc2vec_size in ['']:\n",
    "        if doc2vec_size == '':\n",
    "            x_split, y_split = token_ls, label_ls\n",
    "\n",
    "        for sample in [1e-04, 1e-05, 1e-06]:\n",
    "            for min_count in [1, 5, 15, 50]:\n",
    "                for vector_size in [100,300]:\n",
    "                    for window in [5,15]:\n",
    "                        for n_epochs in [10]:\n",
    "\n",
    "                            # Doc2Vec 모델 생성\n",
    "                            nlp.make_Doc2Vec_model(dm = dm,\n",
    "                                                   min_count = min_count,\n",
    "                                                   sample = sample,\n",
    "                                                   vector_size = vector_size,\n",
    "                                                   window = window,\n",
    "                                                   dm_mean = 0,\n",
    "                                                   dm_concat = 0)\n",
    "\n",
    "                            nlp.build_and_train_Doc2Vec_model(x_split,\n",
    "                                                              y_split,\n",
    "                                                              n_epochs = n_epochs)\n",
    "\n",
    "\n",
    "                            model_name = 'Doc2Vec_dm=%s&cc=%s&vs=%s&win=%s&min=%s&sample=%s&epochs=%s'%(\\\n",
    "                                                                                                                 nlp.Doc2Vec_model.dm,\n",
    "                                                                                                                   nlp.Doc2Vec_model.corpus_count,\n",
    "                                                                                                                   nlp.Doc2Vec_model.vector_size,\n",
    "                                                                                                                   nlp.Doc2Vec_model.window,\n",
    "                                                                                                                   nlp.Doc2Vec_model.min_count,\n",
    "                                                                                                                   nlp.Doc2Vec_model.sample,\n",
    "                                                                                                                   nlp.Doc2Vec_model.epochs)\n",
    "                            # Doc2Vec 모델 저장\n",
    "                            # nlp.Doc2Vec_model.save('Doc2Vec_model/'+model_name)\n",
    "                            X =nlp.infer_vectors_with_Doc2Vec(train_token_ls_split, alpha = 0.1)\n",
    "\n",
    "                            tsne= TSNE(n_components=2)\n",
    "                            X_tsne = tsne.fit_transform(X)\n",
    "                            scatter_df = pd.DataFrame(X_tsne,\n",
    "                                                      index = train_tag_ls_split,\n",
    "                                                      columns = ['x','y'])\n",
    "\n",
    "                            plt.figure(figsize = (10, 10))\n",
    "\n",
    "                            for i,section in enumerate(set(test_df['Section'])):\n",
    "                                temp_df = scatter_df[scatter_df.index == section]\n",
    "                                plt.scatter(temp_df['x'].values, temp_df['y'].values, label = section, c = np.random.rand(3,))\n",
    "\n",
    "                            plt.legend(loc = 'best')\n",
    "                            plt.savefig('추정된 벡터 분포 t-sne ver')\n",
    "                            \n",
    "                            # clf를 각 레이블별 1000개씩 학습, \n",
    "                            X_train = nlp.infer_vectors_with_Doc2Vec(train_token_ls_split)\n",
    "                            y_train = train_tag_ls_split\n",
    "\n",
    "                            X_test = nlp.infer_vectors_with_Doc2Vec(test_token_ls_split)\n",
    "                            y_test = test_tag_ls_split\n",
    "\n",
    "\n",
    "                            clf = LogisticRegression(solver = 'sag',\n",
    "                                                     multi_class = 'multinomial')\n",
    "\n",
    "\n",
    "                            clf.fit(X_train, y_train)\n",
    "                            y_pred = clf.predict(X_test)\n",
    "                            \n",
    "                            result_dict['dm'].append(nlp.Doc2Vec_model.dm)\n",
    "                            result_dict['corpus_count'].append(nlp.Doc2Vec_model.corpus_count)\n",
    "                            result_dict['min_count'].append(nlp.Doc2Vec_model.min_count)\n",
    "                            result_dict['vector_size'].append(nlp.Doc2Vec_model.vector_size)\n",
    "                            result_dict['window'].append(nlp.Doc2Vec_model.window)\n",
    "                            result_dict['n_epochs'].append(nlp.Doc2Vec_model.epochs)\n",
    "                            result_dict['sample'].append(nlp.Doc2Vec_model.sample)\n",
    "                            result_dict['accuracy'].append(accuracy_score(y_pred, y_test))\n",
    "\n",
    "                            print(accuracy_score(y_pred, y_test))\n",
    "\n",
    "                            \n",
    "            pd.DataFrame(result_dict).to_csv('Parameter_tuning_result.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과 저장\n",
    "- ## 최초 1회만 수행한 후, 결과를 저장하여 불러와서 쓴당"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **저장 공간 절약을 위해, ['단어', '단어'] 꼴로 저장된 토큰을 '단어 단어' 꼴로 바꿔준다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df['Token'] = [' '.join(doc) for doc in token_df['Token'].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df.to_csv('Data/meta_morphs.csv', index = False)\n",
    "#test_df.to_csv('Data/test_morphs_final.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fininsight_python_3.5",
   "language": "python",
   "name": "fininsight_python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
