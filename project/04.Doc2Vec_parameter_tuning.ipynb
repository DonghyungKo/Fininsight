{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unset PYTHONPATH first\n",
    "from ko_text import *\n",
    "from ko_crawler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = NLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Data/Train_final.csv', encoding = 'utf-8')\n",
    "test_df = pd.read_csv('Data/Test_final.csv', encoding = 'utf-8')\n",
    "\n",
    "# 용량을 줄이기 위해 '단어 단어' 꼴로 묶어둔 token을 ['단어', '단어'] 꼴로 풀기\n",
    "train_df['Token'] = [token.split() for token in train_df['Token']]\n",
    "test_df['Token'] = [token.split() for token in test_df['Token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87899, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'IT과학': 9996,\n",
       "         '경제': 9660,\n",
       "         '국제': 9844,\n",
       "         '기업': 9965,\n",
       "         '문화': 9846,\n",
       "         '부동산': 9986,\n",
       "         '사회': 9880,\n",
       "         '정치': 9295,\n",
       "         '증권': 9427})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(train_df['Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>Section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[NH, 농협, 금융, 지주, 김광수, 회장, 22일, 서대문, 본사, 에서, 농협...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[바이오, 직스, 올해, 개별, 기준, 3분, 영업, 이익, 105억원, 으로, 작...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[코스피, 붕괴하면서, 달러, 환율, 으로, 뛰었다, 23일, 서울, 외환시장, 에...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[정의당, 국회, 롯데, 갑질, 피해자, 간담, 개최, 김상조, 정거, 위원장, 2...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[코스피, 23일, 무역, 전쟁, 내외, 악재, 힘없이, 2110, 내줬다, 코스피...</td>\n",
       "      <td>경제</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Token Section\n",
       "0  [NH, 농협, 금융, 지주, 김광수, 회장, 22일, 서대문, 본사, 에서, 농협...      경제\n",
       "1  [바이오, 직스, 올해, 개별, 기준, 3분, 영업, 이익, 105억원, 으로, 작...      경제\n",
       "2  [코스피, 붕괴하면서, 달러, 환율, 으로, 뛰었다, 23일, 서울, 외환시장, 에...      경제\n",
       "3  [정의당, 국회, 롯데, 갑질, 피해자, 간담, 개최, 김상조, 정거, 위원장, 2...      경제\n",
       "4  [코스피, 23일, 무역, 전쟁, 내외, 악재, 힘없이, 2110, 내줬다, 코스피...      경제"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한 label마다 학습할 단어의 수\n",
    "train_size_for_each_label = 1000\n",
    "test_size_for_each_label = 300\n",
    "\n",
    "n_class = len(train_df['Section'].unique())\n",
    "\n",
    "\n",
    "# Doc2Vec으로 vector를 추정하기 위한 split 과정\n",
    "train_batch_size = n_class * train_size_for_each_label\n",
    "test_batch_size = n_class * test_size_for_each_label\n",
    "\n",
    "train_token_ls_split, train_tag_ls_split = nlp.extract_a_equally_splited_batch(train_df['Token'], train_df['Section'], train_batch_size)\n",
    "test_token_ls_split, test_tag_ls_split =  nlp.extract_a_equally_splited_batch(test_df['Token'],test_df['Section'], test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아마 단독콘서트 서울 아트 마켓 쇼케이스 연극 소설가 구보 사람 서울 청계천로 한류 복합 창작 전문 공연장 CKL 스테이지 무대 에서 10월 동안 국악 무용 연극 가을 감성 자극 로운 문화 공연 펼쳐진다 27일 한국 콘텐츠 진흥 따르면 소리 연희 전공 여성 모인 국악 창작 그룹 아마 10월 3일 단독 콘서트 발아 Budding 선보인다 이번 무대 에는 형식 얽매 이지 않은 음악 어법 으로 시대 살아가는 여성 시각 이면 이야기 담는다세 상과 사람 대한 시선 나타낸 아스팔트 피는 미세먼지 에서 출발 고통 주는 로부터 벗어나 고픈 마음 노래 dusty city 우리 살아가는 시대 표현 혐오 시대 들려줄 예정 이다 아마 목소리 만으로 강렬한 메시지 전달 강한 여운 남긴다 10월 712일 열리는 국내 최대 규모 국제 예술 공연 마켓 2018 아트 마켓 우수 작품 해외 작품 선정 CKL 스테이지 에서 쇼케이스 펼친다 10일 없는 현실 살아가는 어둠 몸짓 주목 전과 셰익스피어 오셀로 탈춤 접목 오셀로 이아 고가 무대 오른다 11일 동시 살아가는 각자 다룬 동행 스웨덴 에서 시르커스 알폰 하이테크 뮤지컬 AM Somebody 예정 있다 12일 2018 한국 평론 협회 평론가 연기상 수상작 Voice of Acts 다양한 변주 보여주는 80 노파 저승 여행 이야기 차례 만난다 한국 문학사 발자국 남긴 소설가 구보 박태원 바탕 으로 옴니버스 연극 소설가 구보 사람 18일 부터 열흘 무대 오른다 2007년 예술의전당 자유 젊은 연극 선정 으로 일으킨 작품 으로 12 언어 연극 스튜디오 창립 12 주년 기념 창작 11년 만에 재연 극작가 이자 연출가 성기웅 지난 10 선보이는 구보 연작 작품 으로 1930년 경성 생활 풍속 다채롭 담아 냈다 CKL 스테이지 박태원 생가 약국 자리 구보 과거 현재 만나는 장소 로서 남다른 의미 있다 콘텐츠 진흥 운영 하는 CKL 스테이지 서울 중구 CKL 기업 지원 센터 지하 있는 한류 복합 창작 전문 공연장 이다 자세한 사항 CKL 스테이지 홈페이지 에서 확인 있다'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(train_token_ls_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문화'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tag_ls_split[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Parameter 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **set ALPHA as default**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result_dict = {'corpus_count' : [],\n",
    "               'min_count' : [],\n",
    "               'vector_size' : [],\n",
    "               'window' : [],\n",
    "               'n_epochs' : [],\n",
    "               'accuracy' : [],\n",
    "              }\n",
    "\n",
    "testing_section_ls = ['경제','기업','사회','국제','부동산','증권','정치','IT과학','문화']\n",
    "\n",
    "# 하이퍼 파라미터 튜닝 작업 수행\n",
    "\n",
    "for size in [30000, len(train_df)]:\n",
    "    x_split, y_split = nlp.extract_a_equally_splited_batch(train_df['Token'].tolist(), train_df['Section'].tolist(), size)\n",
    "\n",
    "    for min_count in [10, 30, 50]:\n",
    "        for vector_size in [100, 300]:\n",
    "            for window in [5, 10]:\n",
    "                for n_epochs in [10,30]:\n",
    "\n",
    "                    # Doc2Vec 모델 생성\n",
    "                    nlp.make_Doc2Vec_model(min_count = min_count,\n",
    "                                           sample = 1e-05,\n",
    "                                           vector_size = vector_size,\n",
    "                                           window = window,\n",
    "                                           dm_mean = 0,\n",
    "                                           dm_concat = 0)\n",
    "\n",
    "                    nlp.build_and_train_Doc2Vec_model(x_split,\n",
    "                                                      y_split,\n",
    "                                                      n_epochs = n_epochs)\n",
    "\n",
    "\n",
    "                    model_name = 'Doc2Vec_dm=%s&cc=%s&vs=%s&win=%s&neg=%s&min=%s&sample=%s&epochs=%s'%(\\\n",
    "                                                                                                         nlp.Doc2Vec_model.dm,\n",
    "                                                                                                           nlp.Doc2Vec_model.corpus_count,\n",
    "                                                                                                           nlp.Doc2Vec_model.vector_size,\n",
    "                                                                                                           nlp.Doc2Vec_model.window,\n",
    "                                                                                                           nlp.Doc2Vec_model.negative,\n",
    "                                                                                                           nlp.Doc2Vec_model.min_count,\n",
    "                                                                                                           nlp.Doc2Vec_model.sample,\n",
    "                                                                                                           nlp.Doc2Vec_model.epochs)\n",
    "                    # Doc2Vec 모델 저장\n",
    "                    #nlp.Doc2Vec_model.save('Doc2Vec_model/'+model_name)\n",
    "\n",
    "\n",
    "                    '''\n",
    "                    \n",
    "                    X =nlp.infer_vectors_with_Doc2Vec(train_token_for_display[0])\n",
    "\n",
    "                    from sklearn.decomposition import PCA\n",
    "\n",
    "                    pca = PCA(n_components=2)\n",
    "                    X_pca = pca.fit_transform(X)\n",
    "                    scatter_df = pd.DataFrame(X_pca,\n",
    "                                              index = train_tag_for_display[0],\n",
    "                                              columns = ['x','y'])\n",
    "\n",
    "                    plt.figure(figsize = (15, 15))\n",
    "\n",
    "                    for section in testing_section_ls:\n",
    "                        temp_df = scatter_df[scatter_df.index == section]\n",
    "                        plt.scatter(temp_df['x'].values, temp_df['y'].values, label = section, c = np.random.rand(3,))\n",
    "\n",
    "                    plt.legend(loc = 'best')\n",
    "                    plt.savefig('Doc2Vec_model/images/'+model_name)\n",
    "\n",
    "                    '''\n",
    "                    # 각 레이블별 1000개씩 학습, \n",
    "\n",
    "                    # LR\n",
    "                    X_train = nlp.infer_vectors_with_Doc2Vec(train_token_ls_split)\n",
    "                    y_train = train_tag_ls_split\n",
    "\n",
    "                    X_test = nlp.infer_vectors_with_Doc2Vec(test_token_ls_split)\n",
    "                    y_test = test_tag_ls_split\n",
    "\n",
    "\n",
    "                    clf = LogisticRegression(solver = 'sag',\n",
    "                                             multi_class = 'multinomial')\n",
    "\n",
    "\n",
    "                    clf.fit(X_train, y_train)\n",
    "                    y_pred = clf.predict(X_test)\n",
    "\n",
    "                    result_dict['corpus_count'].append(nlp.Doc2Vec_model.corpus_count)\n",
    "                    result_dict['min_count'].append(nlp.Doc2Vec_model.min_count)\n",
    "                    result_dict['vector_size'].append(nlp.Doc2Vec_model.vector_size)\n",
    "                    result_dict['window'].append(nlp.Doc2Vec_model.window)\n",
    "                    result_dict['n_epochs'].append(nlp.Doc2Vec_model.epochs)\n",
    "                    result_dict['accuracy'].append(accuracy_score(y_pred, y_test))\n",
    "                    \n",
    "                    print(accuracy_score(y_pred, y_test))\n",
    "                    \n",
    "                    \n",
    "    pd.DataFrame(result_dict).to_csv('Parameter_tuning_result-dbow.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.Doc2Vec_model.docvecs.vectors_docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('도널드', 0.863656222820282),\n",
       " ('대통령', 0.7049350738525391),\n",
       " ('백악관', 0.6218736171722412),\n",
       " ('행정부', 0.6034835577011108),\n",
       " ('백악', 0.5987303853034973),\n",
       " ('미국', 0.5439364314079285),\n",
       " ('김정은', 0.529384970664978),\n",
       " ('폼페이', 0.5000470876693726),\n",
       " ('폭스뉴스', 0.48696279525756836),\n",
       " ('정상회담', 0.4868667721748352)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.Doc2Vec_model.similar_by_word('트럼프')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
